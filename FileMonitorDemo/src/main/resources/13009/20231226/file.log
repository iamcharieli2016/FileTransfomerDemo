2023-12-26 10:30:13 INFO  FlinkTable2IceTable:52 - create iceberg_catalog now!
2023-12-26 10:30:13 INFO  FlinkTable2IceTable:64 - catalog:create catalog iceberg_catalog with (
   'type'='iceberg',
   'catalog-type'='hive',
   'uri'='thrift://172.20.29.46:9083',
   'hive-conf-dir'='/Users/lifenghua/study/sourcecode/iceberg-demo/config/hive-conf-46',
   'hadoop-conf-dir'='/Users/lifenghua/study/sourcecode/iceberg-demo/config/hive-conf-46',
   'client'='1',
   'property-version'='2',
   'warehouse'='hdfs://172.20.29.46:8020/user/hive/warehouse/')

2023-12-26 10:30:13 WARN  HadoopUtils:139 - Could not find Hadoop configuration via any of the supported methods (Flink configuration, environment variables).
2023-12-26 10:30:13 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-26 10:30:13 INFO  HiveConf:176 - Found configuration file null
2023-12-26 10:30:14 WARN  HiveConf:4015 - HiveConf of name hive.vectorized.use.checked.expressions does not exist
2023-12-26 10:30:14 WARN  HiveConf:4015 - HiveConf of name hive.strict.checks.no.partition.filter does not exist
2023-12-26 10:30:14 WARN  HiveConf:4015 - HiveConf of name hive.strict.checks.orderby.no.limit does not exist
2023-12-26 10:30:14 WARN  HiveConf:4015 - HiveConf of name hive.vectorized.input.format.excludes does not exist
2023-12-26 10:30:14 WARN  HiveConf:4015 - HiveConf of name hive.strict.checks.bucketing does not exist
2023-12-26 10:30:14 INFO  metastore:434 - Trying to connect to metastore with URI thrift://172.20.29.46:9083
2023-12-26 10:30:14 INFO  metastore:479 - Opened a connection to metastore, current connections: 1
2023-12-26 10:30:14 INFO  metastore:531 - Connected to metastore.
2023-12-26 10:30:14 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00001-3432b212-74ce-4e73-bbc0-7277af2417dd.metadata.json
2023-12-26 10:30:16 INFO  BaseMetastoreCatalog:64 - Table loaded by catalog: iceberg_catalog.db1.ice_T01
2023-12-26 10:30:16 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00001-3432b212-74ce-4e73-bbc0-7277af2417dd.metadata.json
2023-12-26 10:30:18 INFO  CatalogUtil:93 - Manifests to delete: 
2023-12-26 10:30:18 INFO  HiveCatalog:179 - Dropped table: db1.ice_T01
2023-12-26 10:30:18 INFO  FlinkTable2IceTable:78 - 执行sql:CREATE TABLE if not exists _db_.kafka_T01 (
    C1 decimal,
    C2 STRING,
    C3 timestamp,
    PRIMARY KEY (`C1`) NOT ENFORCED
 ) PARTITIONED BY (`C1`)
 WITH (
    'connector' = 'kafka',
    'topic' = 'T01',
    'properties.bootstrap.servers' = '172.20.3.227:9092',
    'properties.group.id' = 'g1',
    'scan.startup.mode' = 'latest-offset',
    'scan.topic-partition-discovery.interval'='10000',
    'value.format' = 'debezium-json'
)

2023-12-26 10:30:18 INFO  FlinkTable2IceTable:84 - create iceberg sql :CREATE TABLE if not exists iceberg_catalog.db1.ice_T01 (
    C1 decimal,
    C2 STRING,
    C3 timestamp,
    PRIMARY KEY (`C1`) NOT ENFORCED
) PARTITIONED BY (`C1`)
WITH (
    'type'='iceberg',
    'table_type'='iceberg',
    'format-version'='2',
    'engine.hive.enabled' = 'true',
    'write.upsert.enabled'='true',
    'table.exec.sink.not-null-enforcer'='true'
)

2023-12-26 10:30:18 INFO  BaseMetastoreCatalog:234 - Table properties set at catalog level through catalog properties: {}
2023-12-26 10:30:18 INFO  BaseMetastoreCatalog:246 - Table properties enforced at catalog level through catalog properties: {}
2023-12-26 10:30:19 INFO  HiveTableOperations:315 - Committed to table iceberg_catalog.db1.ice_T01 with the new metadata location hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00000-19ee13a9-f0b9-4a30-aadf-bac93fa20a0e.metadata.json
2023-12-26 10:30:19 INFO  BaseMetastoreTableOperations:137 - Successfully committed to table iceberg_catalog.db1.ice_T01 in 1067 ms
2023-12-26 10:30:19 INFO  FlinkTable2IceTable:88 - insert sql:insert into iceberg_catalog.db1.ice_T01 select * from db1.kafka_T01
2023-12-26 10:30:20 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00000-19ee13a9-f0b9-4a30-aadf-bac93fa20a0e.metadata.json
2023-12-26 10:30:22 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00000-19ee13a9-f0b9-4a30-aadf-bac93fa20a0e.metadata.json
2023-12-26 10:30:22 INFO  BaseMetastoreCatalog:64 - Table loaded by catalog: iceberg_catalog.db1.ice_T01
2023-12-26 10:30:22 INFO  FlinkSink:430 - Write distribution mode is 'none'
2023-12-26 10:30:22 INFO  FlinkSink:436 - Distribute rows by equality fields, because there are equality fields set
2023-12-26 10:30:22 INFO  TypeExtractor:1994 - class org.apache.iceberg.io.WriteResult does not contain a setter for field dataFiles
2023-12-26 10:30:22 INFO  TypeExtractor:2037 - Class class org.apache.iceberg.io.WriteResult cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2023-12-26 10:30:23 INFO  Configuration:804 - Config uses fallback configuration key 'rest.port' instead of key 'rest.bind-port'
2023-12-26 10:30:23 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
2023-12-26 10:30:23 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
2023-12-26 10:30:23 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
2023-12-26 10:30:23 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
2023-12-26 10:30:23 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
2023-12-26 10:30:23 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
2023-12-26 10:30:23 INFO  MiniCluster:269 - Starting Flink Mini Cluster
2023-12-26 10:30:23 INFO  MiniCluster:279 - Starting Metrics Registry
2023-12-26 10:30:23 INFO  MetricRegistryImpl:126 - No metrics reporter configured, no metrics will be exposed/reported.
2023-12-26 10:30:23 INFO  MiniCluster:283 - Starting RPC Service(s)
2023-12-26 10:30:23 INFO  AkkaRpcServiceUtils:265 - Trying to start local actor system
2023-12-26 10:30:23 INFO  Slf4jLogger:92 - Slf4jLogger started
2023-12-26 10:30:23 INFO  AkkaRpcServiceUtils:298 - Actor system started at akka://flink
2023-12-26 10:30:23 INFO  AkkaRpcServiceUtils:265 - Trying to start local actor system
2023-12-26 10:30:23 INFO  Slf4jLogger:92 - Slf4jLogger started
2023-12-26 10:30:23 INFO  AkkaRpcServiceUtils:298 - Actor system started at akka://flink-metrics
2023-12-26 10:30:23 INFO  AkkaRpcService:232 - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
2023-12-26 10:30:23 INFO  MiniCluster:487 - Starting high-availability services
2023-12-26 10:30:23 INFO  BlobServer:138 - Created BLOB server storage directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/blobStore-a5b3ecbf-25b9-4f96-b07f-e37a30e24412
2023-12-26 10:30:23 INFO  BlobServer:213 - Started BLOB server at 0.0.0.0:58095 - max concurrent requests: 50 - max backlog: 1000
2023-12-26 10:30:23 INFO  PermanentBlobCache:90 - Created BLOB cache storage directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/blobStore-22225c21-0ee8-446a-9286-c676276f481d
2023-12-26 10:30:23 INFO  TransientBlobCache:90 - Created BLOB cache storage directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/blobStore-e4aa7229-5927-4043-83f5-4df9f42b4422
2023-12-26 10:30:23 INFO  MiniCluster:606 - Starting 1 TaskManger(s)
2023-12-26 10:30:23 INFO  TaskManagerRunner:474 - Starting TaskManager with ResourceID: 83f9e70c-9728-4659-887f-cb09e984cb47
2023-12-26 10:30:23 INFO  TaskManagerServices:441 - Temporary file directory '/var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T': total 926 GB, usable 342 GB (36.93% usable)
2023-12-26 10:30:23 INFO  FileChannelManagerImpl:98 - FileChannelManager uses directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-io-87b27a90-0961-4020-8d4d-0f3a8fccffbc for spill files.
2023-12-26 10:30:23 INFO  FileChannelManagerImpl:98 - FileChannelManager uses directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-netty-shuffle-c06a278b-9e28-4248-9334-674d14c731a8 for spill files.
2023-12-26 10:30:23 INFO  NetworkBufferPool:145 - Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
2023-12-26 10:30:23 INFO  NettyShuffleEnvironment:328 - Starting the network environment and its components.
2023-12-26 10:30:23 INFO  KvStateService:92 - Starting the kvState service and its components.
2023-12-26 10:30:23 INFO  AkkaRpcService:232 - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
2023-12-26 10:30:23 INFO  DefaultJobLeaderService:123 - Start job leader service.
2023-12-26 10:30:23 INFO  FileCache:116 - User file cache uses directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-dist-cache-2501b30b-61a1-45ac-b8af-e238d2f9f114
2023-12-26 10:30:24 INFO  Configuration:804 - Config uses fallback configuration key 'rest.port' instead of key 'rest.bind-port'
2023-12-26 10:30:24 INFO  DispatcherRestEndpoint:139 - Starting rest endpoint.
2023-12-26 10:30:24 WARN  WebMonitorUtils:82 - Log file environment variable 'log.file' is not set.
2023-12-26 10:30:24 WARN  WebMonitorUtils:88 - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
2023-12-26 10:30:24 INFO  DispatcherRestEndpoint:250 - Rest endpoint listening at localhost:18081
2023-12-26 10:30:24 INFO  EmbeddedLeaderService:308 - Proposing leadership to contender http://localhost:18081
2023-12-26 10:30:24 INFO  DispatcherRestEndpoint:936 - Web frontend listening at http://localhost:18081.
2023-12-26 10:30:24 INFO  DispatcherRestEndpoint:994 - http://localhost:18081 was granted leadership with leaderSessionID=b13c9882-aa84-41b8-b545-6adf0856e50b
2023-12-26 10:30:24 INFO  EmbeddedLeaderService:256 - Received confirmation of leadership for leader http://localhost:18081 , session=b13c9882-aa84-41b8-b545-6adf0856e50b
2023-12-26 10:30:24 INFO  AkkaRpcService:232 - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
2023-12-26 10:30:24 INFO  EmbeddedLeaderService:308 - Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
2023-12-26 10:30:24 INFO  StandaloneResourceManager:234 - Starting the resource manager.
2023-12-26 10:30:24 INFO  EmbeddedLeaderService:308 - Proposing leadership to contender LeaderContender: StandaloneResourceManager
2023-12-26 10:30:24 INFO  DefaultDispatcherRunner:107 - DefaultDispatcherRunner was granted leadership with leader id bfdd6355-c7fe-4ecd-bf4e-a848ae9aaa2f. Creating new DispatcherLeaderProcess.
2023-12-26 10:30:24 INFO  StandaloneResourceManager:1230 - ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token b7878672d93b24f0790a404bd5814aad
2023-12-26 10:30:24 INFO  MiniCluster:413 - Flink Mini Cluster started successfully
2023-12-26 10:30:24 INFO  SessionDispatcherLeaderProcess:97 - Start SessionDispatcherLeaderProcess.
2023-12-26 10:30:24 INFO  SessionDispatcherLeaderProcess:117 - Recover all persisted job graphs.
2023-12-26 10:30:24 INFO  SessionDispatcherLeaderProcess:125 - Successfully recovered 0 persisted job graphs.
2023-12-26 10:30:24 INFO  EmbeddedLeaderService:256 - Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=790a404b-d581-4aad-b787-8672d93b24f0
2023-12-26 10:30:24 INFO  TaskExecutor:1293 - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(b7878672d93b24f0790a404bd5814aad).
2023-12-26 10:30:24 INFO  AkkaRpcService:232 - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
2023-12-26 10:30:24 INFO  EmbeddedLeaderService:256 - Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=bfdd6355-c7fe-4ecd-bf4e-a848ae9aaa2f
2023-12-26 10:30:24 INFO  TaskExecutor:162 - Resolved ResourceManager address, beginning registration
2023-12-26 10:30:24 INFO  StandaloneResourceManager:982 - Registering TaskManager with ResourceID 83f9e70c-9728-4659-887f-cb09e984cb47 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
2023-12-26 10:30:24 INFO  TaskExecutor:99 - Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id b8f6a5fa40a3698829bb84adc79f8344.
2023-12-26 10:30:24 INFO  StandaloneDispatcher:300 - Received JobGraph submission da35b83ed2a1a640ae5005270ab42a67 (insert-into_iceberg_catalog.db1.ice_T01).
2023-12-26 10:30:24 INFO  StandaloneDispatcher:362 - Submitting job da35b83ed2a1a640ae5005270ab42a67 (insert-into_iceberg_catalog.db1.ice_T01).
2023-12-26 10:30:24 INFO  EmbeddedLeaderService:308 - Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
2023-12-26 10:30:24 INFO  AkkaRpcService:232 - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
2023-12-26 10:30:24 INFO  JobMaster:289 - Initializing job insert-into_iceberg_catalog.db1.ice_T01 (da35b83ed2a1a640ae5005270ab42a67).
2023-12-26 10:30:24 INFO  JobMaster:98 - Using restart back off time strategy FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2147483647, backoffTimeMS=1000) for insert-into_iceberg_catalog.db1.ice_T01 (da35b83ed2a1a640ae5005270ab42a67).
2023-12-26 10:30:24 INFO  JobMaster:159 - Running initialization on master for job insert-into_iceberg_catalog.db1.ice_T01 (da35b83ed2a1a640ae5005270ab42a67).
2023-12-26 10:30:24 INFO  JobMaster:183 - Successfully ran initialization on master in 0 ms.
2023-12-26 10:30:24 INFO  DefaultExecutionTopology:271 - Built 1 pipelined regions in 3 ms
2023-12-26 10:30:24 INFO  JobMaster:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5249fb43
2023-12-26 10:30:24 INFO  JobMaster:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:24 INFO  CheckpointCoordinator:1532 - No checkpoint found during restore.
2023-12-26 10:30:24 INFO  JobMaster:145 - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@1aaf2b9e for insert-into_iceberg_catalog.db1.ice_T01 (da35b83ed2a1a640ae5005270ab42a67).
2023-12-26 10:30:24 INFO  EmbeddedLeaderService:256 - Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=96291bb1-da93-4567-b823-f51a0f1c6ffa
2023-12-26 10:30:24 INFO  JobMaster:867 - Starting execution of job insert-into_iceberg_catalog.db1.ice_T01 (da35b83ed2a1a640ae5005270ab42a67) under job master id b823f51a0f1c6ffa96291bb1da934567.
2023-12-26 10:30:24 INFO  JobMaster:183 - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
2023-12-26 10:30:24 INFO  ExecutionGraph:1039 - Job insert-into_iceberg_catalog.db1.ice_T01 (da35b83ed2a1a640ae5005270ab42a67) switched from state CREATED to RUNNING.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) (5570ccdc081e9b5b8770abdaf59355c0) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12) (f725c20d35d5a9c64e0822292a33f12d) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12) (6fba0aa0b13d3d230f447674226d8888) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12) (12d2d2a9d35128c4916303cc3fe26871) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12) (d82a9916df449c644374a744987e0ad8) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12) (f7e85b139ccb75c45a5073fcc33754ae) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12) (96b391b09dae97769a2f235bcccc7ae0) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12) (7257e5c1614e58f85056c59cf823c280) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12) (ff95be03a753640350b2768ca0fb9045) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12) (f3eb0b87a5ff932546aa1a9333477b35) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12) (8cf1f84ab595a58d7b4a67ef053f56a0) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12) (b0a95c24a267dc250d0dc04589006750) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (1/12) (08d51be3d78ad3e6eb9a9e5cbbf037fd) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (2/12) (ade2d6b8262b316633d49c24a913c106) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (3/12) (a5a185d11659f1b582e954f6e784d75e) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (4/12) (1de0f245870c746ac1d2a81f125b16c4) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (5/12) (bcf82538594a92f8ca5c1f9d9afcbb36) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (6/12) (e0803636f756594523517f2b0d1f1da7) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (7/12) (0f831be2913e337d3758bf7adc05c7ef) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (8/12) (073462d5ce03a522a959f9c2460a5402) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (9/12) (8a43eec0b08ddd67567c077865668a64) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (10/12) (c93929d55b40bcbec8e4da6076dd73dc) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (11/12) (c8474662d9861faf33d8d0c4ff775267) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (12/12) (6af176c5dd2082ebe9c48a06454fb56f) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1) (c1e03f807afb54a1bb70ff5573b41d3c) switched from CREATED to SCHEDULED.
2023-12-26 10:30:24 INFO  JobMaster:1040 - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(b7878672d93b24f0790a404bd5814aad)
2023-12-26 10:30:24 INFO  JobMaster:162 - Resolved ResourceManager address, beginning registration
2023-12-26 10:30:24 INFO  StandaloneResourceManager:355 - Registering job manager b823f51a0f1c6ffa96291bb1da934567@akka://flink/user/rpc/jobmanager_3 for job da35b83ed2a1a640ae5005270ab42a67.
2023-12-26 10:30:24 INFO  StandaloneResourceManager:909 - Registered job manager b823f51a0f1c6ffa96291bb1da934567@akka://flink/user/rpc/jobmanager_3 for job da35b83ed2a1a640ae5005270ab42a67.
2023-12-26 10:30:24 INFO  JobMaster:1064 - JobManager successfully registered at ResourceManager, leader id: b7878672d93b24f0790a404bd5814aad.
2023-12-26 10:30:24 INFO  DeclarativeSlotManager:263 - Received resource requirements from job da35b83ed2a1a640ae5005270ab42a67: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=12}]
2023-12-26 10:30:24 INFO  TaskExecutor:1027 - Receive slot request b46ea8fcb66a48b423893bb1a88f9daf for job da35b83ed2a1a640ae5005270ab42a67 from resource manager with leader id b7878672d93b24f0790a404bd5814aad.
2023-12-26 10:30:24 INFO  TaskExecutor:1104 - Allocated slot for b46ea8fcb66a48b423893bb1a88f9daf.
2023-12-26 10:30:24 INFO  DefaultJobLeaderService:188 - Add job da35b83ed2a1a640ae5005270ab42a67 for job leader monitoring.
2023-12-26 10:30:24 INFO  DefaultJobLeaderService:346 - Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 96291bb1-da93-4567-b823-f51a0f1c6ffa.
2023-12-26 10:30:24 INFO  TaskExecutor:1027 - Receive slot request 6d17f38b54b06011b09026cd15a6ec55 for job da35b83ed2a1a640ae5005270ab42a67 from resource manager with leader id b7878672d93b24f0790a404bd5814aad.
2023-12-26 10:30:24 INFO  TaskExecutor:1104 - Allocated slot for 6d17f38b54b06011b09026cd15a6ec55.
2023-12-26 10:30:24 INFO  TaskExecutor:1027 - Receive slot request 2d1a71231364bfb5d6523b79da66a28b for job da35b83ed2a1a640ae5005270ab42a67 from resource manager with leader id b7878672d93b24f0790a404bd5814aad.
2023-12-26 10:30:24 INFO  TaskExecutor:1104 - Allocated slot for 2d1a71231364bfb5d6523b79da66a28b.
2023-12-26 10:30:24 INFO  TaskExecutor:1027 - Receive slot request b6df3d61500df4a389955e5b112825bf for job da35b83ed2a1a640ae5005270ab42a67 from resource manager with leader id b7878672d93b24f0790a404bd5814aad.
2023-12-26 10:30:24 INFO  TaskExecutor:1104 - Allocated slot for b6df3d61500df4a389955e5b112825bf.
2023-12-26 10:30:24 INFO  TaskExecutor:1027 - Receive slot request bad107b2030061e0b1634bd15005d806 for job da35b83ed2a1a640ae5005270ab42a67 from resource manager with leader id b7878672d93b24f0790a404bd5814aad.
2023-12-26 10:30:24 INFO  TaskExecutor:1104 - Allocated slot for bad107b2030061e0b1634bd15005d806.
2023-12-26 10:30:24 INFO  TaskExecutor:1027 - Receive slot request 31335638c84e938dce70ef7cee2bf351 for job da35b83ed2a1a640ae5005270ab42a67 from resource manager with leader id b7878672d93b24f0790a404bd5814aad.
2023-12-26 10:30:24 INFO  TaskExecutor:1104 - Allocated slot for 31335638c84e938dce70ef7cee2bf351.
2023-12-26 10:30:24 INFO  DefaultJobLeaderService:162 - Resolved JobManager address, beginning registration
2023-12-26 10:30:24 INFO  TaskExecutor:1027 - Receive slot request ae7092f53a032f38d9253004787f63de for job da35b83ed2a1a640ae5005270ab42a67 from resource manager with leader id b7878672d93b24f0790a404bd5814aad.
2023-12-26 10:30:24 INFO  TaskExecutor:1104 - Allocated slot for ae7092f53a032f38d9253004787f63de.
2023-12-26 10:30:24 INFO  TaskExecutor:1027 - Receive slot request f1594721c899156ea75a3311d3b355f0 for job da35b83ed2a1a640ae5005270ab42a67 from resource manager with leader id b7878672d93b24f0790a404bd5814aad.
2023-12-26 10:30:24 INFO  TaskExecutor:1104 - Allocated slot for f1594721c899156ea75a3311d3b355f0.
2023-12-26 10:30:24 INFO  TaskExecutor:1027 - Receive slot request 7d9128e040db8894f3205da0863d6ac1 for job da35b83ed2a1a640ae5005270ab42a67 from resource manager with leader id b7878672d93b24f0790a404bd5814aad.
2023-12-26 10:30:24 INFO  TaskExecutor:1104 - Allocated slot for 7d9128e040db8894f3205da0863d6ac1.
2023-12-26 10:30:24 INFO  TaskExecutor:1027 - Receive slot request e90cdf1216a16cf6d27a947ac723c8b7 for job da35b83ed2a1a640ae5005270ab42a67 from resource manager with leader id b7878672d93b24f0790a404bd5814aad.
2023-12-26 10:30:24 INFO  TaskExecutor:1104 - Allocated slot for e90cdf1216a16cf6d27a947ac723c8b7.
2023-12-26 10:30:24 INFO  TaskExecutor:1027 - Receive slot request 544d9f882773f620f97f80fce69f7ad6 for job da35b83ed2a1a640ae5005270ab42a67 from resource manager with leader id b7878672d93b24f0790a404bd5814aad.
2023-12-26 10:30:24 INFO  TaskExecutor:1104 - Allocated slot for 544d9f882773f620f97f80fce69f7ad6.
2023-12-26 10:30:24 INFO  TaskExecutor:1027 - Receive slot request 70360b605862556c64c3de1cd640d376 for job da35b83ed2a1a640ae5005270ab42a67 from resource manager with leader id b7878672d93b24f0790a404bd5814aad.
2023-12-26 10:30:24 INFO  TaskExecutor:1104 - Allocated slot for 70360b605862556c64c3de1cd640d376.
2023-12-26 10:30:24 INFO  DefaultJobLeaderService:413 - Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job da35b83ed2a1a640ae5005270ab42a67.
2023-12-26 10:30:24 INFO  TaskExecutor:1608 - Establish JobManager connection for job da35b83ed2a1a640ae5005270ab42a67.
2023-12-26 10:30:24 INFO  TaskExecutor:1459 - Offer reserved slots to the leader of job da35b83ed2a1a640ae5005270ab42a67.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) (5570ccdc081e9b5b8770abdaf59355c0) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) (attempt #0) with attempt id 5570ccdc081e9b5b8770abdaf59355c0 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id f1594721c899156ea75a3311d3b355f0
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12) (f725c20d35d5a9c64e0822292a33f12d) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12) (attempt #0) with attempt id f725c20d35d5a9c64e0822292a33f12d to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id ae7092f53a032f38d9253004787f63de
2023-12-26 10:30:24 INFO  TaskSlotTableImpl:388 - Activate slot f1594721c899156ea75a3311d3b355f0.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12) (6fba0aa0b13d3d230f447674226d8888) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12) (attempt #0) with attempt id 6fba0aa0b13d3d230f447674226d8888 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id 544d9f882773f620f97f80fce69f7ad6
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12) (12d2d2a9d35128c4916303cc3fe26871) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12) (attempt #0) with attempt id 12d2d2a9d35128c4916303cc3fe26871 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id 7d9128e040db8894f3205da0863d6ac1
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12) (d82a9916df449c644374a744987e0ad8) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12) (attempt #0) with attempt id d82a9916df449c644374a744987e0ad8 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id bad107b2030061e0b1634bd15005d806
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12) (f7e85b139ccb75c45a5073fcc33754ae) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12) (attempt #0) with attempt id f7e85b139ccb75c45a5073fcc33754ae to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id 70360b605862556c64c3de1cd640d376
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12) (96b391b09dae97769a2f235bcccc7ae0) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12) (attempt #0) with attempt id 96b391b09dae97769a2f235bcccc7ae0 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id b46ea8fcb66a48b423893bb1a88f9daf
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12) (7257e5c1614e58f85056c59cf823c280) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12) (attempt #0) with attempt id 7257e5c1614e58f85056c59cf823c280 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id 31335638c84e938dce70ef7cee2bf351
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12) (ff95be03a753640350b2768ca0fb9045) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12) (attempt #0) with attempt id ff95be03a753640350b2768ca0fb9045 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id b6df3d61500df4a389955e5b112825bf
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12) (f3eb0b87a5ff932546aa1a9333477b35) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12) (attempt #0) with attempt id f3eb0b87a5ff932546aa1a9333477b35 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id 6d17f38b54b06011b09026cd15a6ec55
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12) (8cf1f84ab595a58d7b4a67ef053f56a0) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12) (attempt #0) with attempt id 8cf1f84ab595a58d7b4a67ef053f56a0 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id 2d1a71231364bfb5d6523b79da66a28b
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12) (b0a95c24a267dc250d0dc04589006750) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12) (attempt #0) with attempt id b0a95c24a267dc250d0dc04589006750 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id e90cdf1216a16cf6d27a947ac723c8b7
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (1/12) (08d51be3d78ad3e6eb9a9e5cbbf037fd) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (1/12) (attempt #0) with attempt id 08d51be3d78ad3e6eb9a9e5cbbf037fd to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id f1594721c899156ea75a3311d3b355f0
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (2/12) (ade2d6b8262b316633d49c24a913c106) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (2/12) (attempt #0) with attempt id ade2d6b8262b316633d49c24a913c106 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id ae7092f53a032f38d9253004787f63de
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (3/12) (a5a185d11659f1b582e954f6e784d75e) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (3/12) (attempt #0) with attempt id a5a185d11659f1b582e954f6e784d75e to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id 544d9f882773f620f97f80fce69f7ad6
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (4/12) (1de0f245870c746ac1d2a81f125b16c4) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (4/12) (attempt #0) with attempt id 1de0f245870c746ac1d2a81f125b16c4 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id 7d9128e040db8894f3205da0863d6ac1
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (5/12) (bcf82538594a92f8ca5c1f9d9afcbb36) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (5/12) (attempt #0) with attempt id bcf82538594a92f8ca5c1f9d9afcbb36 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id bad107b2030061e0b1634bd15005d806
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (6/12) (e0803636f756594523517f2b0d1f1da7) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (6/12) (attempt #0) with attempt id e0803636f756594523517f2b0d1f1da7 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id 70360b605862556c64c3de1cd640d376
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (7/12) (0f831be2913e337d3758bf7adc05c7ef) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (7/12) (attempt #0) with attempt id 0f831be2913e337d3758bf7adc05c7ef to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id b46ea8fcb66a48b423893bb1a88f9daf
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (8/12) (073462d5ce03a522a959f9c2460a5402) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (8/12) (attempt #0) with attempt id 073462d5ce03a522a959f9c2460a5402 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id 31335638c84e938dce70ef7cee2bf351
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (9/12) (8a43eec0b08ddd67567c077865668a64) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (9/12) (attempt #0) with attempt id 8a43eec0b08ddd67567c077865668a64 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id b6df3d61500df4a389955e5b112825bf
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (10/12) (c93929d55b40bcbec8e4da6076dd73dc) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (10/12) (attempt #0) with attempt id c93929d55b40bcbec8e4da6076dd73dc to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id 6d17f38b54b06011b09026cd15a6ec55
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (11/12) (c8474662d9861faf33d8d0c4ff775267) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (11/12) (attempt #0) with attempt id c8474662d9861faf33d8d0c4ff775267 to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id 2d1a71231364bfb5d6523b79da66a28b
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergStreamWriter (12/12) (6af176c5dd2082ebe9c48a06454fb56f) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (12/12) (attempt #0) with attempt id 6af176c5dd2082ebe9c48a06454fb56f to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id e90cdf1216a16cf6d27a947ac723c8b7
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1) (c1e03f807afb54a1bb70ff5573b41d3c) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:30:24 INFO  ExecutionGraph:571 - Deploying IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1) (attempt #0) with attempt id c1e03f807afb54a1bb70ff5573b41d3c to 83f9e70c-9728-4659-887f-cb09e984cb47 @ localhost (dataPort=-1) with allocation id f1594721c899156ea75a3311d3b355f0
2023-12-26 10:30:24 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12)#0 (5570ccdc081e9b5b8770abdaf59355c0), deploy into slot with allocation id f1594721c899156ea75a3311d3b355f0.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12)#0 (5570ccdc081e9b5b8770abdaf59355c0) switched from CREATED to DEPLOYING.
2023-12-26 10:30:24 INFO  TaskSlotTableImpl:388 - Activate slot ae7092f53a032f38d9253004787f63de.
2023-12-26 10:30:24 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12)#0 (5570ccdc081e9b5b8770abdaf59355c0) [DEPLOYING].
2023-12-26 10:30:24 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12)#0 (f725c20d35d5a9c64e0822292a33f12d), deploy into slot with allocation id ae7092f53a032f38d9253004787f63de.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12)#0 (f725c20d35d5a9c64e0822292a33f12d) switched from CREATED to DEPLOYING.
2023-12-26 10:30:24 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12)#0 (f725c20d35d5a9c64e0822292a33f12d) [DEPLOYING].
2023-12-26 10:30:24 INFO  TaskSlotTableImpl:388 - Activate slot 544d9f882773f620f97f80fce69f7ad6.
2023-12-26 10:30:24 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12)#0 (6fba0aa0b13d3d230f447674226d8888), deploy into slot with allocation id 544d9f882773f620f97f80fce69f7ad6.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12)#0 (6fba0aa0b13d3d230f447674226d8888) switched from CREATED to DEPLOYING.
2023-12-26 10:30:24 INFO  TaskSlotTableImpl:388 - Activate slot 7d9128e040db8894f3205da0863d6ac1.
2023-12-26 10:30:24 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12)#0 (6fba0aa0b13d3d230f447674226d8888) [DEPLOYING].
2023-12-26 10:30:24 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12)#0 (12d2d2a9d35128c4916303cc3fe26871), deploy into slot with allocation id 7d9128e040db8894f3205da0863d6ac1.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12)#0 (12d2d2a9d35128c4916303cc3fe26871) switched from CREATED to DEPLOYING.
2023-12-26 10:30:24 INFO  TaskSlotTableImpl:388 - Activate slot 70360b605862556c64c3de1cd640d376.
2023-12-26 10:30:24 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12)#0 (12d2d2a9d35128c4916303cc3fe26871) [DEPLOYING].
2023-12-26 10:30:24 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12)#0 (f7e85b139ccb75c45a5073fcc33754ae), deploy into slot with allocation id 70360b605862556c64c3de1cd640d376.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12)#0 (f7e85b139ccb75c45a5073fcc33754ae) switched from CREATED to DEPLOYING.
2023-12-26 10:30:24 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12)#0 (f7e85b139ccb75c45a5073fcc33754ae) [DEPLOYING].
2023-12-26 10:30:24 INFO  TaskSlotTableImpl:388 - Activate slot b46ea8fcb66a48b423893bb1a88f9daf.
2023-12-26 10:30:24 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12)#0 (96b391b09dae97769a2f235bcccc7ae0), deploy into slot with allocation id b46ea8fcb66a48b423893bb1a88f9daf.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12)#0 (96b391b09dae97769a2f235bcccc7ae0) switched from CREATED to DEPLOYING.
2023-12-26 10:30:24 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12)#0 (96b391b09dae97769a2f235bcccc7ae0) [DEPLOYING].
2023-12-26 10:30:24 INFO  TaskSlotTableImpl:388 - Activate slot bad107b2030061e0b1634bd15005d806.
2023-12-26 10:30:24 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12)#0 (d82a9916df449c644374a744987e0ad8), deploy into slot with allocation id bad107b2030061e0b1634bd15005d806.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12)#0 (d82a9916df449c644374a744987e0ad8) switched from CREATED to DEPLOYING.
2023-12-26 10:30:24 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12)#0 (d82a9916df449c644374a744987e0ad8) [DEPLOYING].
2023-12-26 10:30:24 INFO  TaskSlotTableImpl:388 - Activate slot 31335638c84e938dce70ef7cee2bf351.
2023-12-26 10:30:24 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12)#0 (7257e5c1614e58f85056c59cf823c280), deploy into slot with allocation id 31335638c84e938dce70ef7cee2bf351.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12)#0 (7257e5c1614e58f85056c59cf823c280) switched from CREATED to DEPLOYING.
2023-12-26 10:30:24 INFO  TaskSlotTableImpl:388 - Activate slot b6df3d61500df4a389955e5b112825bf.
2023-12-26 10:30:24 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6cd8a866
2023-12-26 10:30:24 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@49729f7b
2023-12-26 10:30:24 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@46a51ec
2023-12-26 10:30:24 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@59f9e3a4
2023-12-26 10:30:24 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@63d92a55
2023-12-26 10:30:24 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@62d24be
2023-12-26 10:30:24 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:24 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:24 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:24 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:24 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:24 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:24 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12)#0 (ff95be03a753640350b2768ca0fb9045), deploy into slot with allocation id b6df3d61500df4a389955e5b112825bf.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12)#0 (ff95be03a753640350b2768ca0fb9045) switched from CREATED to DEPLOYING.
2023-12-26 10:30:24 INFO  TaskSlotTableImpl:388 - Activate slot 6d17f38b54b06011b09026cd15a6ec55.
2023-12-26 10:30:24 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12)#0 (ff95be03a753640350b2768ca0fb9045) [DEPLOYING].
2023-12-26 10:30:24 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12)#0 (7257e5c1614e58f85056c59cf823c280) [DEPLOYING].
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12)#0 (5570ccdc081e9b5b8770abdaf59355c0) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12)#0 (96b391b09dae97769a2f235bcccc7ae0) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12)#0 (6fba0aa0b13d3d230f447674226d8888) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12)#0 (f725c20d35d5a9c64e0822292a33f12d) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12)#0 (12d2d2a9d35128c4916303cc3fe26871) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12)#0 (f7e85b139ccb75c45a5073fcc33754ae) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:24 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12)#0 (f3eb0b87a5ff932546aa1a9333477b35), deploy into slot with allocation id 6d17f38b54b06011b09026cd15a6ec55.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12)#0 (f3eb0b87a5ff932546aa1a9333477b35) switched from CREATED to DEPLOYING.
2023-12-26 10:30:24 INFO  TaskSlotTableImpl:388 - Activate slot 2d1a71231364bfb5d6523b79da66a28b.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) (5570ccdc081e9b5b8770abdaf59355c0) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:24 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12)#0 (f3eb0b87a5ff932546aa1a9333477b35) [DEPLOYING].
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12) (96b391b09dae97769a2f235bcccc7ae0) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12) (6fba0aa0b13d3d230f447674226d8888) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12) (f725c20d35d5a9c64e0822292a33f12d) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12) (12d2d2a9d35128c4916303cc3fe26871) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12) (f7e85b139ccb75c45a5073fcc33754ae) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:24 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7bc8a0bc
2023-12-26 10:30:24 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12)#0 (d82a9916df449c644374a744987e0ad8) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:24 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12) (d82a9916df449c644374a744987e0ad8) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:24 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12)#0 (8cf1f84ab595a58d7b4a67ef053f56a0), deploy into slot with allocation id 2d1a71231364bfb5d6523b79da66a28b.
2023-12-26 10:30:24 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12)#0 (8cf1f84ab595a58d7b4a67ef053f56a0) switched from CREATED to DEPLOYING.
2023-12-26 10:30:24 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12)#0 (8cf1f84ab595a58d7b4a67ef053f56a0) [DEPLOYING].
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot e90cdf1216a16cf6d27a947ac723c8b7.
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6485dd5e
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6b115f06
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1590462
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12)#0 (b0a95c24a267dc250d0dc04589006750), deploy into slot with allocation id e90cdf1216a16cf6d27a947ac723c8b7.
2023-12-26 10:30:25 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12)#0 (f3eb0b87a5ff932546aa1a9333477b35) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12)#0 (ff95be03a753640350b2768ca0fb9045) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12)#0 (b0a95c24a267dc250d0dc04589006750) switched from CREATED to DEPLOYING.
2023-12-26 10:30:25 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12)#0 (b0a95c24a267dc250d0dc04589006750) [DEPLOYING].
2023-12-26 10:30:25 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12)#0 (7257e5c1614e58f85056c59cf823c280) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12) (ff95be03a753640350b2768ca0fb9045) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12) (f3eb0b87a5ff932546aa1a9333477b35) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12) (7257e5c1614e58f85056c59cf823c280) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot f1594721c899156ea75a3311d3b355f0.
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2f39fc00
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12)#0 (8cf1f84ab595a58d7b4a67ef053f56a0) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12) (8cf1f84ab595a58d7b4a67ef053f56a0) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@63d6aa99
2023-12-26 10:30:25 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (1/12)#0 (08d51be3d78ad3e6eb9a9e5cbbf037fd), deploy into slot with allocation id f1594721c899156ea75a3311d3b355f0.
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (1/12)#0 (08d51be3d78ad3e6eb9a9e5cbbf037fd) switched from CREATED to DEPLOYING.
2023-12-26 10:30:25 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (1/12)#0 (08d51be3d78ad3e6eb9a9e5cbbf037fd) [DEPLOYING].
2023-12-26 10:30:25 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12)#0 (b0a95c24a267dc250d0dc04589006750) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot ae7092f53a032f38d9253004787f63de.
2023-12-26 10:30:25 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:30:25 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:30:25 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:30:25 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:30:25 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:30:25 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:30:25 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:30:25 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:30:25 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:30:25 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:30:25 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12) (b0a95c24a267dc250d0dc04589006750) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (2/12)#0 (ade2d6b8262b316633d49c24a913c106), deploy into slot with allocation id ae7092f53a032f38d9253004787f63de.
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (2/12)#0 (ade2d6b8262b316633d49c24a913c106) switched from CREATED to DEPLOYING.
2023-12-26 10:30:25 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (2/12)#0 (ade2d6b8262b316633d49c24a913c106) [DEPLOYING].
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot 544d9f882773f620f97f80fce69f7ad6.
2023-12-26 10:30:25 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (3/12)#0 (a5a185d11659f1b582e954f6e784d75e), deploy into slot with allocation id 544d9f882773f620f97f80fce69f7ad6.
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (3/12)#0 (a5a185d11659f1b582e954f6e784d75e) switched from CREATED to DEPLOYING.
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7216b290
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@dffa431
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (1/12)#0 (08d51be3d78ad3e6eb9a9e5cbbf037fd) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot 7d9128e040db8894f3205da0863d6ac1.
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (2/12)#0 (ade2d6b8262b316633d49c24a913c106) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - IcebergStreamWriter (1/12) (08d51be3d78ad3e6eb9a9e5cbbf037fd) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - IcebergStreamWriter (2/12) (ade2d6b8262b316633d49c24a913c106) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (3/12)#0 (a5a185d11659f1b582e954f6e784d75e) [DEPLOYING].
2023-12-26 10:30:25 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (4/12)#0 (1de0f245870c746ac1d2a81f125b16c4), deploy into slot with allocation id 7d9128e040db8894f3205da0863d6ac1.
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (4/12)#0 (1de0f245870c746ac1d2a81f125b16c4) switched from CREATED to DEPLOYING.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot bad107b2030061e0b1634bd15005d806.
2023-12-26 10:30:25 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (4/12)#0 (1de0f245870c746ac1d2a81f125b16c4) [DEPLOYING].
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@42ff644e
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1ee19261
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (5/12)#0 (bcf82538594a92f8ca5c1f9d9afcbb36), deploy into slot with allocation id bad107b2030061e0b1634bd15005d806.
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (3/12)#0 (a5a185d11659f1b582e954f6e784d75e) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (4/12)#0 (1de0f245870c746ac1d2a81f125b16c4) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (5/12)#0 (bcf82538594a92f8ca5c1f9d9afcbb36) switched from CREATED to DEPLOYING.
2023-12-26 10:30:25 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (5/12)#0 (bcf82538594a92f8ca5c1f9d9afcbb36) [DEPLOYING].
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - IcebergStreamWriter (3/12) (a5a185d11659f1b582e954f6e784d75e) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@603dc0a2
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot 70360b605862556c64c3de1cd640d376.
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (5/12)#0 (bcf82538594a92f8ca5c1f9d9afcbb36) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - IcebergStreamWriter (4/12) (1de0f245870c746ac1d2a81f125b16c4) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - IcebergStreamWriter (5/12) (bcf82538594a92f8ca5c1f9d9afcbb36) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (6/12)#0 (e0803636f756594523517f2b0d1f1da7), deploy into slot with allocation id 70360b605862556c64c3de1cd640d376.
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (6/12)#0 (e0803636f756594523517f2b0d1f1da7) switched from CREATED to DEPLOYING.
2023-12-26 10:30:25 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (6/12)#0 (e0803636f756594523517f2b0d1f1da7) [DEPLOYING].
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot b46ea8fcb66a48b423893bb1a88f9daf.
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5580c09c
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (6/12)#0 (e0803636f756594523517f2b0d1f1da7) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - IcebergStreamWriter (6/12) (e0803636f756594523517f2b0d1f1da7) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (7/12)#0 (0f831be2913e337d3758bf7adc05c7ef), deploy into slot with allocation id b46ea8fcb66a48b423893bb1a88f9daf.
2023-12-26 10:30:25 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 7 has no restore state.
2023-12-26 10:30:25 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 2 has no restore state.
2023-12-26 10:30:25 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 10 has no restore state.
2023-12-26 10:30:25 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 8 has no restore state.
2023-12-26 10:30:25 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 6 has no restore state.
2023-12-26 10:30:25 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 11 has no restore state.
2023-12-26 10:30:25 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 4 has no restore state.
2023-12-26 10:30:25 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 1 has no restore state.
2023-12-26 10:30:25 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 5 has no restore state.
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (7/12)#0 (0f831be2913e337d3758bf7adc05c7ef) switched from CREATED to DEPLOYING.
2023-12-26 10:30:25 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 3 has no restore state.
2023-12-26 10:30:25 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 0 has no restore state.
2023-12-26 10:30:25 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 9 has no restore state.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot 31335638c84e938dce70ef7cee2bf351.
2023-12-26 10:30:25 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (7/12)#0 (0f831be2913e337d3758bf7adc05c7ef) [DEPLOYING].
2023-12-26 10:30:25 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (8/12)#0 (073462d5ce03a522a959f9c2460a5402), deploy into slot with allocation id 31335638c84e938dce70ef7cee2bf351.
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (8/12)#0 (073462d5ce03a522a959f9c2460a5402) switched from CREATED to DEPLOYING.
2023-12-26 10:30:25 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (8/12)#0 (073462d5ce03a522a959f9c2460a5402) [DEPLOYING].
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot b6df3d61500df4a389955e5b112825bf.
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4cf61ead
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4831e80c
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (7/12)#0 (0f831be2913e337d3758bf7adc05c7ef) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (8/12)#0 (073462d5ce03a522a959f9c2460a5402) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (9/12)#0 (8a43eec0b08ddd67567c077865668a64), deploy into slot with allocation id b6df3d61500df4a389955e5b112825bf.
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (9/12)#0 (8a43eec0b08ddd67567c077865668a64) switched from CREATED to DEPLOYING.
2023-12-26 10:30:25 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (9/12)#0 (8a43eec0b08ddd67567c077865668a64) [DEPLOYING].
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot 6d17f38b54b06011b09026cd15a6ec55.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - IcebergStreamWriter (8/12) (073462d5ce03a522a959f9c2460a5402) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:25 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:25 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:25 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:25 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:25 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:25 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:25 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:25 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:25 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:25 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:25 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:25 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - IcebergStreamWriter (7/12) (0f831be2913e337d3758bf7adc05c7ef) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3e9ba1e6
2023-12-26 10:30:25 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (10/12)#0 (c93929d55b40bcbec8e4da6076dd73dc), deploy into slot with allocation id 6d17f38b54b06011b09026cd15a6ec55.
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (10/12)#0 (c93929d55b40bcbec8e4da6076dd73dc) switched from CREATED to DEPLOYING.
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (9/12)#0 (8a43eec0b08ddd67567c077865668a64) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (10/12)#0 (c93929d55b40bcbec8e4da6076dd73dc) [DEPLOYING].
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot 2d1a71231364bfb5d6523b79da66a28b.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - IcebergStreamWriter (9/12) (8a43eec0b08ddd67567c077865668a64) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@35165a23
2023-12-26 10:30:25 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (11/12)#0 (c8474662d9861faf33d8d0c4ff775267), deploy into slot with allocation id 2d1a71231364bfb5d6523b79da66a28b.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (11/12)#0 (c8474662d9861faf33d8d0c4ff775267) switched from CREATED to DEPLOYING.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot e90cdf1216a16cf6d27a947ac723c8b7.
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (10/12)#0 (c93929d55b40bcbec8e4da6076dd73dc) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (11/12)#0 (c8474662d9861faf33d8d0c4ff775267) [DEPLOYING].
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - IcebergStreamWriter (10/12) (c93929d55b40bcbec8e4da6076dd73dc) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:30:25 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (12/12)#0 (6af176c5dd2082ebe9c48a06454fb56f), deploy into slot with allocation id e90cdf1216a16cf6d27a947ac723c8b7.
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@e4f261e
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (12/12)#0 (6af176c5dd2082ebe9c48a06454fb56f) switched from CREATED to DEPLOYING.
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot f1594721c899156ea75a3311d3b355f0.
2023-12-26 10:30:25 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (12/12)#0 (6af176c5dd2082ebe9c48a06454fb56f) [DEPLOYING].
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (11/12)#0 (c8474662d9861faf33d8d0c4ff775267) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - IcebergStreamWriter (11/12) (c8474662d9861faf33d8d0c4ff775267) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  TaskExecutor:722 - Received task IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1)#0 (c1e03f807afb54a1bb70ff5573b41d3c), deploy into slot with allocation id f1594721c899156ea75a3311d3b355f0.
2023-12-26 10:30:25 INFO  Task:1067 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1)#0 (c1e03f807afb54a1bb70ff5573b41d3c) switched from CREATED to DEPLOYING.
2023-12-26 10:30:25 INFO  Task:626 - Loading JAR files for task IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1)#0 (c1e03f807afb54a1bb70ff5573b41d3c) [DEPLOYING].
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot 70360b605862556c64c3de1cd640d376.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot bad107b2030061e0b1634bd15005d806.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot 7d9128e040db8894f3205da0863d6ac1.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot 6d17f38b54b06011b09026cd15a6ec55.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot b46ea8fcb66a48b423893bb1a88f9daf.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot f1594721c899156ea75a3311d3b355f0.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot b6df3d61500df4a389955e5b112825bf.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot 544d9f882773f620f97f80fce69f7ad6.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot e90cdf1216a16cf6d27a947ac723c8b7.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot 2d1a71231364bfb5d6523b79da66a28b.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot 31335638c84e938dce70ef7cee2bf351.
2023-12-26 10:30:25 INFO  TaskSlotTableImpl:388 - Activate slot ae7092f53a032f38d9253004787f63de.
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@fc3cab4
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  Task:1067 - IcebergStreamWriter (12/12)#0 (6af176c5dd2082ebe9c48a06454fb56f) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - IcebergStreamWriter (12/12) (6af176c5dd2082ebe9c48a06454fb56f) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1cd9f339
2023-12-26 10:30:25 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:30:25 INFO  Task:1067 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1)#0 (c1e03f807afb54a1bb70ff5573b41d3c) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  ExecutionGraph:1438 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1) (c1e03f807afb54a1bb70ff5573b41d3c) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:30:25 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:30:26 INFO  Task:1067 - IcebergStreamWriter (9/12)#0 (8a43eec0b08ddd67567c077865668a64) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - IcebergStreamWriter (10/12)#0 (c93929d55b40bcbec8e4da6076dd73dc) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - IcebergStreamWriter (7/12)#0 (0f831be2913e337d3758bf7adc05c7ef) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - IcebergStreamWriter (2/12)#0 (ade2d6b8262b316633d49c24a913c106) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - IcebergStreamWriter (12/12)#0 (6af176c5dd2082ebe9c48a06454fb56f) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - IcebergStreamWriter (6/12)#0 (e0803636f756594523517f2b0d1f1da7) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - IcebergStreamWriter (1/12)#0 (08d51be3d78ad3e6eb9a9e5cbbf037fd) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - IcebergStreamWriter (3/12)#0 (a5a185d11659f1b582e954f6e784d75e) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - IcebergStreamWriter (8/12)#0 (073462d5ce03a522a959f9c2460a5402) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - IcebergStreamWriter (5/12)#0 (bcf82538594a92f8ca5c1f9d9afcbb36) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - IcebergStreamWriter (11/12)#0 (c8474662d9861faf33d8d0c4ff775267) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - IcebergStreamWriter (4/12)#0 (1de0f245870c746ac1d2a81f125b16c4) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - IcebergStreamWriter (9/12) (8a43eec0b08ddd67567c077865668a64) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - IcebergStreamWriter (2/12) (ade2d6b8262b316633d49c24a913c106) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - IcebergStreamWriter (6/12) (e0803636f756594523517f2b0d1f1da7) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - IcebergStreamWriter (7/12) (0f831be2913e337d3758bf7adc05c7ef) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - IcebergStreamWriter (8/12) (073462d5ce03a522a959f9c2460a5402) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - IcebergStreamWriter (5/12) (bcf82538594a92f8ca5c1f9d9afcbb36) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - IcebergStreamWriter (11/12) (c8474662d9861faf33d8d0c4ff775267) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - IcebergStreamWriter (4/12) (1de0f245870c746ac1d2a81f125b16c4) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - IcebergStreamWriter (10/12) (c93929d55b40bcbec8e4da6076dd73dc) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - IcebergStreamWriter (3/12) (a5a185d11659f1b582e954f6e784d75e) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - IcebergStreamWriter (12/12) (6af176c5dd2082ebe9c48a06454fb56f) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - IcebergStreamWriter (1/12) (08d51be3d78ad3e6eb9a9e5cbbf037fd) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:26 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:26 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:26 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:26 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:26 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:26 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:26 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:26 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:26 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:26 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:26 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:26 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:26 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:26 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557826077
2023-12-26 10:30:26 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:26 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:26 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557826077
2023-12-26 10:30:26 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:26 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:26 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557826077
2023-12-26 10:30:26 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:26 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:26 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557826077
2023-12-26 10:30:26 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:26 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:26 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557826077
2023-12-26 10:30:26 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:26 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:26 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557826079
2023-12-26 10:30:26 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:26 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:26 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557826079
2023-12-26 10:30:26 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:26 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:26 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557826079
2023-12-26 10:30:26 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:26 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:26 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557826079
2023-12-26 10:30:26 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:26 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:26 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557826080
2023-12-26 10:30:26 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:26 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:26 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557826080
2023-12-26 10:30:26 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:26 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:26 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557826080
2023-12-26 10:30:26 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00000-19ee13a9-f0b9-4a30-aadf-bac93fa20a0e.metadata.json
2023-12-26 10:30:26 INFO  BaseMetastoreCatalog:64 - Table loaded by catalog: iceberg_catalog.db1.ice_T01
2023-12-26 10:30:26 INFO  Task:1067 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1)#0 (c1e03f807afb54a1bb70ff5573b41d3c) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1) (c1e03f807afb54a1bb70ff5573b41d3c) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Metadata:259 - [Consumer clientId=consumer-g1-4, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:30:26 INFO  Metadata:259 - [Consumer clientId=consumer-g1-3, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:30:26 INFO  Metadata:259 - [Consumer clientId=consumer-g1-8, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:30:26 INFO  Metadata:259 - [Consumer clientId=consumer-g1-1, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:30:26 INFO  Metadata:259 - [Consumer clientId=consumer-g1-10, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:30:26 INFO  Metadata:259 - [Consumer clientId=consumer-g1-12, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:30:26 INFO  Metadata:259 - [Consumer clientId=consumer-g1-5, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:30:26 INFO  Metadata:259 - [Consumer clientId=consumer-g1-2, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:30:26 INFO  Metadata:259 - [Consumer clientId=consumer-g1-9, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:30:26 INFO  Metadata:259 - [Consumer clientId=consumer-g1-6, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:30:26 INFO  Metadata:259 - [Consumer clientId=consumer-g1-7, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:30:26 INFO  Metadata:259 - [Consumer clientId=consumer-g1-11, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 8 initially has no partitions to read from.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 4 initially has no partitions to read from.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 6 initially has no partitions to read from.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 9 initially has no partitions to read from.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:696 - Consumer subtask 7 will start reading the following 1 partitions from the latest offsets: [KafkaTopicPartition{topic='T01', partition=0}]
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 10 initially has no partitions to read from.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 3 initially has no partitions to read from.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 1 initially has no partitions to read from.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 5 initially has no partitions to read from.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 0 initially has no partitions to read from.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 2 initially has no partitions to read from.
2023-12-26 10:30:26 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12)#0 (8cf1f84ab595a58d7b4a67ef053f56a0) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 11 initially has no partitions to read from.
2023-12-26 10:30:26 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12)#0 (d82a9916df449c644374a744987e0ad8) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12)#0 (96b391b09dae97769a2f235bcccc7ae0) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12)#0 (12d2d2a9d35128c4916303cc3fe26871) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12)#0 (6fba0aa0b13d3d230f447674226d8888) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12)#0 (ff95be03a753640350b2768ca0fb9045) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12)#0 (7257e5c1614e58f85056c59cf823c280) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12)#0 (f725c20d35d5a9c64e0822292a33f12d) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12)#0 (f3eb0b87a5ff932546aa1a9333477b35) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12)#0 (f7e85b139ccb75c45a5073fcc33754ae) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12) (8cf1f84ab595a58d7b4a67ef053f56a0) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12) (d82a9916df449c644374a744987e0ad8) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12)#0 (b0a95c24a267dc250d0dc04589006750) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12)#0 (5570ccdc081e9b5b8770abdaf59355c0) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12) (96b391b09dae97769a2f235bcccc7ae0) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12) (12d2d2a9d35128c4916303cc3fe26871) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12) (6fba0aa0b13d3d230f447674226d8888) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 7 creating fetcher with offsets {KafkaTopicPartition{topic='T01', partition=0}=-915623761774}.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12) (ff95be03a753640350b2768ca0fb9045) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12) (7257e5c1614e58f85056c59cf823c280) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12) (f725c20d35d5a9c64e0822292a33f12d) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12) (f3eb0b87a5ff932546aa1a9333477b35) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12) (f7e85b139ccb75c45a5073fcc33754ae) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12) (b0a95c24a267dc250d0dc04589006750) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) (5570ccdc081e9b5b8770abdaf59355c0) switched from INITIALIZING to RUNNING.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 10 creating fetcher with offsets {}.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 1 creating fetcher with offsets {}.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 6 creating fetcher with offsets {}.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 11 creating fetcher with offsets {}.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 5 creating fetcher with offsets {}.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 9 creating fetcher with offsets {}.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 8 creating fetcher with offsets {}.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 3 creating fetcher with offsets {}.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 0 creating fetcher with offsets {}.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 2 creating fetcher with offsets {}.
2023-12-26 10:30:26 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 4 creating fetcher with offsets {}.
2023-12-26 10:30:26 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:26 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:26 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:26 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:26 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:26 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:26 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:26 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:26 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:26 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:26 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:26 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:30:26 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:26 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:26 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:26 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557826987
2023-12-26 10:30:27 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:27 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:27 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:27 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:27 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:27 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557827012
2023-12-26 10:30:27 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:27 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:27 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:27 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557827013
2023-12-26 10:30:27 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:27 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:27 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:27 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557827012
2023-12-26 10:30:27 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:27 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:27 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:27 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:27 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:27 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:30:27 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:27 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:27 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557827022
2023-12-26 10:30:27 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:27 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:27 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557827022
2023-12-26 10:30:27 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:27 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:27 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557827021
2023-12-26 10:30:27 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:27 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:27 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557827021
2023-12-26 10:30:27 INFO  KafkaConsumer:1123 - [Consumer clientId=consumer-g1-22, groupId=g1] Subscribed to partition(s): T01-0
2023-12-26 10:30:27 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:27 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:27 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557827021
2023-12-26 10:30:27 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:27 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:27 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557827021
2023-12-26 10:30:27 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:27 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:27 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557827019
2023-12-26 10:30:27 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:30:27 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:30:27 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703557827023
2023-12-26 10:30:27 INFO  SubscriptionState:564 - [Consumer clientId=consumer-g1-22, groupId=g1] Seeking to LATEST offset of partition T01-0
2023-12-26 10:30:27 INFO  Metadata:259 - [Consumer clientId=consumer-g1-22, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:30:27 INFO  SubscriptionState:381 - [Consumer clientId=consumer-g1-22, groupId=g1] Resetting offset for partition T01-0 to offset 46.
2023-12-26 10:30:28 INFO  CheckpointCoordinator:742 - Triggering checkpoint 1 (type=CHECKPOINT) @ 1703557828083 for job da35b83ed2a1a640ae5005270ab42a67.
2023-12-26 10:30:28 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 1
2023-12-26 10:30:28 INFO  CheckpointCoordinator:1246 - Completed checkpoint 1 for job da35b83ed2a1a640ae5005270ab42a67 (13265 bytes in 527 ms).
2023-12-26 10:30:29 INFO  AbstractCoordinator:756 - [Consumer clientId=consumer-g1-22, groupId=g1] Discovered group coordinator 172.20.3.227:9092 (id: 2147483647 rack: null)
2023-12-26 10:30:38 INFO  CheckpointCoordinator:742 - Triggering checkpoint 2 (type=CHECKPOINT) @ 1703557838080 for job da35b83ed2a1a640ae5005270ab42a67.
2023-12-26 10:30:38 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 2
2023-12-26 10:30:38 INFO  CheckpointCoordinator:1246 - Completed checkpoint 2 for job da35b83ed2a1a640ae5005270ab42a67 (13265 bytes in 182 ms).
2023-12-26 10:30:48 INFO  CheckpointCoordinator:742 - Triggering checkpoint 3 (type=CHECKPOINT) @ 1703557848080 for job da35b83ed2a1a640ae5005270ab42a67.
2023-12-26 10:30:48 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 3
2023-12-26 10:30:48 INFO  CheckpointCoordinator:1246 - Completed checkpoint 3 for job da35b83ed2a1a640ae5005270ab42a67 (13265 bytes in 117 ms).
2023-12-26 10:30:50 ERROR TaskManagerDetailsHandler:260 - Unhandled exception.
org.apache.flink.runtime.resourcemanager.exceptions.UnknownTaskExecutorException: No TaskExecutor registered under 9e719f08-04c7-422b-991c-20dbd63652b2.
	at org.apache.flink.runtime.resourcemanager.ResourceManager.requestTaskManagerDetailsInfo(ResourceManager.java:683)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2023-12-26 10:30:53 ERROR TaskManagerDetailsHandler:260 - Unhandled exception.
org.apache.flink.runtime.resourcemanager.exceptions.UnknownTaskExecutorException: No TaskExecutor registered under 9e719f08-04c7-422b-991c-20dbd63652b2.
	at org.apache.flink.runtime.resourcemanager.ResourceManager.requestTaskManagerDetailsInfo(ResourceManager.java:683)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2023-12-26 10:30:56 ERROR TaskManagerDetailsHandler:260 - Unhandled exception.
org.apache.flink.runtime.resourcemanager.exceptions.UnknownTaskExecutorException: No TaskExecutor registered under 9e719f08-04c7-422b-991c-20dbd63652b2.
	at org.apache.flink.runtime.resourcemanager.ResourceManager.requestTaskManagerDetailsInfo(ResourceManager.java:683)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2023-12-26 10:30:58 INFO  CheckpointCoordinator:742 - Triggering checkpoint 4 (type=CHECKPOINT) @ 1703557858081 for job da35b83ed2a1a640ae5005270ab42a67.
2023-12-26 10:30:58 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 4
2023-12-26 10:30:58 INFO  CheckpointCoordinator:1246 - Completed checkpoint 4 for job da35b83ed2a1a640ae5005270ab42a67 (13265 bytes in 107 ms).
2023-12-26 10:31:06 ERROR TaskManagerLogFileHandler:157 - Failed to transfer file from TaskExecutor 83f9e70c-9728-4659-887f-cb09e984cb47.
java.util.concurrent.CompletionException: org.apache.flink.util.FlinkException: The file LOG is not available on the TaskExecutor.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1079)
	at akka.dispatch.OnComplete.internal(Future.scala:263)
	at akka.dispatch.OnComplete.internal(Future.scala:261)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.util.FlinkException: The file LOG is not available on the TaskExecutor.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByFilePath(TaskExecutor.java:2077)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByType(TaskExecutor.java:1158)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	... 4 more
2023-12-26 10:31:06 ERROR TaskManagerLogFileHandler:260 - Unhandled exception.
org.apache.flink.util.FlinkException: The file LOG is not available on the TaskExecutor.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByFilePath(TaskExecutor.java:2077)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByType(TaskExecutor.java:1158)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2023-12-26 10:31:08 INFO  CheckpointCoordinator:742 - Triggering checkpoint 5 (type=CHECKPOINT) @ 1703557868083 for job da35b83ed2a1a640ae5005270ab42a67.
2023-12-26 10:31:08 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 5
2023-12-26 10:31:08 INFO  CheckpointCoordinator:1246 - Completed checkpoint 5 for job da35b83ed2a1a640ae5005270ab42a67 (13265 bytes in 91 ms).
2023-12-26 10:31:09 ERROR TaskManagerStdoutFileHandler:157 - Failed to transfer file from TaskExecutor 83f9e70c-9728-4659-887f-cb09e984cb47.
java.util.concurrent.CompletionException: org.apache.flink.util.FlinkException: The file STDOUT is not available on the TaskExecutor.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1079)
	at akka.dispatch.OnComplete.internal(Future.scala:263)
	at akka.dispatch.OnComplete.internal(Future.scala:261)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.util.FlinkException: The file STDOUT is not available on the TaskExecutor.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByFilePath(TaskExecutor.java:2077)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByType(TaskExecutor.java:1158)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	... 4 more
2023-12-26 10:31:09 ERROR TaskManagerStdoutFileHandler:260 - Unhandled exception.
org.apache.flink.util.FlinkException: The file STDOUT is not available on the TaskExecutor.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByFilePath(TaskExecutor.java:2077)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByType(TaskExecutor.java:1158)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2023-12-26 10:31:18 INFO  CheckpointCoordinator:742 - Triggering checkpoint 6 (type=CHECKPOINT) @ 1703557878079 for job da35b83ed2a1a640ae5005270ab42a67.
2023-12-26 10:31:18 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 6
2023-12-26 10:31:18 INFO  CheckpointCoordinator:1246 - Completed checkpoint 6 for job da35b83ed2a1a640ae5005270ab42a67 (13265 bytes in 143 ms).
2023-12-26 10:31:18 ERROR TaskManagerLogFileHandler:157 - Failed to transfer file from TaskExecutor 83f9e70c-9728-4659-887f-cb09e984cb47.
java.util.concurrent.CompletionException: org.apache.flink.util.FlinkException: The file LOG is not available on the TaskExecutor.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1079)
	at akka.dispatch.OnComplete.internal(Future.scala:263)
	at akka.dispatch.OnComplete.internal(Future.scala:261)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.util.FlinkException: The file LOG is not available on the TaskExecutor.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByFilePath(TaskExecutor.java:2077)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByType(TaskExecutor.java:1158)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	... 4 more
2023-12-26 10:31:18 ERROR TaskManagerLogFileHandler:260 - Unhandled exception.
org.apache.flink.util.FlinkException: The file LOG is not available on the TaskExecutor.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByFilePath(TaskExecutor.java:2077)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByType(TaskExecutor.java:1158)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2023-12-26 10:31:22 ERROR TaskManagerLogFileHandler:157 - Failed to transfer file from TaskExecutor 83f9e70c-9728-4659-887f-cb09e984cb47.
java.util.concurrent.CompletionException: org.apache.flink.util.FlinkException: The file LOG is not available on the TaskExecutor.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1079)
	at akka.dispatch.OnComplete.internal(Future.scala:263)
	at akka.dispatch.OnComplete.internal(Future.scala:261)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.util.FlinkException: The file LOG is not available on the TaskExecutor.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByFilePath(TaskExecutor.java:2077)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByType(TaskExecutor.java:1158)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	... 4 more
2023-12-26 10:31:22 ERROR TaskManagerLogFileHandler:260 - Unhandled exception.
org.apache.flink.util.FlinkException: The file LOG is not available on the TaskExecutor.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByFilePath(TaskExecutor.java:2077)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestFileUploadByType(TaskExecutor.java:1158)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2023-12-26 10:31:28 INFO  CheckpointCoordinator:742 - Triggering checkpoint 7 (type=CHECKPOINT) @ 1703557888083 for job da35b83ed2a1a640ae5005270ab42a67.
2023-12-26 10:31:28 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 7
2023-12-26 10:31:28 INFO  CheckpointCoordinator:1246 - Completed checkpoint 7 for job da35b83ed2a1a640ae5005270ab42a67 (13265 bytes in 94 ms).
2023-12-26 10:31:38 INFO  CheckpointCoordinator:742 - Triggering checkpoint 8 (type=CHECKPOINT) @ 1703557898082 for job da35b83ed2a1a640ae5005270ab42a67.
2023-12-26 10:31:38 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 8
2023-12-26 10:31:38 INFO  CheckpointCoordinator:1246 - Completed checkpoint 8 for job da35b83ed2a1a640ae5005270ab42a67 (13265 bytes in 56 ms).
2023-12-26 10:31:41 INFO  PermanentBlobCache:240 - Shutting down BLOB cache
2023-12-26 10:31:41 INFO  TaskExecutorLocalStateStoresManager:231 - Shutting down TaskExecutorLocalStateStoresManager.
2023-12-26 10:31:41 INFO  TransientBlobCache:240 - Shutting down BLOB cache
2023-12-26 10:31:41 INFO  BlobServer:345 - Stopped BLOB server at 0.0.0.0:58095
2023-12-26 10:31:41 INFO  FileChannelManagerImpl:149 - FileChannelManager removed spill file directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-netty-shuffle-c06a278b-9e28-4248-9334-674d14c731a8
2023-12-26 10:31:41 INFO  FileCache:160 - removed file cache directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-dist-cache-2501b30b-61a1-45ac-b8af-e238d2f9f114
2023-12-26 10:31:41 INFO  FileChannelManagerImpl:149 - FileChannelManager removed spill file directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-io-87b27a90-0961-4020-8d4d-0f3a8fccffbc
2023-12-26 10:34:36 INFO  FlinkTable2IceTable:53 - create iceberg_catalog now!
2023-12-26 10:34:36 INFO  FlinkTable2IceTable:65 - catalog:create catalog iceberg_catalog with (
   'type'='iceberg',
   'catalog-type'='hive',
   'uri'='thrift://172.20.29.46:9083',
   'hive-conf-dir'='/Users/lifenghua/study/sourcecode/iceberg-demo/config/hive-conf-46',
   'hadoop-conf-dir'='/Users/lifenghua/study/sourcecode/iceberg-demo/config/hive-conf-46',
   'client'='1',
   'property-version'='2',
   'warehouse'='hdfs://172.20.29.46:8020/user/hive/warehouse/')

2023-12-26 10:34:36 WARN  HadoopUtils:139 - Could not find Hadoop configuration via any of the supported methods (Flink configuration, environment variables).
2023-12-26 10:34:36 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-26 10:34:36 INFO  HiveConf:176 - Found configuration file null
2023-12-26 10:34:36 WARN  HiveConf:4015 - HiveConf of name hive.vectorized.use.checked.expressions does not exist
2023-12-26 10:34:36 WARN  HiveConf:4015 - HiveConf of name hive.strict.checks.no.partition.filter does not exist
2023-12-26 10:34:36 WARN  HiveConf:4015 - HiveConf of name hive.strict.checks.orderby.no.limit does not exist
2023-12-26 10:34:36 WARN  HiveConf:4015 - HiveConf of name hive.vectorized.input.format.excludes does not exist
2023-12-26 10:34:36 WARN  HiveConf:4015 - HiveConf of name hive.strict.checks.bucketing does not exist
2023-12-26 10:34:36 INFO  metastore:434 - Trying to connect to metastore with URI thrift://172.20.29.46:9083
2023-12-26 10:34:36 INFO  metastore:479 - Opened a connection to metastore, current connections: 1
2023-12-26 10:34:36 INFO  metastore:531 - Connected to metastore.
2023-12-26 10:34:37 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00000-19ee13a9-f0b9-4a30-aadf-bac93fa20a0e.metadata.json
2023-12-26 10:34:38 INFO  BaseMetastoreCatalog:64 - Table loaded by catalog: iceberg_catalog.db1.ice_T01
2023-12-26 10:34:39 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00000-19ee13a9-f0b9-4a30-aadf-bac93fa20a0e.metadata.json
2023-12-26 10:34:39 INFO  CatalogUtil:93 - Manifests to delete: 
2023-12-26 10:34:39 INFO  HiveCatalog:179 - Dropped table: db1.ice_T01
2023-12-26 10:34:39 INFO  FlinkTable2IceTable:79 - 执行sql:CREATE TABLE if not exists _db_.kafka_T01 (
    C1 decimal,
    C2 STRING,
    C3 timestamp,
    PRIMARY KEY (`C1`) NOT ENFORCED
 ) PARTITIONED BY (`C1`)
 WITH (
    'connector' = 'kafka',
    'topic' = 'T01',
    'properties.bootstrap.servers' = '172.20.3.227:9092',
    'properties.group.id' = 'g1',
    'scan.startup.mode' = 'latest-offset',
    'scan.topic-partition-discovery.interval'='10000',
    'value.format' = 'debezium-json'
)

2023-12-26 10:34:39 INFO  FlinkTable2IceTable:85 - create iceberg sql :CREATE TABLE if not exists iceberg_catalog.db1.ice_T01 (
    C1 decimal,
    C2 STRING,
    C3 timestamp,
    PRIMARY KEY (`C1`) NOT ENFORCED
) PARTITIONED BY (`C1`)
WITH (
    'type'='iceberg',
    'table_type'='iceberg',
    'format-version'='2',
    'engine.hive.enabled' = 'true',
    'write.upsert.enabled'='true',
    'table.exec.sink.not-null-enforcer'='true'
)

2023-12-26 10:34:40 INFO  BaseMetastoreCatalog:234 - Table properties set at catalog level through catalog properties: {}
2023-12-26 10:34:40 INFO  BaseMetastoreCatalog:246 - Table properties enforced at catalog level through catalog properties: {}
2023-12-26 10:34:41 INFO  HiveTableOperations:315 - Committed to table iceberg_catalog.db1.ice_T01 with the new metadata location hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00000-ce30aa22-5d7a-4c14-ba2e-5bbcf37e4a97.metadata.json
2023-12-26 10:34:41 INFO  BaseMetastoreTableOperations:137 - Successfully committed to table iceberg_catalog.db1.ice_T01 in 1466 ms
2023-12-26 10:34:41 INFO  FlinkTable2IceTable:89 - insert sql:insert into iceberg_catalog.db1.ice_T01 select * from db1.kafka_T01
2023-12-26 10:34:42 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00000-ce30aa22-5d7a-4c14-ba2e-5bbcf37e4a97.metadata.json
2023-12-26 10:34:44 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00000-ce30aa22-5d7a-4c14-ba2e-5bbcf37e4a97.metadata.json
2023-12-26 10:34:44 INFO  BaseMetastoreCatalog:64 - Table loaded by catalog: iceberg_catalog.db1.ice_T01
2023-12-26 10:34:44 INFO  FlinkSink:430 - Write distribution mode is 'none'
2023-12-26 10:34:44 INFO  FlinkSink:436 - Distribute rows by equality fields, because there are equality fields set
2023-12-26 10:34:45 INFO  TypeExtractor:1994 - class org.apache.iceberg.io.WriteResult does not contain a setter for field dataFiles
2023-12-26 10:34:45 INFO  TypeExtractor:2037 - Class class org.apache.iceberg.io.WriteResult cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2023-12-26 10:34:45 INFO  Configuration:804 - Config uses fallback configuration key 'rest.port' instead of key 'rest.bind-port'
2023-12-26 10:34:45 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
2023-12-26 10:34:45 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
2023-12-26 10:34:45 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
2023-12-26 10:34:45 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
2023-12-26 10:34:45 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
2023-12-26 10:34:45 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
2023-12-26 10:34:45 INFO  MiniCluster:269 - Starting Flink Mini Cluster
2023-12-26 10:34:45 INFO  MiniCluster:279 - Starting Metrics Registry
2023-12-26 10:34:45 INFO  MetricRegistryImpl:126 - No metrics reporter configured, no metrics will be exposed/reported.
2023-12-26 10:34:45 INFO  MiniCluster:283 - Starting RPC Service(s)
2023-12-26 10:34:45 INFO  AkkaRpcServiceUtils:265 - Trying to start local actor system
2023-12-26 10:34:45 INFO  Slf4jLogger:92 - Slf4jLogger started
2023-12-26 10:34:45 INFO  AkkaRpcServiceUtils:298 - Actor system started at akka://flink
2023-12-26 10:34:45 INFO  AkkaRpcServiceUtils:265 - Trying to start local actor system
2023-12-26 10:34:45 INFO  Slf4jLogger:92 - Slf4jLogger started
2023-12-26 10:34:45 INFO  AkkaRpcServiceUtils:298 - Actor system started at akka://flink-metrics
2023-12-26 10:34:45 INFO  AkkaRpcService:232 - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
2023-12-26 10:34:46 INFO  MiniCluster:487 - Starting high-availability services
2023-12-26 10:34:46 INFO  BlobServer:138 - Created BLOB server storage directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/blobStore-ca7ec0bb-f42f-4bd4-abdd-4daf9735e19d
2023-12-26 10:34:46 INFO  BlobServer:213 - Started BLOB server at 0.0.0.0:49673 - max concurrent requests: 50 - max backlog: 1000
2023-12-26 10:34:46 INFO  PermanentBlobCache:90 - Created BLOB cache storage directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/blobStore-98a813d2-7425-41ad-a2ba-b00b052328c1
2023-12-26 10:34:46 INFO  TransientBlobCache:90 - Created BLOB cache storage directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/blobStore-087485da-10e2-44da-9846-824b86d53dc2
2023-12-26 10:34:46 INFO  MiniCluster:606 - Starting 1 TaskManger(s)
2023-12-26 10:34:46 INFO  TaskManagerRunner:474 - Starting TaskManager with ResourceID: 68dd5848-5f12-4dca-9f75-4dba964f42ae
2023-12-26 10:34:46 INFO  TaskManagerServices:441 - Temporary file directory '/var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T': total 926 GB, usable 342 GB (36.93% usable)
2023-12-26 10:34:46 INFO  FileChannelManagerImpl:98 - FileChannelManager uses directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-io-072961ea-3a6c-46b4-9464-72651f146964 for spill files.
2023-12-26 10:34:46 INFO  FileChannelManagerImpl:98 - FileChannelManager uses directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-netty-shuffle-38032ac3-b887-4d71-8a1c-0f62f11093c7 for spill files.
2023-12-26 10:34:46 INFO  NetworkBufferPool:145 - Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
2023-12-26 10:34:46 INFO  NettyShuffleEnvironment:328 - Starting the network environment and its components.
2023-12-26 10:34:46 INFO  KvStateService:92 - Starting the kvState service and its components.
2023-12-26 10:34:46 INFO  AkkaRpcService:232 - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
2023-12-26 10:34:46 INFO  DefaultJobLeaderService:123 - Start job leader service.
2023-12-26 10:34:46 INFO  FileCache:116 - User file cache uses directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-dist-cache-b34b06e4-8fce-48b0-9df7-8cdfca143376
2023-12-26 10:34:46 INFO  Configuration:804 - Config uses fallback configuration key 'rest.port' instead of key 'rest.bind-port'
2023-12-26 10:34:46 INFO  DispatcherRestEndpoint:139 - Starting rest endpoint.
2023-12-26 10:34:46 WARN  WebMonitorUtils:82 - Log file environment variable 'log.file' is not set.
2023-12-26 10:34:46 INFO  WebMonitorUtils:103 - Determined location of main cluster component log file: /Users/lifenghua/study/sourcecode/iceberg-demo/logs/file.log
2023-12-26 10:34:46 INFO  WebMonitorUtils:104 - Determined location of main cluster component stdout file: /Users/lifenghua/study/sourcecode/iceberg-demo/logs/file.out
2023-12-26 10:34:46 INFO  DispatcherRestEndpoint:250 - Rest endpoint listening at localhost:18081
2023-12-26 10:34:46 INFO  EmbeddedLeaderService:308 - Proposing leadership to contender http://localhost:18081
2023-12-26 10:34:46 INFO  DispatcherRestEndpoint:936 - Web frontend listening at http://localhost:18081.
2023-12-26 10:34:46 INFO  DispatcherRestEndpoint:994 - http://localhost:18081 was granted leadership with leaderSessionID=171633b5-d96f-4cda-b19f-f0bd9cc9339f
2023-12-26 10:34:46 INFO  EmbeddedLeaderService:256 - Received confirmation of leadership for leader http://localhost:18081 , session=171633b5-d96f-4cda-b19f-f0bd9cc9339f
2023-12-26 10:34:46 INFO  AkkaRpcService:232 - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
2023-12-26 10:34:46 INFO  EmbeddedLeaderService:308 - Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
2023-12-26 10:34:46 INFO  StandaloneResourceManager:234 - Starting the resource manager.
2023-12-26 10:34:46 INFO  EmbeddedLeaderService:308 - Proposing leadership to contender LeaderContender: StandaloneResourceManager
2023-12-26 10:34:46 INFO  DefaultDispatcherRunner:107 - DefaultDispatcherRunner was granted leadership with leader id 858322fd-3f94-4c31-944f-53758d939e05. Creating new DispatcherLeaderProcess.
2023-12-26 10:34:46 INFO  StandaloneResourceManager:1230 - ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token 86cf0ed9c7bb9f3cad2a5d9ba7be41d2
2023-12-26 10:34:46 INFO  MiniCluster:413 - Flink Mini Cluster started successfully
2023-12-26 10:34:46 INFO  SessionDispatcherLeaderProcess:97 - Start SessionDispatcherLeaderProcess.
2023-12-26 10:34:46 INFO  SessionDispatcherLeaderProcess:117 - Recover all persisted job graphs.
2023-12-26 10:34:46 INFO  SessionDispatcherLeaderProcess:125 - Successfully recovered 0 persisted job graphs.
2023-12-26 10:34:46 INFO  EmbeddedLeaderService:256 - Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=ad2a5d9b-a7be-41d2-86cf-0ed9c7bb9f3c
2023-12-26 10:34:46 INFO  TaskExecutor:1293 - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(86cf0ed9c7bb9f3cad2a5d9ba7be41d2).
2023-12-26 10:34:46 INFO  AkkaRpcService:232 - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
2023-12-26 10:34:46 INFO  EmbeddedLeaderService:256 - Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=858322fd-3f94-4c31-944f-53758d939e05
2023-12-26 10:34:46 INFO  TaskExecutor:162 - Resolved ResourceManager address, beginning registration
2023-12-26 10:34:46 INFO  StandaloneDispatcher:300 - Received JobGraph submission a6871a695473e51d3520688e0f3cd838 (insert-into_iceberg_catalog.db1.ice_T01).
2023-12-26 10:34:46 INFO  StandaloneResourceManager:982 - Registering TaskManager with ResourceID 68dd5848-5f12-4dca-9f75-4dba964f42ae (akka://flink/user/rpc/taskmanager_0) at ResourceManager
2023-12-26 10:34:46 INFO  StandaloneDispatcher:362 - Submitting job a6871a695473e51d3520688e0f3cd838 (insert-into_iceberg_catalog.db1.ice_T01).
2023-12-26 10:34:46 INFO  TaskExecutor:99 - Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id b70658e7b33f2e12f92cbc68ee915085.
2023-12-26 10:34:46 INFO  EmbeddedLeaderService:308 - Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
2023-12-26 10:34:46 INFO  AkkaRpcService:232 - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
2023-12-26 10:34:46 INFO  JobMaster:289 - Initializing job insert-into_iceberg_catalog.db1.ice_T01 (a6871a695473e51d3520688e0f3cd838).
2023-12-26 10:34:46 INFO  JobMaster:98 - Using restart back off time strategy FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2147483647, backoffTimeMS=1000) for insert-into_iceberg_catalog.db1.ice_T01 (a6871a695473e51d3520688e0f3cd838).
2023-12-26 10:34:46 INFO  JobMaster:159 - Running initialization on master for job insert-into_iceberg_catalog.db1.ice_T01 (a6871a695473e51d3520688e0f3cd838).
2023-12-26 10:34:46 INFO  JobMaster:183 - Successfully ran initialization on master in 0 ms.
2023-12-26 10:34:46 INFO  DefaultExecutionTopology:271 - Built 1 pipelined regions in 3 ms
2023-12-26 10:34:46 INFO  JobMaster:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@919280
2023-12-26 10:34:46 INFO  JobMaster:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:46 INFO  CheckpointCoordinator:1532 - No checkpoint found during restore.
2023-12-26 10:34:46 INFO  JobMaster:145 - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@135d2cfe for insert-into_iceberg_catalog.db1.ice_T01 (a6871a695473e51d3520688e0f3cd838).
2023-12-26 10:34:46 INFO  EmbeddedLeaderService:256 - Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=2ab135e9-3a10-472d-a9a9-97e46d0a63cc
2023-12-26 10:34:46 INFO  JobMaster:867 - Starting execution of job insert-into_iceberg_catalog.db1.ice_T01 (a6871a695473e51d3520688e0f3cd838) under job master id a9a997e46d0a63cc2ab135e93a10472d.
2023-12-26 10:34:46 INFO  JobMaster:183 - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
2023-12-26 10:34:46 INFO  ExecutionGraph:1039 - Job insert-into_iceberg_catalog.db1.ice_T01 (a6871a695473e51d3520688e0f3cd838) switched from state CREATED to RUNNING.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) (2f2ed81100f551cc852e9ef5b4686fd9) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12) (1183e83f7f32aeb257c62d98bdd29fde) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12) (8d1350263ba1ab4d5107aeef42a8c042) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12) (314bc688b9420fcfd3b3297575935dca) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12) (4a50400e9e1ec36082990ffe195856b9) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12) (71b39e7d8e901d4335ed260ac3b98a9a) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12) (19e39f2906efa43752f2d5604328423c) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12) (66afbca6e5bde01f13a0c8d2aff893fe) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12) (12eff7c4e52121e42762e0667ab0484c) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12) (8b1abbbd0fae659a98fed29848c10269) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12) (b5d851a6a7dbb2e1b1110ef7cfb5b16e) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12) (c195972f058d9f8d15dcb5f988d78d92) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (1/12) (4ffeec868027d8893982ba0cfccf25aa) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (2/12) (49678d9e9fa2d4f81c617dec3c0a40f3) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (3/12) (c1b0ca92f2a7179cf39f843395f7990f) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (4/12) (9987d3651cd0a27ba8f90435c2b5b04d) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (5/12) (d1080aeaed4380bfc211780df8e2482d) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (6/12) (ef3b1509ace48addb80777f4180c3c22) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (7/12) (03cf1423d317c8fa651a76d17021c499) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (8/12) (e0eec9cf12181789031734154b51663a) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (9/12) (71ae8ec63711f2386b38f6c74a74116d) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (10/12) (f13c5623d865890c5c7ae5dcb07b361f) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (11/12) (1f302fcdcc1283ca913611ccbfc5cfc5) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (12/12) (f13dfe3113921799459e036368ebada0) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1) (3960532d678a52e47776e3d45dbe7b43) switched from CREATED to SCHEDULED.
2023-12-26 10:34:46 INFO  JobMaster:1040 - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(86cf0ed9c7bb9f3cad2a5d9ba7be41d2)
2023-12-26 10:34:46 INFO  JobMaster:162 - Resolved ResourceManager address, beginning registration
2023-12-26 10:34:46 INFO  StandaloneResourceManager:355 - Registering job manager a9a997e46d0a63cc2ab135e93a10472d@akka://flink/user/rpc/jobmanager_3 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:34:46 INFO  StandaloneResourceManager:909 - Registered job manager a9a997e46d0a63cc2ab135e93a10472d@akka://flink/user/rpc/jobmanager_3 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:34:46 INFO  JobMaster:1064 - JobManager successfully registered at ResourceManager, leader id: 86cf0ed9c7bb9f3cad2a5d9ba7be41d2.
2023-12-26 10:34:46 INFO  DeclarativeSlotManager:263 - Received resource requirements from job a6871a695473e51d3520688e0f3cd838: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=12}]
2023-12-26 10:34:46 INFO  TaskExecutor:1027 - Receive slot request ea99d8290930f008995a5311e5fde0f7 for job a6871a695473e51d3520688e0f3cd838 from resource manager with leader id 86cf0ed9c7bb9f3cad2a5d9ba7be41d2.
2023-12-26 10:34:46 INFO  TaskExecutor:1104 - Allocated slot for ea99d8290930f008995a5311e5fde0f7.
2023-12-26 10:34:46 INFO  DefaultJobLeaderService:188 - Add job a6871a695473e51d3520688e0f3cd838 for job leader monitoring.
2023-12-26 10:34:46 INFO  TaskExecutor:1027 - Receive slot request 367203734b68b79c12e22b56e22e6e55 for job a6871a695473e51d3520688e0f3cd838 from resource manager with leader id 86cf0ed9c7bb9f3cad2a5d9ba7be41d2.
2023-12-26 10:34:46 INFO  DefaultJobLeaderService:346 - Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 2ab135e9-3a10-472d-a9a9-97e46d0a63cc.
2023-12-26 10:34:46 INFO  TaskExecutor:1104 - Allocated slot for 367203734b68b79c12e22b56e22e6e55.
2023-12-26 10:34:46 INFO  TaskExecutor:1027 - Receive slot request 50298afcbb3bc59699911b4443302f23 for job a6871a695473e51d3520688e0f3cd838 from resource manager with leader id 86cf0ed9c7bb9f3cad2a5d9ba7be41d2.
2023-12-26 10:34:46 INFO  TaskExecutor:1104 - Allocated slot for 50298afcbb3bc59699911b4443302f23.
2023-12-26 10:34:46 INFO  TaskExecutor:1027 - Receive slot request c758c7a7dd3c69b6ca9fce6c55e0e501 for job a6871a695473e51d3520688e0f3cd838 from resource manager with leader id 86cf0ed9c7bb9f3cad2a5d9ba7be41d2.
2023-12-26 10:34:46 INFO  TaskExecutor:1104 - Allocated slot for c758c7a7dd3c69b6ca9fce6c55e0e501.
2023-12-26 10:34:46 INFO  TaskExecutor:1027 - Receive slot request 71a1649253a8cc2eab0824d4f7666548 for job a6871a695473e51d3520688e0f3cd838 from resource manager with leader id 86cf0ed9c7bb9f3cad2a5d9ba7be41d2.
2023-12-26 10:34:46 INFO  TaskExecutor:1104 - Allocated slot for 71a1649253a8cc2eab0824d4f7666548.
2023-12-26 10:34:46 INFO  DefaultJobLeaderService:162 - Resolved JobManager address, beginning registration
2023-12-26 10:34:46 INFO  TaskExecutor:1027 - Receive slot request 8a67fbcbb12c5e3d3e4580cb226c344b for job a6871a695473e51d3520688e0f3cd838 from resource manager with leader id 86cf0ed9c7bb9f3cad2a5d9ba7be41d2.
2023-12-26 10:34:46 INFO  TaskExecutor:1104 - Allocated slot for 8a67fbcbb12c5e3d3e4580cb226c344b.
2023-12-26 10:34:46 INFO  TaskExecutor:1027 - Receive slot request 4a3e4a4ed8b5311cd26b9df0788910e0 for job a6871a695473e51d3520688e0f3cd838 from resource manager with leader id 86cf0ed9c7bb9f3cad2a5d9ba7be41d2.
2023-12-26 10:34:46 INFO  TaskExecutor:1104 - Allocated slot for 4a3e4a4ed8b5311cd26b9df0788910e0.
2023-12-26 10:34:46 INFO  TaskExecutor:1027 - Receive slot request b8282f2f710d571ecaeb45986773341e for job a6871a695473e51d3520688e0f3cd838 from resource manager with leader id 86cf0ed9c7bb9f3cad2a5d9ba7be41d2.
2023-12-26 10:34:46 INFO  TaskExecutor:1104 - Allocated slot for b8282f2f710d571ecaeb45986773341e.
2023-12-26 10:34:46 INFO  TaskExecutor:1027 - Receive slot request 32d4ea0feb46b31220fd745316d24967 for job a6871a695473e51d3520688e0f3cd838 from resource manager with leader id 86cf0ed9c7bb9f3cad2a5d9ba7be41d2.
2023-12-26 10:34:46 INFO  TaskExecutor:1104 - Allocated slot for 32d4ea0feb46b31220fd745316d24967.
2023-12-26 10:34:46 INFO  TaskExecutor:1027 - Receive slot request 73894cd91309d9ebef22c4c99aea62a8 for job a6871a695473e51d3520688e0f3cd838 from resource manager with leader id 86cf0ed9c7bb9f3cad2a5d9ba7be41d2.
2023-12-26 10:34:46 INFO  TaskExecutor:1104 - Allocated slot for 73894cd91309d9ebef22c4c99aea62a8.
2023-12-26 10:34:46 INFO  TaskExecutor:1027 - Receive slot request df0ed4a7edbeeaa2e5ae33ce5a4b6cdf for job a6871a695473e51d3520688e0f3cd838 from resource manager with leader id 86cf0ed9c7bb9f3cad2a5d9ba7be41d2.
2023-12-26 10:34:46 INFO  TaskExecutor:1104 - Allocated slot for df0ed4a7edbeeaa2e5ae33ce5a4b6cdf.
2023-12-26 10:34:46 INFO  TaskExecutor:1027 - Receive slot request 55c6de04cb91805b862f91ae4d64e8dd for job a6871a695473e51d3520688e0f3cd838 from resource manager with leader id 86cf0ed9c7bb9f3cad2a5d9ba7be41d2.
2023-12-26 10:34:46 INFO  TaskExecutor:1104 - Allocated slot for 55c6de04cb91805b862f91ae4d64e8dd.
2023-12-26 10:34:46 INFO  DefaultJobLeaderService:413 - Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:34:46 INFO  TaskExecutor:1608 - Establish JobManager connection for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:34:46 INFO  TaskExecutor:1459 - Offer reserved slots to the leader of job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) (2f2ed81100f551cc852e9ef5b4686fd9) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) (attempt #0) with attempt id 2f2ed81100f551cc852e9ef5b4686fd9 to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 55c6de04cb91805b862f91ae4d64e8dd
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12) (1183e83f7f32aeb257c62d98bdd29fde) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  TaskSlotTableImpl:388 - Activate slot 55c6de04cb91805b862f91ae4d64e8dd.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12) (attempt #0) with attempt id 1183e83f7f32aeb257c62d98bdd29fde to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 71a1649253a8cc2eab0824d4f7666548
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12) (8d1350263ba1ab4d5107aeef42a8c042) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12) (attempt #0) with attempt id 8d1350263ba1ab4d5107aeef42a8c042 to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id c758c7a7dd3c69b6ca9fce6c55e0e501
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12) (314bc688b9420fcfd3b3297575935dca) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12) (attempt #0) with attempt id 314bc688b9420fcfd3b3297575935dca to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 32d4ea0feb46b31220fd745316d24967
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12) (4a50400e9e1ec36082990ffe195856b9) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12) (attempt #0) with attempt id 4a50400e9e1ec36082990ffe195856b9 to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id ea99d8290930f008995a5311e5fde0f7
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12) (71b39e7d8e901d4335ed260ac3b98a9a) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12) (attempt #0) with attempt id 71b39e7d8e901d4335ed260ac3b98a9a to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 4a3e4a4ed8b5311cd26b9df0788910e0
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12) (19e39f2906efa43752f2d5604328423c) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12) (attempt #0) with attempt id 19e39f2906efa43752f2d5604328423c to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 50298afcbb3bc59699911b4443302f23
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12) (66afbca6e5bde01f13a0c8d2aff893fe) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12) (attempt #0) with attempt id 66afbca6e5bde01f13a0c8d2aff893fe to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id df0ed4a7edbeeaa2e5ae33ce5a4b6cdf
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12) (12eff7c4e52121e42762e0667ab0484c) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12) (attempt #0) with attempt id 12eff7c4e52121e42762e0667ab0484c to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id b8282f2f710d571ecaeb45986773341e
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12) (8b1abbbd0fae659a98fed29848c10269) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12) (attempt #0) with attempt id 8b1abbbd0fae659a98fed29848c10269 to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 367203734b68b79c12e22b56e22e6e55
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12) (b5d851a6a7dbb2e1b1110ef7cfb5b16e) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12) (attempt #0) with attempt id b5d851a6a7dbb2e1b1110ef7cfb5b16e to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 8a67fbcbb12c5e3d3e4580cb226c344b
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12) (c195972f058d9f8d15dcb5f988d78d92) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12) (attempt #0) with attempt id c195972f058d9f8d15dcb5f988d78d92 to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 73894cd91309d9ebef22c4c99aea62a8
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (1/12) (4ffeec868027d8893982ba0cfccf25aa) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (1/12) (attempt #0) with attempt id 4ffeec868027d8893982ba0cfccf25aa to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 55c6de04cb91805b862f91ae4d64e8dd
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (2/12) (49678d9e9fa2d4f81c617dec3c0a40f3) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (2/12) (attempt #0) with attempt id 49678d9e9fa2d4f81c617dec3c0a40f3 to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 71a1649253a8cc2eab0824d4f7666548
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (3/12) (c1b0ca92f2a7179cf39f843395f7990f) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (3/12) (attempt #0) with attempt id c1b0ca92f2a7179cf39f843395f7990f to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id c758c7a7dd3c69b6ca9fce6c55e0e501
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (4/12) (9987d3651cd0a27ba8f90435c2b5b04d) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (4/12) (attempt #0) with attempt id 9987d3651cd0a27ba8f90435c2b5b04d to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 32d4ea0feb46b31220fd745316d24967
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (5/12) (d1080aeaed4380bfc211780df8e2482d) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (5/12) (attempt #0) with attempt id d1080aeaed4380bfc211780df8e2482d to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id ea99d8290930f008995a5311e5fde0f7
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (6/12) (ef3b1509ace48addb80777f4180c3c22) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (6/12) (attempt #0) with attempt id ef3b1509ace48addb80777f4180c3c22 to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 4a3e4a4ed8b5311cd26b9df0788910e0
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (7/12) (03cf1423d317c8fa651a76d17021c499) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (7/12) (attempt #0) with attempt id 03cf1423d317c8fa651a76d17021c499 to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 50298afcbb3bc59699911b4443302f23
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (8/12) (e0eec9cf12181789031734154b51663a) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (8/12) (attempt #0) with attempt id e0eec9cf12181789031734154b51663a to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id df0ed4a7edbeeaa2e5ae33ce5a4b6cdf
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (9/12) (71ae8ec63711f2386b38f6c74a74116d) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (9/12) (attempt #0) with attempt id 71ae8ec63711f2386b38f6c74a74116d to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id b8282f2f710d571ecaeb45986773341e
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (10/12) (f13c5623d865890c5c7ae5dcb07b361f) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (10/12) (attempt #0) with attempt id f13c5623d865890c5c7ae5dcb07b361f to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 367203734b68b79c12e22b56e22e6e55
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (11/12) (1f302fcdcc1283ca913611ccbfc5cfc5) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (11/12) (attempt #0) with attempt id 1f302fcdcc1283ca913611ccbfc5cfc5 to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 8a67fbcbb12c5e3d3e4580cb226c344b
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergStreamWriter (12/12) (f13dfe3113921799459e036368ebada0) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (12/12) (attempt #0) with attempt id f13dfe3113921799459e036368ebada0 to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 73894cd91309d9ebef22c4c99aea62a8
2023-12-26 10:34:46 INFO  ExecutionGraph:1438 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1) (3960532d678a52e47776e3d45dbe7b43) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:34:46 INFO  ExecutionGraph:571 - Deploying IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1) (attempt #0) with attempt id 3960532d678a52e47776e3d45dbe7b43 to 68dd5848-5f12-4dca-9f75-4dba964f42ae @ localhost (dataPort=-1) with allocation id 55c6de04cb91805b862f91ae4d64e8dd
2023-12-26 10:34:46 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12)#0 (2f2ed81100f551cc852e9ef5b4686fd9), deploy into slot with allocation id 55c6de04cb91805b862f91ae4d64e8dd.
2023-12-26 10:34:46 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12)#0 (2f2ed81100f551cc852e9ef5b4686fd9) switched from CREATED to DEPLOYING.
2023-12-26 10:34:46 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12)#0 (2f2ed81100f551cc852e9ef5b4686fd9) [DEPLOYING].
2023-12-26 10:34:46 INFO  TaskSlotTableImpl:388 - Activate slot 71a1649253a8cc2eab0824d4f7666548.
2023-12-26 10:34:46 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12)#0 (1183e83f7f32aeb257c62d98bdd29fde), deploy into slot with allocation id 71a1649253a8cc2eab0824d4f7666548.
2023-12-26 10:34:46 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12)#0 (1183e83f7f32aeb257c62d98bdd29fde) switched from CREATED to DEPLOYING.
2023-12-26 10:34:46 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12)#0 (1183e83f7f32aeb257c62d98bdd29fde) [DEPLOYING].
2023-12-26 10:34:46 INFO  TaskSlotTableImpl:388 - Activate slot c758c7a7dd3c69b6ca9fce6c55e0e501.
2023-12-26 10:34:46 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12)#0 (8d1350263ba1ab4d5107aeef42a8c042), deploy into slot with allocation id c758c7a7dd3c69b6ca9fce6c55e0e501.
2023-12-26 10:34:46 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12)#0 (8d1350263ba1ab4d5107aeef42a8c042) switched from CREATED to DEPLOYING.
2023-12-26 10:34:46 INFO  TaskSlotTableImpl:388 - Activate slot 32d4ea0feb46b31220fd745316d24967.
2023-12-26 10:34:46 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12)#0 (8d1350263ba1ab4d5107aeef42a8c042) [DEPLOYING].
2023-12-26 10:34:46 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12)#0 (314bc688b9420fcfd3b3297575935dca), deploy into slot with allocation id 32d4ea0feb46b31220fd745316d24967.
2023-12-26 10:34:46 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12)#0 (314bc688b9420fcfd3b3297575935dca) switched from CREATED to DEPLOYING.
2023-12-26 10:34:46 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12)#0 (314bc688b9420fcfd3b3297575935dca) [DEPLOYING].
2023-12-26 10:34:46 INFO  TaskSlotTableImpl:388 - Activate slot ea99d8290930f008995a5311e5fde0f7.
2023-12-26 10:34:46 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12)#0 (4a50400e9e1ec36082990ffe195856b9), deploy into slot with allocation id ea99d8290930f008995a5311e5fde0f7.
2023-12-26 10:34:46 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12)#0 (4a50400e9e1ec36082990ffe195856b9) switched from CREATED to DEPLOYING.
2023-12-26 10:34:46 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12)#0 (4a50400e9e1ec36082990ffe195856b9) [DEPLOYING].
2023-12-26 10:34:46 INFO  TaskSlotTableImpl:388 - Activate slot 4a3e4a4ed8b5311cd26b9df0788910e0.
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12)#0 (71b39e7d8e901d4335ed260ac3b98a9a), deploy into slot with allocation id 4a3e4a4ed8b5311cd26b9df0788910e0.
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12)#0 (71b39e7d8e901d4335ed260ac3b98a9a) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12)#0 (71b39e7d8e901d4335ed260ac3b98a9a) [DEPLOYING].
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 50298afcbb3bc59699911b4443302f23.
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12)#0 (19e39f2906efa43752f2d5604328423c), deploy into slot with allocation id 50298afcbb3bc59699911b4443302f23.
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12)#0 (19e39f2906efa43752f2d5604328423c) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12)#0 (19e39f2906efa43752f2d5604328423c) [DEPLOYING].
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot df0ed4a7edbeeaa2e5ae33ce5a4b6cdf.
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5b43d400
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2d7e767f
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1d1b15bc
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1f013e12
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@70205413
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12)#0 (66afbca6e5bde01f13a0c8d2aff893fe), deploy into slot with allocation id df0ed4a7edbeeaa2e5ae33ce5a4b6cdf.
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12)#0 (66afbca6e5bde01f13a0c8d2aff893fe) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12)#0 (66afbca6e5bde01f13a0c8d2aff893fe) [DEPLOYING].
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot b8282f2f710d571ecaeb45986773341e.
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12)#0 (314bc688b9420fcfd3b3297575935dca) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12)#0 (1183e83f7f32aeb257c62d98bdd29fde) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12)#0 (4a50400e9e1ec36082990ffe195856b9) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12)#0 (2f2ed81100f551cc852e9ef5b4686fd9) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12)#0 (8d1350263ba1ab4d5107aeef42a8c042) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@55a9ecb5
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1e37825f
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12)#0 (12eff7c4e52121e42762e0667ab0484c), deploy into slot with allocation id b8282f2f710d571ecaeb45986773341e.
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12)#0 (19e39f2906efa43752f2d5604328423c) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12)#0 (71b39e7d8e901d4335ed260ac3b98a9a) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12)#0 (12eff7c4e52121e42762e0667ab0484c) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12)#0 (12eff7c4e52121e42762e0667ab0484c) [DEPLOYING].
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 367203734b68b79c12e22b56e22e6e55.
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6ec081ad
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12)#0 (66afbca6e5bde01f13a0c8d2aff893fe) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12)#0 (8b1abbbd0fae659a98fed29848c10269), deploy into slot with allocation id 367203734b68b79c12e22b56e22e6e55.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12) (4a50400e9e1ec36082990ffe195856b9) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12)#0 (8b1abbbd0fae659a98fed29848c10269) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 73894cd91309d9ebef22c4c99aea62a8.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12) (314bc688b9420fcfd3b3297575935dca) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12)#0 (8b1abbbd0fae659a98fed29848c10269) [DEPLOYING].
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12)#0 (c195972f058d9f8d15dcb5f988d78d92), deploy into slot with allocation id 73894cd91309d9ebef22c4c99aea62a8.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) (2f2ed81100f551cc852e9ef5b4686fd9) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12)#0 (c195972f058d9f8d15dcb5f988d78d92) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12)#0 (c195972f058d9f8d15dcb5f988d78d92) [DEPLOYING].
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 8a67fbcbb12c5e3d3e4580cb226c344b.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12) (8d1350263ba1ab4d5107aeef42a8c042) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12) (1183e83f7f32aeb257c62d98bdd29fde) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12) (71b39e7d8e901d4335ed260ac3b98a9a) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12) (19e39f2906efa43752f2d5604328423c) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12) (66afbca6e5bde01f13a0c8d2aff893fe) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@471d504
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12)#0 (12eff7c4e52121e42762e0667ab0484c) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12) (12eff7c4e52121e42762e0667ab0484c) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12)#0 (b5d851a6a7dbb2e1b1110ef7cfb5b16e), deploy into slot with allocation id 8a67fbcbb12c5e3d3e4580cb226c344b.
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12)#0 (b5d851a6a7dbb2e1b1110ef7cfb5b16e) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12)#0 (b5d851a6a7dbb2e1b1110ef7cfb5b16e) [DEPLOYING].
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 55c6de04cb91805b862f91ae4d64e8dd.
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@eb8777d
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4fd6308c
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12)#0 (c195972f058d9f8d15dcb5f988d78d92) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12)#0 (8b1abbbd0fae659a98fed29848c10269) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12) (c195972f058d9f8d15dcb5f988d78d92) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12) (8b1abbbd0fae659a98fed29848c10269) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@48abb54c
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12)#0 (b5d851a6a7dbb2e1b1110ef7cfb5b16e) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12) (b5d851a6a7dbb2e1b1110ef7cfb5b16e) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (1/12)#0 (4ffeec868027d8893982ba0cfccf25aa), deploy into slot with allocation id 55c6de04cb91805b862f91ae4d64e8dd.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (1/12)#0 (4ffeec868027d8893982ba0cfccf25aa) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (1/12)#0 (4ffeec868027d8893982ba0cfccf25aa) [DEPLOYING].
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 71a1649253a8cc2eab0824d4f7666548.
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (2/12)#0 (49678d9e9fa2d4f81c617dec3c0a40f3), deploy into slot with allocation id 71a1649253a8cc2eab0824d4f7666548.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (2/12)#0 (49678d9e9fa2d4f81c617dec3c0a40f3) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (2/12)#0 (49678d9e9fa2d4f81c617dec3c0a40f3) [DEPLOYING].
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot c758c7a7dd3c69b6ca9fce6c55e0e501.
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@10c7dc50
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (1/12)#0 (4ffeec868027d8893982ba0cfccf25aa) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - IcebergStreamWriter (1/12) (4ffeec868027d8893982ba0cfccf25aa) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (3/12)#0 (c1b0ca92f2a7179cf39f843395f7990f), deploy into slot with allocation id c758c7a7dd3c69b6ca9fce6c55e0e501.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (3/12)#0 (c1b0ca92f2a7179cf39f843395f7990f) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (3/12)#0 (c1b0ca92f2a7179cf39f843395f7990f) [DEPLOYING].
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 32d4ea0feb46b31220fd745316d24967.
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (4/12)#0 (9987d3651cd0a27ba8f90435c2b5b04d), deploy into slot with allocation id 32d4ea0feb46b31220fd745316d24967.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (4/12)#0 (9987d3651cd0a27ba8f90435c2b5b04d) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (4/12)#0 (9987d3651cd0a27ba8f90435c2b5b04d) [DEPLOYING].
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@692097e
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (2/12)#0 (49678d9e9fa2d4f81c617dec3c0a40f3) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - IcebergStreamWriter (2/12) (49678d9e9fa2d4f81c617dec3c0a40f3) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot ea99d8290930f008995a5311e5fde0f7.
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@15203702
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@71d69fb4
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (4/12)#0 (9987d3651cd0a27ba8f90435c2b5b04d) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (3/12)#0 (c1b0ca92f2a7179cf39f843395f7990f) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (5/12)#0 (d1080aeaed4380bfc211780df8e2482d), deploy into slot with allocation id ea99d8290930f008995a5311e5fde0f7.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (5/12)#0 (d1080aeaed4380bfc211780df8e2482d) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 4a3e4a4ed8b5311cd26b9df0788910e0.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (5/12)#0 (d1080aeaed4380bfc211780df8e2482d) [DEPLOYING].
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - IcebergStreamWriter (4/12) (9987d3651cd0a27ba8f90435c2b5b04d) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - IcebergStreamWriter (3/12) (c1b0ca92f2a7179cf39f843395f7990f) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@63bef08b
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (6/12)#0 (ef3b1509ace48addb80777f4180c3c22), deploy into slot with allocation id 4a3e4a4ed8b5311cd26b9df0788910e0.
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 50298afcbb3bc59699911b4443302f23.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (6/12)#0 (ef3b1509ace48addb80777f4180c3c22) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (5/12)#0 (d1080aeaed4380bfc211780df8e2482d) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - IcebergStreamWriter (5/12) (d1080aeaed4380bfc211780df8e2482d) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (6/12)#0 (ef3b1509ace48addb80777f4180c3c22) [DEPLOYING].
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (7/12)#0 (03cf1423d317c8fa651a76d17021c499), deploy into slot with allocation id 50298afcbb3bc59699911b4443302f23.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (7/12)#0 (03cf1423d317c8fa651a76d17021c499) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot df0ed4a7edbeeaa2e5ae33ce5a4b6cdf.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (7/12)#0 (03cf1423d317c8fa651a76d17021c499) [DEPLOYING].
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (8/12)#0 (e0eec9cf12181789031734154b51663a), deploy into slot with allocation id df0ed4a7edbeeaa2e5ae33ce5a4b6cdf.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (8/12)#0 (e0eec9cf12181789031734154b51663a) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot b8282f2f710d571ecaeb45986773341e.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (8/12)#0 (e0eec9cf12181789031734154b51663a) [DEPLOYING].
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (9/12)#0 (71ae8ec63711f2386b38f6c74a74116d), deploy into slot with allocation id b8282f2f710d571ecaeb45986773341e.
2023-12-26 10:34:47 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@76d6b57f
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3ff7e279
2023-12-26 10:34:47 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:34:47 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:34:47 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:34:47 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:34:47 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (7/12)#0 (03cf1423d317c8fa651a76d17021c499) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:34:47 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:34:47 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:34:47 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:34:47 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (9/12)#0 (71ae8ec63711f2386b38f6c74a74116d) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 367203734b68b79c12e22b56e22e6e55.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (6/12)#0 (ef3b1509ace48addb80777f4180c3c22) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - IcebergStreamWriter (7/12) (03cf1423d317c8fa651a76d17021c499) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (9/12)#0 (71ae8ec63711f2386b38f6c74a74116d) [DEPLOYING].
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - IcebergStreamWriter (6/12) (ef3b1509ace48addb80777f4180c3c22) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (10/12)#0 (f13c5623d865890c5c7ae5dcb07b361f), deploy into slot with allocation id 367203734b68b79c12e22b56e22e6e55.
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4e7d0d32
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7f27e340
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (10/12)#0 (f13c5623d865890c5c7ae5dcb07b361f) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (10/12)#0 (f13c5623d865890c5c7ae5dcb07b361f) [DEPLOYING].
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (8/12)#0 (e0eec9cf12181789031734154b51663a) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (9/12)#0 (71ae8ec63711f2386b38f6c74a74116d) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 8a67fbcbb12c5e3d3e4580cb226c344b.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - IcebergStreamWriter (8/12) (e0eec9cf12181789031734154b51663a) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - IcebergStreamWriter (9/12) (71ae8ec63711f2386b38f6c74a74116d) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@46131958
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (10/12)#0 (f13c5623d865890c5c7ae5dcb07b361f) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - IcebergStreamWriter (10/12) (f13c5623d865890c5c7ae5dcb07b361f) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (11/12)#0 (1f302fcdcc1283ca913611ccbfc5cfc5), deploy into slot with allocation id 8a67fbcbb12c5e3d3e4580cb226c344b.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (11/12)#0 (1f302fcdcc1283ca913611ccbfc5cfc5) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 73894cd91309d9ebef22c4c99aea62a8.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (11/12)#0 (1f302fcdcc1283ca913611ccbfc5cfc5) [DEPLOYING].
2023-12-26 10:34:47 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 5 has no restore state.
2023-12-26 10:34:47 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 2 has no restore state.
2023-12-26 10:34:47 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 1 has no restore state.
2023-12-26 10:34:47 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 4 has no restore state.
2023-12-26 10:34:47 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 6 has no restore state.
2023-12-26 10:34:47 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 8 has no restore state.
2023-12-26 10:34:47 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 9 has no restore state.
2023-12-26 10:34:47 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 11 has no restore state.
2023-12-26 10:34:47 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 0 has no restore state.
2023-12-26 10:34:47 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 3 has no restore state.
2023-12-26 10:34:47 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 10 has no restore state.
2023-12-26 10:34:47 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 7 has no restore state.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5eb4f335
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (11/12)#0 (1f302fcdcc1283ca913611ccbfc5cfc5) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (12/12)#0 (f13dfe3113921799459e036368ebada0), deploy into slot with allocation id 73894cd91309d9ebef22c4c99aea62a8.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - IcebergStreamWriter (11/12) (1f302fcdcc1283ca913611ccbfc5cfc5) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (12/12)#0 (f13dfe3113921799459e036368ebada0) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 55c6de04cb91805b862f91ae4d64e8dd.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (12/12)#0 (f13dfe3113921799459e036368ebada0) [DEPLOYING].
2023-12-26 10:34:47 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:34:47 INFO  TaskExecutor:722 - Received task IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1)#0 (3960532d678a52e47776e3d45dbe7b43), deploy into slot with allocation id 55c6de04cb91805b862f91ae4d64e8dd.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@30a72c5e
2023-12-26 10:34:47 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  Task:1067 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1)#0 (3960532d678a52e47776e3d45dbe7b43) switched from CREATED to DEPLOYING.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergStreamWriter (12/12)#0 (f13dfe3113921799459e036368ebada0) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - IcebergStreamWriter (12/12) (f13dfe3113921799459e036368ebada0) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 55c6de04cb91805b862f91ae4d64e8dd.
2023-12-26 10:34:47 INFO  Task:626 - Loading JAR files for task IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1)#0 (3960532d678a52e47776e3d45dbe7b43) [DEPLOYING].
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot df0ed4a7edbeeaa2e5ae33ce5a4b6cdf.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 8a67fbcbb12c5e3d3e4580cb226c344b.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 32d4ea0feb46b31220fd745316d24967.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot c758c7a7dd3c69b6ca9fce6c55e0e501.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 71a1649253a8cc2eab0824d4f7666548.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot ea99d8290930f008995a5311e5fde0f7.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 367203734b68b79c12e22b56e22e6e55.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot b8282f2f710d571ecaeb45986773341e.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 4a3e4a4ed8b5311cd26b9df0788910e0.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 73894cd91309d9ebef22c4c99aea62a8.
2023-12-26 10:34:47 INFO  TaskSlotTableImpl:388 - Activate slot 50298afcbb3bc59699911b4443302f23.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:34:47 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@63eb5e28
2023-12-26 10:34:47 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:34:47 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:34:47 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:34:47 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:34:47 INFO  Task:1067 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1)#0 (3960532d678a52e47776e3d45dbe7b43) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:47 INFO  ExecutionGraph:1438 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1) (3960532d678a52e47776e3d45dbe7b43) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:34:48 INFO  Task:1067 - IcebergStreamWriter (7/12)#0 (03cf1423d317c8fa651a76d17021c499) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  Task:1067 - IcebergStreamWriter (8/12)#0 (e0eec9cf12181789031734154b51663a) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  Task:1067 - IcebergStreamWriter (9/12)#0 (71ae8ec63711f2386b38f6c74a74116d) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  Task:1067 - IcebergStreamWriter (5/12)#0 (d1080aeaed4380bfc211780df8e2482d) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  Task:1067 - IcebergStreamWriter (2/12)#0 (49678d9e9fa2d4f81c617dec3c0a40f3) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  Task:1067 - IcebergStreamWriter (3/12)#0 (c1b0ca92f2a7179cf39f843395f7990f) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  Task:1067 - IcebergStreamWriter (11/12)#0 (1f302fcdcc1283ca913611ccbfc5cfc5) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  Task:1067 - IcebergStreamWriter (12/12)#0 (f13dfe3113921799459e036368ebada0) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  Task:1067 - IcebergStreamWriter (4/12)#0 (9987d3651cd0a27ba8f90435c2b5b04d) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  Task:1067 - IcebergStreamWriter (1/12)#0 (4ffeec868027d8893982ba0cfccf25aa) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  Task:1067 - IcebergStreamWriter (6/12)#0 (ef3b1509ace48addb80777f4180c3c22) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  ExecutionGraph:1438 - IcebergStreamWriter (9/12) (71ae8ec63711f2386b38f6c74a74116d) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  Task:1067 - IcebergStreamWriter (10/12)#0 (f13c5623d865890c5c7ae5dcb07b361f) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  ExecutionGraph:1438 - IcebergStreamWriter (12/12) (f13dfe3113921799459e036368ebada0) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  ExecutionGraph:1438 - IcebergStreamWriter (8/12) (e0eec9cf12181789031734154b51663a) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  ExecutionGraph:1438 - IcebergStreamWriter (2/12) (49678d9e9fa2d4f81c617dec3c0a40f3) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  ExecutionGraph:1438 - IcebergStreamWriter (11/12) (1f302fcdcc1283ca913611ccbfc5cfc5) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  ExecutionGraph:1438 - IcebergStreamWriter (3/12) (c1b0ca92f2a7179cf39f843395f7990f) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  ExecutionGraph:1438 - IcebergStreamWriter (4/12) (9987d3651cd0a27ba8f90435c2b5b04d) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  ExecutionGraph:1438 - IcebergStreamWriter (1/12) (4ffeec868027d8893982ba0cfccf25aa) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  ExecutionGraph:1438 - IcebergStreamWriter (7/12) (03cf1423d317c8fa651a76d17021c499) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  ExecutionGraph:1438 - IcebergStreamWriter (6/12) (ef3b1509ace48addb80777f4180c3c22) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  ExecutionGraph:1438 - IcebergStreamWriter (5/12) (d1080aeaed4380bfc211780df8e2482d) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  ExecutionGraph:1438 - IcebergStreamWriter (10/12) (f13c5623d865890c5c7ae5dcb07b361f) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:48 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:48 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:48 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:48 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:48 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:48 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:48 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:48 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:48 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:48 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:48 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:48 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:48 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:48 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:48 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:48 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:48 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:48 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:48 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:48 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:48 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:48 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:48 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:48 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:48 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:48 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:48 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558088277
2023-12-26 10:34:48 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:48 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:48 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558088281
2023-12-26 10:34:48 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:48 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:48 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558088281
2023-12-26 10:34:48 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:48 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:48 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558088281
2023-12-26 10:34:48 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:48 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:48 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558088281
2023-12-26 10:34:48 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:48 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:48 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558088281
2023-12-26 10:34:48 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:48 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:48 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558088281
2023-12-26 10:34:48 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:48 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:48 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558088281
2023-12-26 10:34:48 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:48 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:48 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558088281
2023-12-26 10:34:48 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:48 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:48 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558088281
2023-12-26 10:34:48 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:48 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:48 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558088285
2023-12-26 10:34:48 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:48 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:48 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558088285
2023-12-26 10:34:48 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00000-ce30aa22-5d7a-4c14-ba2e-5bbcf37e4a97.metadata.json
2023-12-26 10:34:49 INFO  CheckpointCoordinator:888 - Failed to trigger checkpoint for job a6871a695473e51d3520688e0f3cd838 since Checkpoint triggering task Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) of job a6871a695473e51d3520688e0f3cd838 is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
2023-12-26 10:34:50 INFO  BaseMetastoreCatalog:64 - Table loaded by catalog: iceberg_catalog.db1.ice_T01
2023-12-26 10:34:50 INFO  Task:1067 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1)#0 (3960532d678a52e47776e3d45dbe7b43) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  ExecutionGraph:1438 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db1.ice_T01 (1/1) (3960532d678a52e47776e3d45dbe7b43) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  Metadata:259 - [Consumer clientId=consumer-g1-3, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:34:50 INFO  Metadata:259 - [Consumer clientId=consumer-g1-11, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:34:50 INFO  Metadata:259 - [Consumer clientId=consumer-g1-12, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:34:50 INFO  Metadata:259 - [Consumer clientId=consumer-g1-9, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:34:50 INFO  Metadata:259 - [Consumer clientId=consumer-g1-10, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:34:50 INFO  Metadata:259 - [Consumer clientId=consumer-g1-4, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:34:50 INFO  Metadata:259 - [Consumer clientId=consumer-g1-6, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:34:50 INFO  Metadata:259 - [Consumer clientId=consumer-g1-7, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:34:50 INFO  Metadata:259 - [Consumer clientId=consumer-g1-8, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:34:50 INFO  Metadata:259 - [Consumer clientId=consumer-g1-2, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:34:50 INFO  Metadata:259 - [Consumer clientId=consumer-g1-1, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:34:50 INFO  Metadata:259 - [Consumer clientId=consumer-g1-5, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 0 initially has no partitions to read from.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 3 initially has no partitions to read from.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 4 initially has no partitions to read from.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 5 initially has no partitions to read from.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 10 initially has no partitions to read from.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:696 - Consumer subtask 7 will start reading the following 1 partitions from the latest offsets: [KafkaTopicPartition{topic='T01', partition=0}]
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 6 initially has no partitions to read from.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 9 initially has no partitions to read from.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 1 initially has no partitions to read from.
2023-12-26 10:34:50 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12)#0 (4a50400e9e1ec36082990ffe195856b9) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12)#0 (314bc688b9420fcfd3b3297575935dca) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12)#0 (2f2ed81100f551cc852e9ef5b4686fd9) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 8 initially has no partitions to read from.
2023-12-26 10:34:50 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12)#0 (8b1abbbd0fae659a98fed29848c10269) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12)#0 (19e39f2906efa43752f2d5604328423c) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12)#0 (b5d851a6a7dbb2e1b1110ef7cfb5b16e) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12)#0 (71b39e7d8e901d4335ed260ac3b98a9a) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) (2f2ed81100f551cc852e9ef5b4686fd9) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 2 initially has no partitions to read from.
2023-12-26 10:34:50 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12)#0 (12eff7c4e52121e42762e0667ab0484c) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12)#0 (1183e83f7f32aeb257c62d98bdd29fde) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12)#0 (66afbca6e5bde01f13a0c8d2aff893fe) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 11 initially has no partitions to read from.
2023-12-26 10:34:50 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12)#0 (8d1350263ba1ab4d5107aeef42a8c042) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12) (8b1abbbd0fae659a98fed29848c10269) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12) (19e39f2906efa43752f2d5604328423c) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12) (b5d851a6a7dbb2e1b1110ef7cfb5b16e) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12) (71b39e7d8e901d4335ed260ac3b98a9a) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12)#0 (c195972f058d9f8d15dcb5f988d78d92) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12) (314bc688b9420fcfd3b3297575935dca) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12) (4a50400e9e1ec36082990ffe195856b9) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12) (12eff7c4e52121e42762e0667ab0484c) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12) (1183e83f7f32aeb257c62d98bdd29fde) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 4 creating fetcher with offsets {}.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 10 creating fetcher with offsets {}.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 9 creating fetcher with offsets {}.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 3 creating fetcher with offsets {}.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 5 creating fetcher with offsets {}.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 7 creating fetcher with offsets {KafkaTopicPartition{topic='T01', partition=0}=-915623761774}.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 1 creating fetcher with offsets {}.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 0 creating fetcher with offsets {}.
2023-12-26 10:34:50 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12) (66afbca6e5bde01f13a0c8d2aff893fe) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 6 creating fetcher with offsets {}.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 2 creating fetcher with offsets {}.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 8 creating fetcher with offsets {}.
2023-12-26 10:34:50 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 11 creating fetcher with offsets {}.
2023-12-26 10:34:50 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12) (8d1350263ba1ab4d5107aeef42a8c042) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db1, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12) (c195972f058d9f8d15dcb5f988d78d92) switched from INITIALIZING to RUNNING.
2023-12-26 10:34:50 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:50 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:50 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:50 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:50 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:50 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:50 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:50 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:50 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:50 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:50 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:50 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:34:50 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:50 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:50 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:50 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:50 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558090703
2023-12-26 10:34:50 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:50 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:50 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:50 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:50 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:50 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:50 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558090706
2023-12-26 10:34:50 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:50 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:50 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:50 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:50 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:50 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:50 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558090706
2023-12-26 10:34:50 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:50 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:34:50 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:50 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:50 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558090706
2023-12-26 10:34:50 INFO  KafkaConsumer:1123 - [Consumer clientId=consumer-g1-23, groupId=g1] Subscribed to partition(s): T01-0
2023-12-26 10:34:50 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:50 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:50 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558090706
2023-12-26 10:34:50 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:50 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:50 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558090703
2023-12-26 10:34:50 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:50 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:50 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558090711
2023-12-26 10:34:50 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:50 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:50 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558090711
2023-12-26 10:34:50 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:50 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:50 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558090711
2023-12-26 10:34:50 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:50 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:50 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558090711
2023-12-26 10:34:50 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:50 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:50 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558090710
2023-12-26 10:34:50 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:34:50 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:34:50 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703558090710
2023-12-26 10:34:50 INFO  SubscriptionState:564 - [Consumer clientId=consumer-g1-23, groupId=g1] Seeking to LATEST offset of partition T01-0
2023-12-26 10:34:51 INFO  Metadata:259 - [Consumer clientId=consumer-g1-23, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:34:51 INFO  SubscriptionState:381 - [Consumer clientId=consumer-g1-23, groupId=g1] Resetting offset for partition T01-0 to offset 46.
2023-12-26 10:34:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 1 (type=CHECKPOINT) @ 1703558099904 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:35:00 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 1
2023-12-26 10:35:00 INFO  CheckpointCoordinator:1246 - Completed checkpoint 1 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 448 ms).
2023-12-26 10:35:00 INFO  AbstractCoordinator:756 - [Consumer clientId=consumer-g1-23, groupId=g1] Discovered group coordinator 172.20.3.227:9092 (id: 2147483647 rack: null)
2023-12-26 10:35:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 2 (type=CHECKPOINT) @ 1703558109903 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:35:10 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 2
2023-12-26 10:35:10 INFO  CheckpointCoordinator:1246 - Completed checkpoint 2 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 147 ms).
2023-12-26 10:35:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 3 (type=CHECKPOINT) @ 1703558119904 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:35:19 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 3
2023-12-26 10:35:20 INFO  CheckpointCoordinator:1246 - Completed checkpoint 3 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 101 ms).
2023-12-26 10:35:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 4 (type=CHECKPOINT) @ 1703558129904 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:35:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 4
2023-12-26 10:35:30 INFO  CheckpointCoordinator:1246 - Completed checkpoint 4 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 109 ms).
2023-12-26 10:35:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 5 (type=CHECKPOINT) @ 1703558139903 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:35:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 5
2023-12-26 10:35:39 INFO  CheckpointCoordinator:1246 - Completed checkpoint 5 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 57 ms).
2023-12-26 10:35:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 6 (type=CHECKPOINT) @ 1703558149906 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:35:50 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 6
2023-12-26 10:35:50 INFO  CheckpointCoordinator:1246 - Completed checkpoint 6 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 113 ms).
2023-12-26 10:35:54 ERROR TaskManagerDetailsHandler:260 - Unhandled exception.
org.apache.flink.runtime.resourcemanager.exceptions.UnknownTaskExecutorException: No TaskExecutor registered under 83f9e70c-9728-4659-887f-cb09e984cb47.
	at org.apache.flink.runtime.resourcemanager.ResourceManager.requestTaskManagerDetailsInfo(ResourceManager.java:683)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2023-12-26 10:35:57 ERROR TaskManagerDetailsHandler:260 - Unhandled exception.
org.apache.flink.runtime.resourcemanager.exceptions.UnknownTaskExecutorException: No TaskExecutor registered under 83f9e70c-9728-4659-887f-cb09e984cb47.
	at org.apache.flink.runtime.resourcemanager.ResourceManager.requestTaskManagerDetailsInfo(ResourceManager.java:683)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2023-12-26 10:35:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 7 (type=CHECKPOINT) @ 1703558159909 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:35:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 7
2023-12-26 10:35:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 7 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 77 ms).
2023-12-26 10:36:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 8 (type=CHECKPOINT) @ 1703558169906 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:36:10 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 8
2023-12-26 10:36:10 INFO  CheckpointCoordinator:1246 - Completed checkpoint 8 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 110 ms).
2023-12-26 10:36:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 9 (type=CHECKPOINT) @ 1703558179906 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:36:20 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 9
2023-12-26 10:36:20 INFO  CheckpointCoordinator:1246 - Completed checkpoint 9 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 99 ms).
2023-12-26 10:36:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 10 (type=CHECKPOINT) @ 1703558189902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:36:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 10
2023-12-26 10:36:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 10 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 46 ms).
2023-12-26 10:36:29 INFO  IcebergFilesCommitter:306 - Committing append with 0 data files and 0 delete files to table iceberg_catalog.db1.ice_T01
2023-12-26 10:36:32 INFO  HiveTableOperations:315 - Committed to table iceberg_catalog.db1.ice_T01 with the new metadata location hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00001-2865f4cd-3038-45d5-83b7-f5d6c35b8a5e.metadata.json
2023-12-26 10:36:32 INFO  BaseMetastoreTableOperations:137 - Successfully committed to table iceberg_catalog.db1.ice_T01 in 1406 ms
2023-12-26 10:36:32 INFO  SnapshotProducer:327 - Committed snapshot 1810989247364353772 (MergeAppend)
2023-12-26 10:36:32 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00001-2865f4cd-3038-45d5-83b7-f5d6c35b8a5e.metadata.json
2023-12-26 10:36:34 INFO  IcebergFilesCommitter:314 - Committed in 4187 ms
2023-12-26 10:36:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 11 (type=CHECKPOINT) @ 1703558199908 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:36:40 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 11
2023-12-26 10:36:40 INFO  CheckpointCoordinator:1246 - Completed checkpoint 11 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 135 ms).
2023-12-26 10:36:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 12 (type=CHECKPOINT) @ 1703558209906 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:36:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 12
2023-12-26 10:36:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 12 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 75 ms).
2023-12-26 10:36:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 13 (type=CHECKPOINT) @ 1703558219905 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:36:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 13
2023-12-26 10:37:00 INFO  CheckpointCoordinator:1246 - Completed checkpoint 13 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 95 ms).
2023-12-26 10:37:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 14 (type=CHECKPOINT) @ 1703558229903 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:37:09 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 14
2023-12-26 10:37:09 INFO  CheckpointCoordinator:1246 - Completed checkpoint 14 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 82 ms).
2023-12-26 10:37:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 15 (type=CHECKPOINT) @ 1703558239909 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:37:19 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 15
2023-12-26 10:37:20 INFO  CheckpointCoordinator:1246 - Completed checkpoint 15 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 107 ms).
2023-12-26 10:37:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 16 (type=CHECKPOINT) @ 1703558249904 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:37:30 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 16
2023-12-26 10:37:30 INFO  CheckpointCoordinator:1246 - Completed checkpoint 16 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 113 ms).
2023-12-26 10:37:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 17 (type=CHECKPOINT) @ 1703558259905 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:37:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 17
2023-12-26 10:37:39 INFO  CheckpointCoordinator:1246 - Completed checkpoint 17 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 53 ms).
2023-12-26 10:37:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 18 (type=CHECKPOINT) @ 1703558269908 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:37:50 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 18
2023-12-26 10:37:50 INFO  CheckpointCoordinator:1246 - Completed checkpoint 18 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 99 ms).
2023-12-26 10:37:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 19 (type=CHECKPOINT) @ 1703558279901 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:37:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 19
2023-12-26 10:37:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 19 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 60 ms).
2023-12-26 10:38:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 20 (type=CHECKPOINT) @ 1703558289905 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:38:09 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 20
2023-12-26 10:38:09 INFO  CheckpointCoordinator:1246 - Completed checkpoint 20 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 86 ms).
2023-12-26 10:38:09 INFO  IcebergFilesCommitter:306 - Committing append with 0 data files and 0 delete files to table iceberg_catalog.db1.ice_T01
2023-12-26 10:38:11 INFO  HiveTableOperations:315 - Committed to table iceberg_catalog.db1.ice_T01 with the new metadata location hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00002-29526458-441a-46c6-9670-1266d5cf5e14.metadata.json
2023-12-26 10:38:11 INFO  BaseMetastoreTableOperations:137 - Successfully committed to table iceberg_catalog.db1.ice_T01 in 1009 ms
2023-12-26 10:38:11 INFO  SnapshotProducer:327 - Committed snapshot 6377432822049050142 (MergeAppend)
2023-12-26 10:38:12 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00002-29526458-441a-46c6-9670-1266d5cf5e14.metadata.json
2023-12-26 10:38:12 INFO  IcebergFilesCommitter:314 - Committed in 2976 ms
2023-12-26 10:38:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 21 (type=CHECKPOINT) @ 1703558299908 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:38:20 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 21
2023-12-26 10:38:20 INFO  CheckpointCoordinator:1246 - Completed checkpoint 21 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 110 ms).
2023-12-26 10:38:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 22 (type=CHECKPOINT) @ 1703558309902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:38:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 22
2023-12-26 10:38:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 22 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 72 ms).
2023-12-26 10:38:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 23 (type=CHECKPOINT) @ 1703558319904 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:38:40 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 23
2023-12-26 10:38:40 INFO  CheckpointCoordinator:1246 - Completed checkpoint 23 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 119 ms).
2023-12-26 10:38:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 24 (type=CHECKPOINT) @ 1703558329905 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:38:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 24
2023-12-26 10:38:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 24 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 33 ms).
2023-12-26 10:38:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 25 (type=CHECKPOINT) @ 1703558339902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:38:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 25
2023-12-26 10:38:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 25 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 50 ms).
2023-12-26 10:39:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 26 (type=CHECKPOINT) @ 1703558349907 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:39:10 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 26
2023-12-26 10:39:10 INFO  CheckpointCoordinator:1246 - Completed checkpoint 26 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 121 ms).
2023-12-26 10:39:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 27 (type=CHECKPOINT) @ 1703558359903 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:39:19 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 27
2023-12-26 10:39:19 INFO  CheckpointCoordinator:1246 - Completed checkpoint 27 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 49 ms).
2023-12-26 10:39:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 28 (type=CHECKPOINT) @ 1703558369905 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:39:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 28
2023-12-26 10:39:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 28 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 90 ms).
2023-12-26 10:39:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 29 (type=CHECKPOINT) @ 1703558379906 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:39:40 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 29
2023-12-26 10:39:40 INFO  CheckpointCoordinator:1246 - Completed checkpoint 29 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 102 ms).
2023-12-26 10:39:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 30 (type=CHECKPOINT) @ 1703558389904 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:39:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 30
2023-12-26 10:39:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 30 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 51 ms).
2023-12-26 10:39:49 INFO  IcebergFilesCommitter:306 - Committing append with 0 data files and 0 delete files to table iceberg_catalog.db1.ice_T01
2023-12-26 10:39:52 INFO  HiveTableOperations:315 - Committed to table iceberg_catalog.db1.ice_T01 with the new metadata location hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00003-3de75ad2-67c3-4793-9cdc-5ba15bb805e0.metadata.json
2023-12-26 10:39:52 INFO  BaseMetastoreTableOperations:137 - Successfully committed to table iceberg_catalog.db1.ice_T01 in 1207 ms
2023-12-26 10:39:52 INFO  SnapshotProducer:327 - Committed snapshot 1192718043533707225 (MergeAppend)
2023-12-26 10:39:52 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00003-3de75ad2-67c3-4793-9cdc-5ba15bb805e0.metadata.json
2023-12-26 10:39:53 INFO  IcebergFilesCommitter:314 - Committed in 3130 ms
2023-12-26 10:39:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 31 (type=CHECKPOINT) @ 1703558399907 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:39:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 31
2023-12-26 10:39:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 31 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 88 ms).
2023-12-26 10:40:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 32 (type=CHECKPOINT) @ 1703558409908 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:40:10 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 32
2023-12-26 10:40:10 INFO  CheckpointCoordinator:1246 - Completed checkpoint 32 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 112 ms).
2023-12-26 10:40:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 33 (type=CHECKPOINT) @ 1703558419902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:40:20 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 33
2023-12-26 10:40:20 INFO  CheckpointCoordinator:1246 - Completed checkpoint 33 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 112 ms).
2023-12-26 10:40:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 34 (type=CHECKPOINT) @ 1703558429903 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:40:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 34
2023-12-26 10:40:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 34 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 81 ms).
2023-12-26 10:40:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 35 (type=CHECKPOINT) @ 1703558439902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:40:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 35
2023-12-26 10:40:39 INFO  CheckpointCoordinator:1246 - Completed checkpoint 35 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 48 ms).
2023-12-26 10:40:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 36 (type=CHECKPOINT) @ 1703558449901 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:40:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 36
2023-12-26 10:40:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 36 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 37 ms).
2023-12-26 10:40:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 37 (type=CHECKPOINT) @ 1703558459902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:40:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 37
2023-12-26 10:40:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 37 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 80 ms).
2023-12-26 10:41:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 38 (type=CHECKPOINT) @ 1703558469904 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:41:09 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 38
2023-12-26 10:41:10 INFO  CheckpointCoordinator:1246 - Completed checkpoint 38 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 97 ms).
2023-12-26 10:41:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 39 (type=CHECKPOINT) @ 1703558479904 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:41:19 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 39
2023-12-26 10:41:19 INFO  CheckpointCoordinator:1246 - Completed checkpoint 39 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 32 ms).
2023-12-26 10:41:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 40 (type=CHECKPOINT) @ 1703558489902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:41:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 40
2023-12-26 10:41:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 40 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 43 ms).
2023-12-26 10:41:29 INFO  IcebergFilesCommitter:306 - Committing append with 0 data files and 0 delete files to table iceberg_catalog.db1.ice_T01
2023-12-26 10:41:32 INFO  HiveTableOperations:315 - Committed to table iceberg_catalog.db1.ice_T01 with the new metadata location hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00004-20c47764-0ff3-4b13-a6e3-2a0a2b9a3df8.metadata.json
2023-12-26 10:41:32 INFO  BaseMetastoreTableOperations:137 - Successfully committed to table iceberg_catalog.db1.ice_T01 in 1167 ms
2023-12-26 10:41:32 INFO  SnapshotProducer:327 - Committed snapshot 804844340807446150 (MergeAppend)
2023-12-26 10:41:32 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00004-20c47764-0ff3-4b13-a6e3-2a0a2b9a3df8.metadata.json
2023-12-26 10:41:33 INFO  IcebergFilesCommitter:314 - Committed in 3132 ms
2023-12-26 10:41:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 41 (type=CHECKPOINT) @ 1703558499908 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:41:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 41
2023-12-26 10:41:40 INFO  CheckpointCoordinator:1246 - Completed checkpoint 41 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 93 ms).
2023-12-26 10:41:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 42 (type=CHECKPOINT) @ 1703558509903 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:41:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 42
2023-12-26 10:41:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 42 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 34 ms).
2023-12-26 10:41:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 43 (type=CHECKPOINT) @ 1703558519902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:41:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 43
2023-12-26 10:41:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 43 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 49 ms).
2023-12-26 10:42:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 44 (type=CHECKPOINT) @ 1703558529908 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:42:10 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 44
2023-12-26 10:42:10 INFO  CheckpointCoordinator:1246 - Completed checkpoint 44 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 103 ms).
2023-12-26 10:42:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 45 (type=CHECKPOINT) @ 1703558539902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:42:19 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 45
2023-12-26 10:42:19 INFO  CheckpointCoordinator:1246 - Completed checkpoint 45 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 89 ms).
2023-12-26 10:42:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 46 (type=CHECKPOINT) @ 1703558549903 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:42:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 46
2023-12-26 10:42:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 46 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 44 ms).
2023-12-26 10:42:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 47 (type=CHECKPOINT) @ 1703558559902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:42:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 47
2023-12-26 10:42:39 INFO  CheckpointCoordinator:1246 - Completed checkpoint 47 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 38 ms).
2023-12-26 10:42:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 48 (type=CHECKPOINT) @ 1703558569906 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:42:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 48
2023-12-26 10:42:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 48 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 64 ms).
2023-12-26 10:42:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 49 (type=CHECKPOINT) @ 1703558579901 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:42:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 49
2023-12-26 10:42:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 49 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 88 ms).
2023-12-26 10:43:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 50 (type=CHECKPOINT) @ 1703558589902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:43:09 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 50
2023-12-26 10:43:09 INFO  CheckpointCoordinator:1246 - Completed checkpoint 50 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 23 ms).
2023-12-26 10:43:09 INFO  IcebergFilesCommitter:306 - Committing append with 0 data files and 0 delete files to table iceberg_catalog.db1.ice_T01
2023-12-26 10:43:12 INFO  HiveTableOperations:315 - Committed to table iceberg_catalog.db1.ice_T01 with the new metadata location hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00005-b94bede3-3b09-42f1-ae86-18c72da95ff7.metadata.json
2023-12-26 10:43:12 INFO  BaseMetastoreTableOperations:137 - Successfully committed to table iceberg_catalog.db1.ice_T01 in 1425 ms
2023-12-26 10:43:12 INFO  SnapshotProducer:327 - Committed snapshot 1706119787612471972 (MergeAppend)
2023-12-26 10:43:12 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00005-b94bede3-3b09-42f1-ae86-18c72da95ff7.metadata.json
2023-12-26 10:43:13 INFO  IcebergFilesCommitter:314 - Committed in 3206 ms
2023-12-26 10:43:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 51 (type=CHECKPOINT) @ 1703558599905 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:43:19 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 51
2023-12-26 10:43:19 INFO  CheckpointCoordinator:1246 - Completed checkpoint 51 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 89 ms).
2023-12-26 10:43:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 52 (type=CHECKPOINT) @ 1703558609904 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:43:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 52
2023-12-26 10:43:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 52 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 23 ms).
2023-12-26 10:43:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 53 (type=CHECKPOINT) @ 1703558619901 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:43:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 53
2023-12-26 10:43:39 INFO  CheckpointCoordinator:1246 - Completed checkpoint 53 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 26 ms).
2023-12-26 10:43:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 54 (type=CHECKPOINT) @ 1703558629901 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:43:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 54
2023-12-26 10:43:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 54 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 41 ms).
2023-12-26 10:43:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 55 (type=CHECKPOINT) @ 1703558639901 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:43:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 55
2023-12-26 10:43:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 55 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 95 ms).
2023-12-26 10:44:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 56 (type=CHECKPOINT) @ 1703558649904 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:44:09 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 56
2023-12-26 10:44:09 INFO  CheckpointCoordinator:1246 - Completed checkpoint 56 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 54 ms).
2023-12-26 10:44:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 57 (type=CHECKPOINT) @ 1703558659903 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:44:19 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 57
2023-12-26 10:44:19 INFO  CheckpointCoordinator:1246 - Completed checkpoint 57 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 36 ms).
2023-12-26 10:44:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 58 (type=CHECKPOINT) @ 1703558669899 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:44:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 58
2023-12-26 10:44:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 58 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 12 ms).
2023-12-26 10:44:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 59 (type=CHECKPOINT) @ 1703558679903 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:44:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 59
2023-12-26 10:44:39 INFO  CheckpointCoordinator:1246 - Completed checkpoint 59 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 17 ms).
2023-12-26 10:44:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 60 (type=CHECKPOINT) @ 1703558689903 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:44:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 60
2023-12-26 10:44:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 60 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 21 ms).
2023-12-26 10:44:49 INFO  IcebergFilesCommitter:306 - Committing append with 0 data files and 0 delete files to table iceberg_catalog.db1.ice_T01
2023-12-26 10:44:51 INFO  HiveTableOperations:315 - Committed to table iceberg_catalog.db1.ice_T01 with the new metadata location hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00006-6d5a162f-b52b-4307-9dda-3f21f2796731.metadata.json
2023-12-26 10:44:51 INFO  BaseMetastoreTableOperations:137 - Successfully committed to table iceberg_catalog.db1.ice_T01 in 1035 ms
2023-12-26 10:44:51 INFO  SnapshotProducer:327 - Committed snapshot 1717162716421681520 (MergeAppend)
2023-12-26 10:44:51 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00006-6d5a162f-b52b-4307-9dda-3f21f2796731.metadata.json
2023-12-26 10:44:52 INFO  IcebergFilesCommitter:314 - Committed in 3023 ms
2023-12-26 10:44:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 61 (type=CHECKPOINT) @ 1703558699900 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:44:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 61
2023-12-26 10:44:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 61 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 83 ms).
2023-12-26 10:45:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 62 (type=CHECKPOINT) @ 1703558709899 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:45:09 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 62
2023-12-26 10:45:09 INFO  CheckpointCoordinator:1246 - Completed checkpoint 62 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 17 ms).
2023-12-26 10:45:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 63 (type=CHECKPOINT) @ 1703558719902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:45:19 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 63
2023-12-26 10:45:19 INFO  CheckpointCoordinator:1246 - Completed checkpoint 63 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 29 ms).
2023-12-26 10:45:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 64 (type=CHECKPOINT) @ 1703558729899 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:45:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 64
2023-12-26 10:45:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 64 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 20 ms).
2023-12-26 10:45:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 65 (type=CHECKPOINT) @ 1703558739903 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:45:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 65
2023-12-26 10:45:39 INFO  CheckpointCoordinator:1246 - Completed checkpoint 65 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 29 ms).
2023-12-26 10:45:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 66 (type=CHECKPOINT) @ 1703558749903 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:45:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 66
2023-12-26 10:45:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 66 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 27 ms).
2023-12-26 10:45:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 67 (type=CHECKPOINT) @ 1703558759901 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:45:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 67
2023-12-26 10:45:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 67 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 71 ms).
2023-12-26 10:46:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 68 (type=CHECKPOINT) @ 1703558769900 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:46:09 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 68
2023-12-26 10:46:09 INFO  CheckpointCoordinator:1246 - Completed checkpoint 68 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 50 ms).
2023-12-26 10:46:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 69 (type=CHECKPOINT) @ 1703558779902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:46:19 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 69
2023-12-26 10:46:19 INFO  CheckpointCoordinator:1246 - Completed checkpoint 69 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 25 ms).
2023-12-26 10:46:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 70 (type=CHECKPOINT) @ 1703558789900 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:46:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 70
2023-12-26 10:46:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 70 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 24 ms).
2023-12-26 10:46:29 INFO  IcebergFilesCommitter:306 - Committing append with 0 data files and 0 delete files to table iceberg_catalog.db1.ice_T01
2023-12-26 10:46:32 INFO  HiveTableOperations:315 - Committed to table iceberg_catalog.db1.ice_T01 with the new metadata location hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00007-52ad4961-03dd-49e4-9d40-4d3d1848f83d.metadata.json
2023-12-26 10:46:32 INFO  BaseMetastoreTableOperations:137 - Successfully committed to table iceberg_catalog.db1.ice_T01 in 1096 ms
2023-12-26 10:46:32 INFO  SnapshotProducer:327 - Committed snapshot 3428673476586756814 (MergeAppend)
2023-12-26 10:46:32 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00007-52ad4961-03dd-49e4-9d40-4d3d1848f83d.metadata.json
2023-12-26 10:46:33 INFO  IcebergFilesCommitter:314 - Committed in 3431 ms
2023-12-26 10:46:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 71 (type=CHECKPOINT) @ 1703558799903 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:46:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 71
2023-12-26 10:46:39 INFO  CheckpointCoordinator:1246 - Completed checkpoint 71 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 79 ms).
2023-12-26 10:46:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 72 (type=CHECKPOINT) @ 1703558809899 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:46:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 72
2023-12-26 10:46:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 72 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 49 ms).
2023-12-26 10:46:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 73 (type=CHECKPOINT) @ 1703558819899 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:46:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 73
2023-12-26 10:46:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 73 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 47 ms).
2023-12-26 10:47:05 INFO  CodecPool:153 - Got brand-new compressor [.gz]
2023-12-26 10:47:06 INFO  CodecPool:153 - Got brand-new compressor [.gz]
2023-12-26 10:47:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 74 (type=CHECKPOINT) @ 1703558829900 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:47:11 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 74
2023-12-26 10:47:12 INFO  CheckpointCoordinator:1246 - Completed checkpoint 74 for job a6871a695473e51d3520688e0f3cd838 (15783 bytes in 2172 ms).
2023-12-26 10:47:13 INFO  IcebergFilesCommitter:306 - Committing rowDelta with 0 data files and 1 delete files to table iceberg_catalog.db1.ice_T01
2023-12-26 10:47:17 INFO  HiveTableOperations:315 - Committed to table iceberg_catalog.db1.ice_T01 with the new metadata location hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00008-33327b1e-009e-4da8-a47c-9b64b43f05a6.metadata.json
2023-12-26 10:47:17 INFO  BaseMetastoreTableOperations:137 - Successfully committed to table iceberg_catalog.db1.ice_T01 in 1417 ms
2023-12-26 10:47:17 INFO  SnapshotProducer:327 - Committed snapshot 2410183073869859141 (BaseRowDelta)
2023-12-26 10:47:17 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00008-33327b1e-009e-4da8-a47c-9b64b43f05a6.metadata.json
2023-12-26 10:47:18 INFO  IcebergFilesCommitter:314 - Committed in 5442 ms
2023-12-26 10:47:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 75 (type=CHECKPOINT) @ 1703558839903 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:47:20 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 75
2023-12-26 10:47:20 INFO  CheckpointCoordinator:1246 - Completed checkpoint 75 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 114 ms).
2023-12-26 10:47:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 76 (type=CHECKPOINT) @ 1703558849898 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:47:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 76
2023-12-26 10:47:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 76 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 57 ms).
2023-12-26 10:47:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 77 (type=CHECKPOINT) @ 1703558859900 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:47:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 77
2023-12-26 10:47:39 INFO  CheckpointCoordinator:1246 - Completed checkpoint 77 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 33 ms).
2023-12-26 10:47:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 78 (type=CHECKPOINT) @ 1703558869903 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:47:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 78
2023-12-26 10:47:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 78 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 85 ms).
2023-12-26 10:47:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 79 (type=CHECKPOINT) @ 1703558879899 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:47:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 79
2023-12-26 10:47:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 79 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 28 ms).
2023-12-26 10:48:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 80 (type=CHECKPOINT) @ 1703558889901 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:48:09 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 80
2023-12-26 10:48:09 INFO  CheckpointCoordinator:1246 - Completed checkpoint 80 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 21 ms).
2023-12-26 10:48:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 81 (type=CHECKPOINT) @ 1703558899898 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:48:19 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 81
2023-12-26 10:48:19 INFO  CheckpointCoordinator:1246 - Completed checkpoint 81 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 24 ms).
2023-12-26 10:48:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 82 (type=CHECKPOINT) @ 1703558909901 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:48:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 82
2023-12-26 10:48:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 82 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 23 ms).
2023-12-26 10:48:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 83 (type=CHECKPOINT) @ 1703558919901 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:48:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 83
2023-12-26 10:48:39 INFO  CheckpointCoordinator:1246 - Completed checkpoint 83 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 42 ms).
2023-12-26 10:48:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 84 (type=CHECKPOINT) @ 1703558929897 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:48:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 84
2023-12-26 10:48:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 84 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 38 ms).
2023-12-26 10:48:49 INFO  IcebergFilesCommitter:306 - Committing append with 0 data files and 0 delete files to table iceberg_catalog.db1.ice_T01
2023-12-26 10:48:52 INFO  HiveTableOperations:315 - Committed to table iceberg_catalog.db1.ice_T01 with the new metadata location hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00009-aa53f853-0371-45f5-9285-dc392cedbf64.metadata.json
2023-12-26 10:48:52 INFO  BaseMetastoreTableOperations:137 - Successfully committed to table iceberg_catalog.db1.ice_T01 in 1040 ms
2023-12-26 10:48:52 INFO  SnapshotProducer:327 - Committed snapshot 23202029038553277 (MergeAppend)
2023-12-26 10:48:52 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00009-aa53f853-0371-45f5-9285-dc392cedbf64.metadata.json
2023-12-26 10:48:53 INFO  IcebergFilesCommitter:314 - Committed in 3252 ms
2023-12-26 10:48:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 85 (type=CHECKPOINT) @ 1703558939904 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:48:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 85
2023-12-26 10:48:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 85 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 94 ms).
2023-12-26 10:49:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 86 (type=CHECKPOINT) @ 1703558949901 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:49:09 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 86
2023-12-26 10:49:09 INFO  CheckpointCoordinator:1246 - Completed checkpoint 86 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 29 ms).
2023-12-26 10:49:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 87 (type=CHECKPOINT) @ 1703558959899 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:49:19 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 87
2023-12-26 10:49:19 INFO  CheckpointCoordinator:1246 - Completed checkpoint 87 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 58 ms).
2023-12-26 10:49:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 88 (type=CHECKPOINT) @ 1703558969901 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:49:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 88
2023-12-26 10:49:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 88 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 68 ms).
2023-12-26 10:49:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 89 (type=CHECKPOINT) @ 1703558979902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:49:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 89
2023-12-26 10:49:39 INFO  CheckpointCoordinator:1246 - Completed checkpoint 89 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 70 ms).
2023-12-26 10:49:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 90 (type=CHECKPOINT) @ 1703558989899 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:49:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 90
2023-12-26 10:49:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 90 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 84 ms).
2023-12-26 10:49:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 91 (type=CHECKPOINT) @ 1703558999900 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:49:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 91
2023-12-26 10:49:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 91 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 19 ms).
2023-12-26 10:50:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 92 (type=CHECKPOINT) @ 1703559009901 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:50:09 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 92
2023-12-26 10:50:09 INFO  CheckpointCoordinator:1246 - Completed checkpoint 92 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 11 ms).
2023-12-26 10:50:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 93 (type=CHECKPOINT) @ 1703559019897 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:50:19 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 93
2023-12-26 10:50:19 INFO  CheckpointCoordinator:1246 - Completed checkpoint 93 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 20 ms).
2023-12-26 10:50:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 94 (type=CHECKPOINT) @ 1703559029897 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:50:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 94
2023-12-26 10:50:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 94 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 22 ms).
2023-12-26 10:50:29 INFO  IcebergFilesCommitter:306 - Committing append with 0 data files and 0 delete files to table iceberg_catalog.db1.ice_T01
2023-12-26 10:50:35 INFO  HiveTableOperations:315 - Committed to table iceberg_catalog.db1.ice_T01 with the new metadata location hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00010-a5910ac2-2e01-4641-8e43-1316a4be3ed6.metadata.json
2023-12-26 10:50:35 INFO  BaseMetastoreTableOperations:137 - Successfully committed to table iceberg_catalog.db1.ice_T01 in 1741 ms
2023-12-26 10:50:35 INFO  SnapshotProducer:327 - Committed snapshot 8208253377295175225 (MergeAppend)
2023-12-26 10:50:35 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db1.db/ice_T01/metadata/00010-a5910ac2-2e01-4641-8e43-1316a4be3ed6.metadata.json
2023-12-26 10:50:36 INFO  IcebergFilesCommitter:314 - Committed in 6949 ms
2023-12-26 10:50:39 INFO  CheckpointCoordinator:742 - Triggering checkpoint 95 (type=CHECKPOINT) @ 1703559039902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:50:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 95
2023-12-26 10:50:39 INFO  CheckpointCoordinator:1246 - Completed checkpoint 95 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 88 ms).
2023-12-26 10:50:49 INFO  CheckpointCoordinator:742 - Triggering checkpoint 96 (type=CHECKPOINT) @ 1703559049896 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:50:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 96
2023-12-26 10:50:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 96 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 20 ms).
2023-12-26 10:50:59 INFO  CheckpointCoordinator:742 - Triggering checkpoint 97 (type=CHECKPOINT) @ 1703559059900 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:50:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 97
2023-12-26 10:50:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 97 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 29 ms).
2023-12-26 10:51:09 INFO  CheckpointCoordinator:742 - Triggering checkpoint 98 (type=CHECKPOINT) @ 1703559069901 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:51:09 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 98
2023-12-26 10:51:09 INFO  CheckpointCoordinator:1246 - Completed checkpoint 98 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 14 ms).
2023-12-26 10:51:19 INFO  CheckpointCoordinator:742 - Triggering checkpoint 99 (type=CHECKPOINT) @ 1703559079898 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:51:19 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 99
2023-12-26 10:51:19 INFO  CheckpointCoordinator:1246 - Completed checkpoint 99 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 27 ms).
2023-12-26 10:51:29 INFO  CheckpointCoordinator:742 - Triggering checkpoint 100 (type=CHECKPOINT) @ 1703559089902 for job a6871a695473e51d3520688e0f3cd838.
2023-12-26 10:51:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db1.ice_T01, checkpointId: 100
2023-12-26 10:51:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 100 for job a6871a695473e51d3520688e0f3cd838 (13265 bytes in 77 ms).
2023-12-26 10:51:33 INFO  TransientBlobCache:240 - Shutting down BLOB cache
2023-12-26 10:51:33 INFO  TaskExecutorLocalStateStoresManager:231 - Shutting down TaskExecutorLocalStateStoresManager.
2023-12-26 10:51:33 INFO  PermanentBlobCache:240 - Shutting down BLOB cache
2023-12-26 10:51:33 INFO  BlobServer:345 - Stopped BLOB server at 0.0.0.0:49673
2023-12-26 10:51:33 INFO  FileChannelManagerImpl:149 - FileChannelManager removed spill file directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-io-072961ea-3a6c-46b4-9464-72651f146964
2023-12-26 10:51:33 INFO  FileChannelManagerImpl:149 - FileChannelManager removed spill file directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-netty-shuffle-38032ac3-b887-4d71-8a1c-0f62f11093c7
2023-12-26 10:51:33 INFO  FileCache:160 - removed file cache directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-dist-cache-b34b06e4-8fce-48b0-9df7-8cdfca143376
2023-12-26 10:53:55 INFO  FlinkTable2IceTable:53 - create iceberg_catalog now!
2023-12-26 10:53:55 INFO  FlinkTable2IceTable:65 - catalog:create catalog iceberg_catalog with (
   'type'='iceberg',
   'catalog-type'='hive',
   'uri'='thrift://172.20.29.46:9083',
   'hive-conf-dir'='/Users/lifenghua/study/sourcecode/iceberg-demo/config/hive-conf-46',
   'hadoop-conf-dir'='/Users/lifenghua/study/sourcecode/iceberg-demo/config/hive-conf-46',
   'client'='1',
   'property-version'='2',
   'warehouse'='hdfs://172.20.29.46:8020/user/hive/warehouse/')

2023-12-26 10:53:55 WARN  HadoopUtils:139 - Could not find Hadoop configuration via any of the supported methods (Flink configuration, environment variables).
2023-12-26 10:53:55 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-26 10:53:55 INFO  HiveConf:176 - Found configuration file null
2023-12-26 10:53:55 WARN  HiveConf:4015 - HiveConf of name hive.vectorized.use.checked.expressions does not exist
2023-12-26 10:53:55 WARN  HiveConf:4015 - HiveConf of name hive.strict.checks.no.partition.filter does not exist
2023-12-26 10:53:55 WARN  HiveConf:4015 - HiveConf of name hive.strict.checks.orderby.no.limit does not exist
2023-12-26 10:53:55 WARN  HiveConf:4015 - HiveConf of name hive.vectorized.input.format.excludes does not exist
2023-12-26 10:53:55 WARN  HiveConf:4015 - HiveConf of name hive.strict.checks.bucketing does not exist
2023-12-26 10:53:55 INFO  metastore:434 - Trying to connect to metastore with URI thrift://172.20.29.46:9083
2023-12-26 10:53:55 INFO  metastore:479 - Opened a connection to metastore, current connections: 1
2023-12-26 10:53:56 INFO  metastore:531 - Connected to metastore.
2023-12-26 10:53:56 INFO  HiveCatalog:268 - Created namespace: db2
2023-12-26 10:53:56 INFO  FlinkTable2IceTable:79 - 执行sql:CREATE TABLE if not exists _db_.kafka_T01 (
    C1 decimal,
    C2 STRING,
    C3 timestamp,
    PRIMARY KEY (`C1`) NOT ENFORCED
 ) PARTITIONED BY (`C1`)
 WITH (
    'connector' = 'kafka',
    'topic' = 'T01',
    'properties.bootstrap.servers' = '172.20.3.227:9092',
    'properties.group.id' = 'g1',
    'scan.startup.mode' = 'latest-offset',
    'scan.topic-partition-discovery.interval'='10000',
    'value.format' = 'debezium-json'
)

2023-12-26 10:53:56 INFO  FlinkTable2IceTable:85 - create iceberg sql :CREATE TABLE if not exists iceberg_catalog.db2.ice_T01 (
    C1 decimal,
    C2 STRING,
    C3 timestamp,
    PRIMARY KEY (`C1`) NOT ENFORCED
) PARTITIONED BY (`C1`)
WITH (
    'type'='iceberg',
    'table_type'='iceberg',
    'format-version'='2',
    'engine.hive.enabled' = 'true',
    'write.upsert.enabled'='true',
    'table.exec.sink.not-null-enforcer'='true'
)

2023-12-26 10:53:56 INFO  BaseMetastoreCatalog:234 - Table properties set at catalog level through catalog properties: {}
2023-12-26 10:53:57 INFO  BaseMetastoreCatalog:246 - Table properties enforced at catalog level through catalog properties: {}
2023-12-26 10:54:02 INFO  HiveTableOperations:315 - Committed to table iceberg_catalog.db2.ice_T01 with the new metadata location hdfs://172.20.29.46:8020/user/hive/warehouse/db2.db/ice_T01/metadata/00000-8d17a90e-c792-4c2a-8938-8a40a3f14665.metadata.json
2023-12-26 10:54:02 INFO  BaseMetastoreTableOperations:137 - Successfully committed to table iceberg_catalog.db2.ice_T01 in 5206 ms
2023-12-26 10:54:02 INFO  FlinkTable2IceTable:89 - insert sql:insert into iceberg_catalog.db2.ice_T01 select * from db2.kafka_T01
2023-12-26 10:54:02 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db2.db/ice_T01/metadata/00000-8d17a90e-c792-4c2a-8938-8a40a3f14665.metadata.json
2023-12-26 10:54:05 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db2.db/ice_T01/metadata/00000-8d17a90e-c792-4c2a-8938-8a40a3f14665.metadata.json
2023-12-26 10:54:05 INFO  BaseMetastoreCatalog:64 - Table loaded by catalog: iceberg_catalog.db2.ice_T01
2023-12-26 10:54:05 INFO  FlinkSink:430 - Write distribution mode is 'none'
2023-12-26 10:54:05 INFO  FlinkSink:436 - Distribute rows by equality fields, because there are equality fields set
2023-12-26 10:54:05 INFO  TypeExtractor:1994 - class org.apache.iceberg.io.WriteResult does not contain a setter for field dataFiles
2023-12-26 10:54:05 INFO  TypeExtractor:2037 - Class class org.apache.iceberg.io.WriteResult cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2023-12-26 10:54:05 INFO  Configuration:804 - Config uses fallback configuration key 'rest.port' instead of key 'rest.bind-port'
2023-12-26 10:54:05 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
2023-12-26 10:54:05 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
2023-12-26 10:54:05 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
2023-12-26 10:54:05 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
2023-12-26 10:54:05 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
2023-12-26 10:54:05 INFO  TaskExecutorResourceUtils:281 - The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
2023-12-26 10:54:05 INFO  MiniCluster:269 - Starting Flink Mini Cluster
2023-12-26 10:54:05 INFO  MiniCluster:279 - Starting Metrics Registry
2023-12-26 10:54:05 INFO  MetricRegistryImpl:126 - No metrics reporter configured, no metrics will be exposed/reported.
2023-12-26 10:54:05 INFO  MiniCluster:283 - Starting RPC Service(s)
2023-12-26 10:54:05 INFO  AkkaRpcServiceUtils:265 - Trying to start local actor system
2023-12-26 10:54:06 INFO  Slf4jLogger:92 - Slf4jLogger started
2023-12-26 10:54:06 INFO  AkkaRpcServiceUtils:298 - Actor system started at akka://flink
2023-12-26 10:54:06 INFO  AkkaRpcServiceUtils:265 - Trying to start local actor system
2023-12-26 10:54:06 INFO  Slf4jLogger:92 - Slf4jLogger started
2023-12-26 10:54:06 INFO  AkkaRpcServiceUtils:298 - Actor system started at akka://flink-metrics
2023-12-26 10:54:06 INFO  AkkaRpcService:232 - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
2023-12-26 10:54:06 INFO  MiniCluster:487 - Starting high-availability services
2023-12-26 10:54:06 INFO  BlobServer:138 - Created BLOB server storage directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/blobStore-792a3d76-cd16-4f13-951b-9c0bfd1da218
2023-12-26 10:54:06 INFO  BlobServer:213 - Started BLOB server at 0.0.0.0:50014 - max concurrent requests: 50 - max backlog: 1000
2023-12-26 10:54:06 INFO  PermanentBlobCache:90 - Created BLOB cache storage directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/blobStore-a9d8f718-1bd9-476c-a229-5989f681b24e
2023-12-26 10:54:06 INFO  TransientBlobCache:90 - Created BLOB cache storage directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/blobStore-76737880-c208-4c17-b8aa-2d2784f0d94c
2023-12-26 10:54:06 INFO  MiniCluster:606 - Starting 1 TaskManger(s)
2023-12-26 10:54:06 INFO  TaskManagerRunner:474 - Starting TaskManager with ResourceID: 57dd203c-f8de-487e-af11-7ee78146cd25
2023-12-26 10:54:06 INFO  TaskManagerServices:441 - Temporary file directory '/var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T': total 926 GB, usable 342 GB (36.93% usable)
2023-12-26 10:54:06 INFO  FileChannelManagerImpl:98 - FileChannelManager uses directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-io-eabca365-e9d4-406a-ad30-48e46784438f for spill files.
2023-12-26 10:54:06 INFO  FileChannelManagerImpl:98 - FileChannelManager uses directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-netty-shuffle-93ac5af5-45c1-47f8-937c-332bad8e4788 for spill files.
2023-12-26 10:54:06 INFO  NetworkBufferPool:145 - Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
2023-12-26 10:54:06 INFO  NettyShuffleEnvironment:328 - Starting the network environment and its components.
2023-12-26 10:54:06 INFO  KvStateService:92 - Starting the kvState service and its components.
2023-12-26 10:54:06 INFO  AkkaRpcService:232 - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
2023-12-26 10:54:06 INFO  DefaultJobLeaderService:123 - Start job leader service.
2023-12-26 10:54:06 INFO  FileCache:116 - User file cache uses directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-dist-cache-b03912f9-b894-4799-af8a-6a7c13dccaf4
2023-12-26 10:54:06 INFO  Configuration:804 - Config uses fallback configuration key 'rest.port' instead of key 'rest.bind-port'
2023-12-26 10:54:06 INFO  DispatcherRestEndpoint:139 - Starting rest endpoint.
2023-12-26 10:54:06 WARN  WebMonitorUtils:82 - Log file environment variable 'log.file' is not set.
2023-12-26 10:54:06 INFO  WebMonitorUtils:103 - Determined location of main cluster component log file: /Users/lifenghua/study/sourcecode/iceberg-demo/logs/file.log
2023-12-26 10:54:06 INFO  WebMonitorUtils:104 - Determined location of main cluster component stdout file: /Users/lifenghua/study/sourcecode/iceberg-demo/logs/file.out
2023-12-26 10:54:06 INFO  DispatcherRestEndpoint:250 - Rest endpoint listening at localhost:18081
2023-12-26 10:54:06 INFO  EmbeddedLeaderService:308 - Proposing leadership to contender http://localhost:18081
2023-12-26 10:54:06 INFO  DispatcherRestEndpoint:936 - Web frontend listening at http://localhost:18081.
2023-12-26 10:54:06 INFO  DispatcherRestEndpoint:994 - http://localhost:18081 was granted leadership with leaderSessionID=634e5231-a175-403d-8deb-35f0b6ceccc9
2023-12-26 10:54:06 INFO  EmbeddedLeaderService:256 - Received confirmation of leadership for leader http://localhost:18081 , session=634e5231-a175-403d-8deb-35f0b6ceccc9
2023-12-26 10:54:06 INFO  AkkaRpcService:232 - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
2023-12-26 10:54:07 INFO  EmbeddedLeaderService:308 - Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
2023-12-26 10:54:07 INFO  StandaloneResourceManager:234 - Starting the resource manager.
2023-12-26 10:54:07 INFO  EmbeddedLeaderService:308 - Proposing leadership to contender LeaderContender: StandaloneResourceManager
2023-12-26 10:54:07 INFO  DefaultDispatcherRunner:107 - DefaultDispatcherRunner was granted leadership with leader id 53c24036-6ee9-4131-a1a3-15ae5b514731. Creating new DispatcherLeaderProcess.
2023-12-26 10:54:07 INFO  StandaloneResourceManager:1230 - ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token 816e863087730a76ee9be9ae0706441f
2023-12-26 10:54:07 INFO  MiniCluster:413 - Flink Mini Cluster started successfully
2023-12-26 10:54:07 INFO  SessionDispatcherLeaderProcess:97 - Start SessionDispatcherLeaderProcess.
2023-12-26 10:54:07 INFO  SessionDispatcherLeaderProcess:117 - Recover all persisted job graphs.
2023-12-26 10:54:07 INFO  SessionDispatcherLeaderProcess:125 - Successfully recovered 0 persisted job graphs.
2023-12-26 10:54:07 INFO  EmbeddedLeaderService:256 - Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=ee9be9ae-0706-441f-816e-863087730a76
2023-12-26 10:54:07 INFO  TaskExecutor:1293 - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(816e863087730a76ee9be9ae0706441f).
2023-12-26 10:54:07 INFO  AkkaRpcService:232 - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
2023-12-26 10:54:07 INFO  EmbeddedLeaderService:256 - Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=53c24036-6ee9-4131-a1a3-15ae5b514731
2023-12-26 10:54:07 INFO  TaskExecutor:162 - Resolved ResourceManager address, beginning registration
2023-12-26 10:54:07 INFO  StandaloneResourceManager:982 - Registering TaskManager with ResourceID 57dd203c-f8de-487e-af11-7ee78146cd25 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
2023-12-26 10:54:07 INFO  StandaloneDispatcher:300 - Received JobGraph submission 0b5e7a236a5e960961b6cd7adf486ca7 (insert-into_iceberg_catalog.db2.ice_T01).
2023-12-26 10:54:07 INFO  StandaloneDispatcher:362 - Submitting job 0b5e7a236a5e960961b6cd7adf486ca7 (insert-into_iceberg_catalog.db2.ice_T01).
2023-12-26 10:54:07 INFO  TaskExecutor:99 - Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id ffbcbaf126b374fa97dd789502184fe9.
2023-12-26 10:54:07 INFO  EmbeddedLeaderService:308 - Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
2023-12-26 10:54:07 INFO  AkkaRpcService:232 - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
2023-12-26 10:54:07 INFO  JobMaster:289 - Initializing job insert-into_iceberg_catalog.db2.ice_T01 (0b5e7a236a5e960961b6cd7adf486ca7).
2023-12-26 10:54:07 INFO  JobMaster:98 - Using restart back off time strategy FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2147483647, backoffTimeMS=1000) for insert-into_iceberg_catalog.db2.ice_T01 (0b5e7a236a5e960961b6cd7adf486ca7).
2023-12-26 10:54:07 INFO  JobMaster:159 - Running initialization on master for job insert-into_iceberg_catalog.db2.ice_T01 (0b5e7a236a5e960961b6cd7adf486ca7).
2023-12-26 10:54:07 INFO  JobMaster:183 - Successfully ran initialization on master in 0 ms.
2023-12-26 10:54:07 INFO  DefaultExecutionTopology:271 - Built 1 pipelined regions in 1 ms
2023-12-26 10:54:07 INFO  JobMaster:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5d0bd45b
2023-12-26 10:54:07 INFO  JobMaster:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  CheckpointCoordinator:1532 - No checkpoint found during restore.
2023-12-26 10:54:07 INFO  JobMaster:145 - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@5aaf5b5a for insert-into_iceberg_catalog.db2.ice_T01 (0b5e7a236a5e960961b6cd7adf486ca7).
2023-12-26 10:54:07 INFO  EmbeddedLeaderService:256 - Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=740f1f4d-2fca-4de6-b7bb-238106b3d8c0
2023-12-26 10:54:07 INFO  JobMaster:867 - Starting execution of job insert-into_iceberg_catalog.db2.ice_T01 (0b5e7a236a5e960961b6cd7adf486ca7) under job master id b7bb238106b3d8c0740f1f4d2fca4de6.
2023-12-26 10:54:07 INFO  JobMaster:183 - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
2023-12-26 10:54:07 INFO  ExecutionGraph:1039 - Job insert-into_iceberg_catalog.db2.ice_T01 (0b5e7a236a5e960961b6cd7adf486ca7) switched from state CREATED to RUNNING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) (c787e45a7df996428f72d21b80449ff8) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12) (d94290d208ffe806e61673392406bd4d) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12) (ce08e7548623a7fb8da6f2a2ddda5389) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12) (8237e3d8fe58271b5762d0c5e572d446) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12) (0d25ee151b606a4de3c337fe8313cc52) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12) (2b548e32b542d30b756e2273261776fa) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12) (78bc38c35abc2e44dbc4891bc034c18b) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12) (ea7465f29e9a4d822d3bf023c7238315) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12) (6c6967fce7c48e4f8cd3e474f2e5ec72) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12) (9cf4b4e02d3af3d541ed3e903dea7486) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12) (1c439751c0bab6c437d23e9db5a9c4c7) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12) (c3c37af38c416f78179bd5c3f29869ff) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (1/12) (c98a47c6ce963e5abd11b0bb3d7c4d50) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (2/12) (782480ddf3929685bae7d79f6ab23978) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (3/12) (d39a1a55045a8ead0595f705b9eff056) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (4/12) (8c163a80e44edc3c0b31a653c6d394ad) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (5/12) (bcad5a680f80dcbd6b763f4c04596c26) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (6/12) (9c8079736913517df2ba1ae9d75d3d9b) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (7/12) (e13358615178b16f463a47712adccf92) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (8/12) (b696ef7f502a355005230bbf10054b07) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (9/12) (efc894151881020ac14ccbe76c6d866e) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (10/12) (35198123380d155db20a2300ee7b7cf7) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (11/12) (2719b74cd45bd8bc0bdd463b0b137613) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (12/12) (a55c681330da7e7476b80533fc93a136) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db2.ice_T01 (1/1) (fe3b6044519e8d13c2cafb88f20729de) switched from CREATED to SCHEDULED.
2023-12-26 10:54:07 INFO  JobMaster:1040 - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(816e863087730a76ee9be9ae0706441f)
2023-12-26 10:54:07 INFO  JobMaster:162 - Resolved ResourceManager address, beginning registration
2023-12-26 10:54:07 INFO  StandaloneResourceManager:355 - Registering job manager b7bb238106b3d8c0740f1f4d2fca4de6@akka://flink/user/rpc/jobmanager_3 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:54:07 INFO  StandaloneResourceManager:909 - Registered job manager b7bb238106b3d8c0740f1f4d2fca4de6@akka://flink/user/rpc/jobmanager_3 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:54:07 INFO  JobMaster:1064 - JobManager successfully registered at ResourceManager, leader id: 816e863087730a76ee9be9ae0706441f.
2023-12-26 10:54:07 INFO  DeclarativeSlotManager:263 - Received resource requirements from job 0b5e7a236a5e960961b6cd7adf486ca7: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=12}]
2023-12-26 10:54:07 INFO  TaskExecutor:1027 - Receive slot request be8177243c0e6c638202866768564939 for job 0b5e7a236a5e960961b6cd7adf486ca7 from resource manager with leader id 816e863087730a76ee9be9ae0706441f.
2023-12-26 10:54:07 INFO  TaskExecutor:1104 - Allocated slot for be8177243c0e6c638202866768564939.
2023-12-26 10:54:07 INFO  DefaultJobLeaderService:188 - Add job 0b5e7a236a5e960961b6cd7adf486ca7 for job leader monitoring.
2023-12-26 10:54:07 INFO  DefaultJobLeaderService:346 - Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 740f1f4d-2fca-4de6-b7bb-238106b3d8c0.
2023-12-26 10:54:07 INFO  TaskExecutor:1027 - Receive slot request 7c307e9cf7feb0210496961f9a5c3fed for job 0b5e7a236a5e960961b6cd7adf486ca7 from resource manager with leader id 816e863087730a76ee9be9ae0706441f.
2023-12-26 10:54:07 INFO  TaskExecutor:1104 - Allocated slot for 7c307e9cf7feb0210496961f9a5c3fed.
2023-12-26 10:54:07 INFO  TaskExecutor:1027 - Receive slot request c75ddc254fbca6fbcb593d5e5e5e11fc for job 0b5e7a236a5e960961b6cd7adf486ca7 from resource manager with leader id 816e863087730a76ee9be9ae0706441f.
2023-12-26 10:54:07 INFO  TaskExecutor:1104 - Allocated slot for c75ddc254fbca6fbcb593d5e5e5e11fc.
2023-12-26 10:54:07 INFO  TaskExecutor:1027 - Receive slot request 0f0e089fd7bb3408302c80f710e75ae8 for job 0b5e7a236a5e960961b6cd7adf486ca7 from resource manager with leader id 816e863087730a76ee9be9ae0706441f.
2023-12-26 10:54:07 INFO  TaskExecutor:1104 - Allocated slot for 0f0e089fd7bb3408302c80f710e75ae8.
2023-12-26 10:54:07 INFO  DefaultJobLeaderService:162 - Resolved JobManager address, beginning registration
2023-12-26 10:54:07 INFO  TaskExecutor:1027 - Receive slot request ae64da0b1659c0c7e12a7cc9d11ed4a5 for job 0b5e7a236a5e960961b6cd7adf486ca7 from resource manager with leader id 816e863087730a76ee9be9ae0706441f.
2023-12-26 10:54:07 INFO  TaskExecutor:1104 - Allocated slot for ae64da0b1659c0c7e12a7cc9d11ed4a5.
2023-12-26 10:54:07 INFO  TaskExecutor:1027 - Receive slot request e0e5f5abc9df9542f70a431363cee65b for job 0b5e7a236a5e960961b6cd7adf486ca7 from resource manager with leader id 816e863087730a76ee9be9ae0706441f.
2023-12-26 10:54:07 INFO  TaskExecutor:1104 - Allocated slot for e0e5f5abc9df9542f70a431363cee65b.
2023-12-26 10:54:07 INFO  TaskExecutor:1027 - Receive slot request b20db18ab6120e704f22dcff19a1db6a for job 0b5e7a236a5e960961b6cd7adf486ca7 from resource manager with leader id 816e863087730a76ee9be9ae0706441f.
2023-12-26 10:54:07 INFO  TaskExecutor:1104 - Allocated slot for b20db18ab6120e704f22dcff19a1db6a.
2023-12-26 10:54:07 INFO  TaskExecutor:1027 - Receive slot request 4e12191fde8d56db88454081c23c06ae for job 0b5e7a236a5e960961b6cd7adf486ca7 from resource manager with leader id 816e863087730a76ee9be9ae0706441f.
2023-12-26 10:54:07 INFO  TaskExecutor:1104 - Allocated slot for 4e12191fde8d56db88454081c23c06ae.
2023-12-26 10:54:07 INFO  TaskExecutor:1027 - Receive slot request 7a63094d4b5d85827bdf4f682fbe5603 for job 0b5e7a236a5e960961b6cd7adf486ca7 from resource manager with leader id 816e863087730a76ee9be9ae0706441f.
2023-12-26 10:54:07 INFO  TaskExecutor:1104 - Allocated slot for 7a63094d4b5d85827bdf4f682fbe5603.
2023-12-26 10:54:07 INFO  TaskExecutor:1027 - Receive slot request 5a7b978d34d04700696733aa1de01a0b for job 0b5e7a236a5e960961b6cd7adf486ca7 from resource manager with leader id 816e863087730a76ee9be9ae0706441f.
2023-12-26 10:54:07 INFO  TaskExecutor:1104 - Allocated slot for 5a7b978d34d04700696733aa1de01a0b.
2023-12-26 10:54:07 INFO  TaskExecutor:1027 - Receive slot request 2006386d885574ec9a2fad018d77062e for job 0b5e7a236a5e960961b6cd7adf486ca7 from resource manager with leader id 816e863087730a76ee9be9ae0706441f.
2023-12-26 10:54:07 INFO  TaskExecutor:1104 - Allocated slot for 2006386d885574ec9a2fad018d77062e.
2023-12-26 10:54:07 INFO  TaskExecutor:1027 - Receive slot request af78593b94c73f832cbb23c12d30ace2 for job 0b5e7a236a5e960961b6cd7adf486ca7 from resource manager with leader id 816e863087730a76ee9be9ae0706441f.
2023-12-26 10:54:07 INFO  TaskExecutor:1104 - Allocated slot for af78593b94c73f832cbb23c12d30ace2.
2023-12-26 10:54:07 INFO  DefaultJobLeaderService:413 - Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:54:07 INFO  TaskExecutor:1608 - Establish JobManager connection for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:54:07 INFO  TaskExecutor:1459 - Offer reserved slots to the leader of job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) (c787e45a7df996428f72d21b80449ff8) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) (attempt #0) with attempt id c787e45a7df996428f72d21b80449ff8 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id 7c307e9cf7feb0210496961f9a5c3fed
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12) (d94290d208ffe806e61673392406bd4d) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot 7c307e9cf7feb0210496961f9a5c3fed.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12) (attempt #0) with attempt id d94290d208ffe806e61673392406bd4d to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id 4e12191fde8d56db88454081c23c06ae
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12) (ce08e7548623a7fb8da6f2a2ddda5389) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12) (attempt #0) with attempt id ce08e7548623a7fb8da6f2a2ddda5389 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id 0f0e089fd7bb3408302c80f710e75ae8
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12) (8237e3d8fe58271b5762d0c5e572d446) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12) (attempt #0) with attempt id 8237e3d8fe58271b5762d0c5e572d446 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id af78593b94c73f832cbb23c12d30ace2
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12) (0d25ee151b606a4de3c337fe8313cc52) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12) (attempt #0) with attempt id 0d25ee151b606a4de3c337fe8313cc52 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id be8177243c0e6c638202866768564939
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12) (2b548e32b542d30b756e2273261776fa) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12) (attempt #0) with attempt id 2b548e32b542d30b756e2273261776fa to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id 2006386d885574ec9a2fad018d77062e
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12) (78bc38c35abc2e44dbc4891bc034c18b) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12) (attempt #0) with attempt id 78bc38c35abc2e44dbc4891bc034c18b to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id ae64da0b1659c0c7e12a7cc9d11ed4a5
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12) (ea7465f29e9a4d822d3bf023c7238315) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12) (attempt #0) with attempt id ea7465f29e9a4d822d3bf023c7238315 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id c75ddc254fbca6fbcb593d5e5e5e11fc
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12) (6c6967fce7c48e4f8cd3e474f2e5ec72) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12) (attempt #0) with attempt id 6c6967fce7c48e4f8cd3e474f2e5ec72 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id 7a63094d4b5d85827bdf4f682fbe5603
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12) (9cf4b4e02d3af3d541ed3e903dea7486) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12) (attempt #0) with attempt id 9cf4b4e02d3af3d541ed3e903dea7486 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id 5a7b978d34d04700696733aa1de01a0b
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12) (1c439751c0bab6c437d23e9db5a9c4c7) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12) (attempt #0) with attempt id 1c439751c0bab6c437d23e9db5a9c4c7 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id b20db18ab6120e704f22dcff19a1db6a
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12) (c3c37af38c416f78179bd5c3f29869ff) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12) (attempt #0) with attempt id c3c37af38c416f78179bd5c3f29869ff to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id e0e5f5abc9df9542f70a431363cee65b
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (1/12) (c98a47c6ce963e5abd11b0bb3d7c4d50) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (1/12) (attempt #0) with attempt id c98a47c6ce963e5abd11b0bb3d7c4d50 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id 7c307e9cf7feb0210496961f9a5c3fed
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (2/12) (782480ddf3929685bae7d79f6ab23978) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (2/12) (attempt #0) with attempt id 782480ddf3929685bae7d79f6ab23978 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id 4e12191fde8d56db88454081c23c06ae
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (3/12) (d39a1a55045a8ead0595f705b9eff056) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (3/12) (attempt #0) with attempt id d39a1a55045a8ead0595f705b9eff056 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id 0f0e089fd7bb3408302c80f710e75ae8
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (4/12) (8c163a80e44edc3c0b31a653c6d394ad) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (4/12) (attempt #0) with attempt id 8c163a80e44edc3c0b31a653c6d394ad to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id af78593b94c73f832cbb23c12d30ace2
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (5/12) (bcad5a680f80dcbd6b763f4c04596c26) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (5/12) (attempt #0) with attempt id bcad5a680f80dcbd6b763f4c04596c26 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id be8177243c0e6c638202866768564939
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (6/12) (9c8079736913517df2ba1ae9d75d3d9b) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (6/12) (attempt #0) with attempt id 9c8079736913517df2ba1ae9d75d3d9b to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id 2006386d885574ec9a2fad018d77062e
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (7/12) (e13358615178b16f463a47712adccf92) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (7/12) (attempt #0) with attempt id e13358615178b16f463a47712adccf92 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id ae64da0b1659c0c7e12a7cc9d11ed4a5
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (8/12) (b696ef7f502a355005230bbf10054b07) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (8/12) (attempt #0) with attempt id b696ef7f502a355005230bbf10054b07 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id c75ddc254fbca6fbcb593d5e5e5e11fc
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (9/12) (efc894151881020ac14ccbe76c6d866e) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (9/12) (attempt #0) with attempt id efc894151881020ac14ccbe76c6d866e to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id 7a63094d4b5d85827bdf4f682fbe5603
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (10/12) (35198123380d155db20a2300ee7b7cf7) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (10/12) (attempt #0) with attempt id 35198123380d155db20a2300ee7b7cf7 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id 5a7b978d34d04700696733aa1de01a0b
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (11/12) (2719b74cd45bd8bc0bdd463b0b137613) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (11/12) (attempt #0) with attempt id 2719b74cd45bd8bc0bdd463b0b137613 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id b20db18ab6120e704f22dcff19a1db6a
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (12/12) (a55c681330da7e7476b80533fc93a136) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying IcebergStreamWriter (12/12) (attempt #0) with attempt id a55c681330da7e7476b80533fc93a136 to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id e0e5f5abc9df9542f70a431363cee65b
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db2.ice_T01 (1/1) (fe3b6044519e8d13c2cafb88f20729de) switched from SCHEDULED to DEPLOYING.
2023-12-26 10:54:07 INFO  ExecutionGraph:571 - Deploying IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db2.ice_T01 (1/1) (attempt #0) with attempt id fe3b6044519e8d13c2cafb88f20729de to 57dd203c-f8de-487e-af11-7ee78146cd25 @ localhost (dataPort=-1) with allocation id 7c307e9cf7feb0210496961f9a5c3fed
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12)#0 (c787e45a7df996428f72d21b80449ff8), deploy into slot with allocation id 7c307e9cf7feb0210496961f9a5c3fed.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12)#0 (c787e45a7df996428f72d21b80449ff8) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot 4e12191fde8d56db88454081c23c06ae.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12)#0 (c787e45a7df996428f72d21b80449ff8) [DEPLOYING].
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12)#0 (d94290d208ffe806e61673392406bd4d), deploy into slot with allocation id 4e12191fde8d56db88454081c23c06ae.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12)#0 (d94290d208ffe806e61673392406bd4d) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot 0f0e089fd7bb3408302c80f710e75ae8.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12)#0 (d94290d208ffe806e61673392406bd4d) [DEPLOYING].
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12)#0 (ce08e7548623a7fb8da6f2a2ddda5389), deploy into slot with allocation id 0f0e089fd7bb3408302c80f710e75ae8.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12)#0 (ce08e7548623a7fb8da6f2a2ddda5389) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot af78593b94c73f832cbb23c12d30ace2.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12)#0 (ce08e7548623a7fb8da6f2a2ddda5389) [DEPLOYING].
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12)#0 (8237e3d8fe58271b5762d0c5e572d446), deploy into slot with allocation id af78593b94c73f832cbb23c12d30ace2.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12)#0 (8237e3d8fe58271b5762d0c5e572d446) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12)#0 (8237e3d8fe58271b5762d0c5e572d446) [DEPLOYING].
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot be8177243c0e6c638202866768564939.
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12)#0 (0d25ee151b606a4de3c337fe8313cc52), deploy into slot with allocation id be8177243c0e6c638202866768564939.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12)#0 (0d25ee151b606a4de3c337fe8313cc52) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12)#0 (0d25ee151b606a4de3c337fe8313cc52) [DEPLOYING].
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot 2006386d885574ec9a2fad018d77062e.
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12)#0 (2b548e32b542d30b756e2273261776fa), deploy into slot with allocation id 2006386d885574ec9a2fad018d77062e.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12)#0 (2b548e32b542d30b756e2273261776fa) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12)#0 (2b548e32b542d30b756e2273261776fa) [DEPLOYING].
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot ae64da0b1659c0c7e12a7cc9d11ed4a5.
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12)#0 (78bc38c35abc2e44dbc4891bc034c18b), deploy into slot with allocation id ae64da0b1659c0c7e12a7cc9d11ed4a5.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12)#0 (78bc38c35abc2e44dbc4891bc034c18b) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12)#0 (78bc38c35abc2e44dbc4891bc034c18b) [DEPLOYING].
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot c75ddc254fbca6fbcb593d5e5e5e11fc.
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12)#0 (ea7465f29e9a4d822d3bf023c7238315), deploy into slot with allocation id c75ddc254fbca6fbcb593d5e5e5e11fc.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12)#0 (ea7465f29e9a4d822d3bf023c7238315) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12)#0 (ea7465f29e9a4d822d3bf023c7238315) [DEPLOYING].
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot 7a63094d4b5d85827bdf4f682fbe5603.
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12)#0 (6c6967fce7c48e4f8cd3e474f2e5ec72), deploy into slot with allocation id 7a63094d4b5d85827bdf4f682fbe5603.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12)#0 (6c6967fce7c48e4f8cd3e474f2e5ec72) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12)#0 (6c6967fce7c48e4f8cd3e474f2e5ec72) [DEPLOYING].
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot 5a7b978d34d04700696733aa1de01a0b.
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12)#0 (9cf4b4e02d3af3d541ed3e903dea7486), deploy into slot with allocation id 5a7b978d34d04700696733aa1de01a0b.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12)#0 (9cf4b4e02d3af3d541ed3e903dea7486) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12)#0 (9cf4b4e02d3af3d541ed3e903dea7486) [DEPLOYING].
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot b20db18ab6120e704f22dcff19a1db6a.
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12)#0 (1c439751c0bab6c437d23e9db5a9c4c7), deploy into slot with allocation id b20db18ab6120e704f22dcff19a1db6a.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12)#0 (1c439751c0bab6c437d23e9db5a9c4c7) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot e0e5f5abc9df9542f70a431363cee65b.
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6e9cce09
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4213c2cb
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@653ef2c4
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@429a6f79
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@35a480aa
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@c04033e
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@620aa4e2
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12)#0 (c3c37af38c416f78179bd5c3f29869ff), deploy into slot with allocation id e0e5f5abc9df9542f70a431363cee65b.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12)#0 (c3c37af38c416f78179bd5c3f29869ff) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12)#0 (c3c37af38c416f78179bd5c3f29869ff) [DEPLOYING].
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12)#0 (1c439751c0bab6c437d23e9db5a9c4c7) [DEPLOYING].
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot 7c307e9cf7feb0210496961f9a5c3fed.
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5989d2ed
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12)#0 (c787e45a7df996428f72d21b80449ff8) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12)#0 (ce08e7548623a7fb8da6f2a2ddda5389) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12)#0 (9cf4b4e02d3af3d541ed3e903dea7486) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12)#0 (78bc38c35abc2e44dbc4891bc034c18b) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12)#0 (ea7465f29e9a4d822d3bf023c7238315) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12)#0 (2b548e32b542d30b756e2273261776fa) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12)#0 (8237e3d8fe58271b5762d0c5e572d446) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12)#0 (d94290d208ffe806e61673392406bd4d) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6207030c
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@246b9333
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12)#0 (0d25ee151b606a4de3c337fe8313cc52) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12) (9cf4b4e02d3af3d541ed3e903dea7486) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12)#0 (6c6967fce7c48e4f8cd3e474f2e5ec72) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4ebe8ef2
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12)#0 (1c439751c0bab6c437d23e9db5a9c4c7) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@615934c6
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12)#0 (c3c37af38c416f78179bd5c3f29869ff) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12) (8237e3d8fe58271b5762d0c5e572d446) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12) (78bc38c35abc2e44dbc4891bc034c18b) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12) (d94290d208ffe806e61673392406bd4d) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) (c787e45a7df996428f72d21b80449ff8) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12) (2b548e32b542d30b756e2273261776fa) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12) (ea7465f29e9a4d822d3bf023c7238315) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12) (ce08e7548623a7fb8da6f2a2ddda5389) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12) (0d25ee151b606a4de3c337fe8313cc52) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12) (6c6967fce7c48e4f8cd3e474f2e5ec72) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12) (1c439751c0bab6c437d23e9db5a9c4c7) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12) (c3c37af38c416f78179bd5c3f29869ff) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (1/12)#0 (c98a47c6ce963e5abd11b0bb3d7c4d50), deploy into slot with allocation id 7c307e9cf7feb0210496961f9a5c3fed.
2023-12-26 10:54:07 INFO  Task:1067 - IcebergStreamWriter (1/12)#0 (c98a47c6ce963e5abd11b0bb3d7c4d50) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (1/12)#0 (c98a47c6ce963e5abd11b0bb3d7c4d50) [DEPLOYING].
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot 4e12191fde8d56db88454081c23c06ae.
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@66b71793
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  Task:1067 - IcebergStreamWriter (1/12)#0 (c98a47c6ce963e5abd11b0bb3d7c4d50) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (1/12) (c98a47c6ce963e5abd11b0bb3d7c4d50) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (2/12)#0 (782480ddf3929685bae7d79f6ab23978), deploy into slot with allocation id 4e12191fde8d56db88454081c23c06ae.
2023-12-26 10:54:07 INFO  Task:1067 - IcebergStreamWriter (2/12)#0 (782480ddf3929685bae7d79f6ab23978) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot 0f0e089fd7bb3408302c80f710e75ae8.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (2/12)#0 (782480ddf3929685bae7d79f6ab23978) [DEPLOYING].
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (3/12)#0 (d39a1a55045a8ead0595f705b9eff056), deploy into slot with allocation id 0f0e089fd7bb3408302c80f710e75ae8.
2023-12-26 10:54:07 INFO  Task:1067 - IcebergStreamWriter (3/12)#0 (d39a1a55045a8ead0595f705b9eff056) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (3/12)#0 (d39a1a55045a8ead0595f705b9eff056) [DEPLOYING].
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1b0c1305
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  Task:1067 - IcebergStreamWriter (2/12)#0 (782480ddf3929685bae7d79f6ab23978) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6555924b
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot af78593b94c73f832cbb23c12d30ace2.
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  Task:1067 - IcebergStreamWriter (3/12)#0 (d39a1a55045a8ead0595f705b9eff056) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (2/12) (782480ddf3929685bae7d79f6ab23978) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (3/12) (d39a1a55045a8ead0595f705b9eff056) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (4/12)#0 (8c163a80e44edc3c0b31a653c6d394ad), deploy into slot with allocation id af78593b94c73f832cbb23c12d30ace2.
2023-12-26 10:54:07 INFO  Task:1067 - IcebergStreamWriter (4/12)#0 (8c163a80e44edc3c0b31a653c6d394ad) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot be8177243c0e6c638202866768564939.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (4/12)#0 (8c163a80e44edc3c0b31a653c6d394ad) [DEPLOYING].
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (5/12)#0 (bcad5a680f80dcbd6b763f4c04596c26), deploy into slot with allocation id be8177243c0e6c638202866768564939.
2023-12-26 10:54:07 INFO  Task:1067 - IcebergStreamWriter (5/12)#0 (bcad5a680f80dcbd6b763f4c04596c26) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot 2006386d885574ec9a2fad018d77062e.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (5/12)#0 (bcad5a680f80dcbd6b763f4c04596c26) [DEPLOYING].
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4481673e
2023-12-26 10:54:07 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (6/12)#0 (9c8079736913517df2ba1ae9d75d3d9b), deploy into slot with allocation id 2006386d885574ec9a2fad018d77062e.
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  Task:1067 - IcebergStreamWriter (6/12)#0 (9c8079736913517df2ba1ae9d75d3d9b) switched from CREATED to DEPLOYING.
2023-12-26 10:54:07 INFO  Task:1067 - IcebergStreamWriter (4/12)#0 (8c163a80e44edc3c0b31a653c6d394ad) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (6/12)#0 (9c8079736913517df2ba1ae9d75d3d9b) [DEPLOYING].
2023-12-26 10:54:07 INFO  TaskSlotTableImpl:388 - Activate slot ae64da0b1659c0c7e12a7cc9d11ed4a5.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (4/12) (8c163a80e44edc3c0b31a653c6d394ad) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@27f43ad6
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  Task:1067 - IcebergStreamWriter (5/12)#0 (bcad5a680f80dcbd6b763f4c04596c26) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (5/12) (bcad5a680f80dcbd6b763f4c04596c26) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3b6058d5
2023-12-26 10:54:07 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:07 INFO  Task:1067 - IcebergStreamWriter (6/12)#0 (9c8079736913517df2ba1ae9d75d3d9b) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:07 INFO  ExecutionGraph:1438 - IcebergStreamWriter (6/12) (9c8079736913517df2ba1ae9d75d3d9b) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:08 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (7/12)#0 (e13358615178b16f463a47712adccf92), deploy into slot with allocation id ae64da0b1659c0c7e12a7cc9d11ed4a5.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (7/12)#0 (e13358615178b16f463a47712adccf92) switched from CREATED to DEPLOYING.
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot c75ddc254fbca6fbcb593d5e5e5e11fc.
2023-12-26 10:54:08 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (7/12)#0 (e13358615178b16f463a47712adccf92) [DEPLOYING].
2023-12-26 10:54:08 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (8/12)#0 (b696ef7f502a355005230bbf10054b07), deploy into slot with allocation id c75ddc254fbca6fbcb593d5e5e5e11fc.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (8/12)#0 (b696ef7f502a355005230bbf10054b07) switched from CREATED to DEPLOYING.
2023-12-26 10:54:08 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (8/12)#0 (b696ef7f502a355005230bbf10054b07) [DEPLOYING].
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot 7a63094d4b5d85827bdf4f682fbe5603.
2023-12-26 10:54:08 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6b6b66b0
2023-12-26 10:54:08 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (7/12)#0 (e13358615178b16f463a47712adccf92) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (7/12) (e13358615178b16f463a47712adccf92) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:08 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (9/12)#0 (efc894151881020ac14ccbe76c6d866e), deploy into slot with allocation id 7a63094d4b5d85827bdf4f682fbe5603.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (9/12)#0 (efc894151881020ac14ccbe76c6d866e) switched from CREATED to DEPLOYING.
2023-12-26 10:54:08 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (9/12)#0 (efc894151881020ac14ccbe76c6d866e) [DEPLOYING].
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot 5a7b978d34d04700696733aa1de01a0b.
2023-12-26 10:54:08 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (10/12)#0 (35198123380d155db20a2300ee7b7cf7), deploy into slot with allocation id 5a7b978d34d04700696733aa1de01a0b.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (10/12)#0 (35198123380d155db20a2300ee7b7cf7) switched from CREATED to DEPLOYING.
2023-12-26 10:54:08 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (10/12)#0 (35198123380d155db20a2300ee7b7cf7) [DEPLOYING].
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot b20db18ab6120e704f22dcff19a1db6a.
2023-12-26 10:54:08 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (11/12)#0 (2719b74cd45bd8bc0bdd463b0b137613), deploy into slot with allocation id b20db18ab6120e704f22dcff19a1db6a.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (11/12)#0 (2719b74cd45bd8bc0bdd463b0b137613) switched from CREATED to DEPLOYING.
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot e0e5f5abc9df9542f70a431363cee65b.
2023-12-26 10:54:08 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (11/12)#0 (2719b74cd45bd8bc0bdd463b0b137613) [DEPLOYING].
2023-12-26 10:54:08 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1a48c0f6
2023-12-26 10:54:08 INFO  TaskExecutor:722 - Received task IcebergStreamWriter (12/12)#0 (a55c681330da7e7476b80533fc93a136), deploy into slot with allocation id e0e5f5abc9df9542f70a431363cee65b.
2023-12-26 10:54:08 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:54:08 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:54:08 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6264be
2023-12-26 10:54:08 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot 7c307e9cf7feb0210496961f9a5c3fed.
2023-12-26 10:54:08 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7a3d0850
2023-12-26 10:54:08 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (12/12)#0 (a55c681330da7e7476b80533fc93a136) switched from CREATED to DEPLOYING.
2023-12-26 10:54:08 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:08 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@45bce390
2023-12-26 10:54:08 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (9/12)#0 (efc894151881020ac14ccbe76c6d866e) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:08 INFO  Task:626 - Loading JAR files for task IcebergStreamWriter (12/12)#0 (a55c681330da7e7476b80533fc93a136) [DEPLOYING].
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (11/12)#0 (2719b74cd45bd8bc0bdd463b0b137613) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:08 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (10/12)#0 (35198123380d155db20a2300ee7b7cf7) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (8/12)#0 (b696ef7f502a355005230bbf10054b07) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (10/12) (35198123380d155db20a2300ee7b7cf7) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (11/12) (2719b74cd45bd8bc0bdd463b0b137613) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (8/12) (b696ef7f502a355005230bbf10054b07) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (9/12) (efc894151881020ac14ccbe76c6d866e) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:08 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:54:08 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:54:08 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:54:08 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:54:08 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:54:08 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:54:08 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:54:08 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:54:08 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2f4a67a5
2023-12-26 10:54:08 INFO  TaskExecutor:722 - Received task IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db2.ice_T01 (1/1)#0 (fe3b6044519e8d13c2cafb88f20729de), deploy into slot with allocation id 7c307e9cf7feb0210496961f9a5c3fed.
2023-12-26 10:54:08 WARN  MetricGroup:154 - The operator name Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) exceeded the 80 characters length limit and was truncated.
2023-12-26 10:54:08 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:08 INFO  Task:1067 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db2.ice_T01 (1/1)#0 (fe3b6044519e8d13c2cafb88f20729de) switched from CREATED to DEPLOYING.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (12/12)#0 (a55c681330da7e7476b80533fc93a136) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (12/12) (a55c681330da7e7476b80533fc93a136) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot 5a7b978d34d04700696733aa1de01a0b.
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot be8177243c0e6c638202866768564939.
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot b20db18ab6120e704f22dcff19a1db6a.
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot 2006386d885574ec9a2fad018d77062e.
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot 0f0e089fd7bb3408302c80f710e75ae8.
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot ae64da0b1659c0c7e12a7cc9d11ed4a5.
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot c75ddc254fbca6fbcb593d5e5e5e11fc.
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot 7c307e9cf7feb0210496961f9a5c3fed.
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot 7a63094d4b5d85827bdf4f682fbe5603.
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot 4e12191fde8d56db88454081c23c06ae.
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot e0e5f5abc9df9542f70a431363cee65b.
2023-12-26 10:54:08 INFO  TaskSlotTableImpl:388 - Activate slot af78593b94c73f832cbb23c12d30ace2.
2023-12-26 10:54:08 INFO  Task:626 - Loading JAR files for task IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db2.ice_T01 (1/1)#0 (fe3b6044519e8d13c2cafb88f20729de) [DEPLOYING].
2023-12-26 10:54:08 INFO  StreamTask:300 - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@28d8af1a
2023-12-26 10:54:08 INFO  StreamTask:274 - Checkpoint storage is set to 'jobmanager'
2023-12-26 10:54:08 INFO  Task:1067 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db2.ice_T01 (1/1)#0 (fe3b6044519e8d13c2cafb88f20729de) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db2.ice_T01 (1/1) (fe3b6044519e8d13c2cafb88f20729de) switched from DEPLOYING to INITIALIZING.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackendBuilder:175 - Finished to build heap keyed state-backend.
2023-12-26 10:54:08 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 1 has no restore state.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:54:08 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 9 has no restore state.
2023-12-26 10:54:08 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 0 has no restore state.
2023-12-26 10:54:08 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 6 has no restore state.
2023-12-26 10:54:08 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 7 has no restore state.
2023-12-26 10:54:08 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 11 has no restore state.
2023-12-26 10:54:08 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 10 has no restore state.
2023-12-26 10:54:08 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 3 has no restore state.
2023-12-26 10:54:08 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 8 has no restore state.
2023-12-26 10:54:08 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 4 has no restore state.
2023-12-26 10:54:08 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 5 has no restore state.
2023-12-26 10:54:08 INFO  FlinkKafkaConsumerBase:1003 - Consumer subtask 2 has no restore state.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:54:08 INFO  HeapKeyedStateBackend:150 - Initializing heap keyed state backend with stream factory.
2023-12-26 10:54:08 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:08 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:08 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:08 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:08 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:08 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:08 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:08 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:08 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:08 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:08 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:08 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (6/12)#0 (9c8079736913517df2ba1ae9d75d3d9b) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (1/12)#0 (c98a47c6ce963e5abd11b0bb3d7c4d50) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (2/12)#0 (782480ddf3929685bae7d79f6ab23978) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (12/12)#0 (a55c681330da7e7476b80533fc93a136) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (5/12)#0 (bcad5a680f80dcbd6b763f4c04596c26) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (11/12)#0 (2719b74cd45bd8bc0bdd463b0b137613) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (10/12)#0 (35198123380d155db20a2300ee7b7cf7) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (4/12)#0 (8c163a80e44edc3c0b31a653c6d394ad) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (7/12)#0 (e13358615178b16f463a47712adccf92) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (8/12)#0 (b696ef7f502a355005230bbf10054b07) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (3/12)#0 (d39a1a55045a8ead0595f705b9eff056) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  Task:1067 - IcebergStreamWriter (9/12)#0 (efc894151881020ac14ccbe76c6d866e) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (6/12) (9c8079736913517df2ba1ae9d75d3d9b) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (1/12) (c98a47c6ce963e5abd11b0bb3d7c4d50) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (2/12) (782480ddf3929685bae7d79f6ab23978) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (12/12) (a55c681330da7e7476b80533fc93a136) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (5/12) (bcad5a680f80dcbd6b763f4c04596c26) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (11/12) (2719b74cd45bd8bc0bdd463b0b137613) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (10/12) (35198123380d155db20a2300ee7b7cf7) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (4/12) (8c163a80e44edc3c0b31a653c6d394ad) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (7/12) (e13358615178b16f463a47712adccf92) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (8/12) (b696ef7f502a355005230bbf10054b07) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (3/12) (d39a1a55045a8ead0595f705b9eff056) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  ExecutionGraph:1438 - IcebergStreamWriter (9/12) (efc894151881020ac14ccbe76c6d866e) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:08 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db2.db/ice_T01/metadata/00000-8d17a90e-c792-4c2a-8938-8a40a3f14665.metadata.json
2023-12-26 10:54:08 INFO  CheckpointCoordinator:888 - Failed to trigger checkpoint for job 0b5e7a236a5e960961b6cd7adf486ca7 since Checkpoint triggering task Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) of job 0b5e7a236a5e960961b6cd7adf486ca7 is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
2023-12-26 10:54:09 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:09 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:09 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:09 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:09 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:09 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:09 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:09 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:09 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:09 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:09 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:09 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:09 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:09 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:09 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559249090
2023-12-26 10:54:09 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:09 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:09 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559249091
2023-12-26 10:54:09 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:09 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:09 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559249091
2023-12-26 10:54:09 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:09 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:09 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559249091
2023-12-26 10:54:09 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:09 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:09 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559249092
2023-12-26 10:54:09 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:09 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:09 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559249093
2023-12-26 10:54:09 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:09 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:09 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559249093
2023-12-26 10:54:09 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:09 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:09 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559249093
2023-12-26 10:54:09 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:09 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:09 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559249094
2023-12-26 10:54:09 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:09 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:09 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559249094
2023-12-26 10:54:09 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:09 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:09 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559249094
2023-12-26 10:54:09 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:09 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:09 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559249094
2023-12-26 10:54:09 INFO  BaseMetastoreCatalog:64 - Table loaded by catalog: iceberg_catalog.db2.ice_T01
2023-12-26 10:54:09 INFO  Task:1067 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db2.ice_T01 (1/1)#0 (fe3b6044519e8d13c2cafb88f20729de) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:09 INFO  ExecutionGraph:1438 - IcebergFilesCommitter -> Sink: IcebergSink iceberg_catalog.db2.ice_T01 (1/1) (fe3b6044519e8d13c2cafb88f20729de) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  Metadata:259 - [Consumer clientId=consumer-g1-10, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:54:10 INFO  Metadata:259 - [Consumer clientId=consumer-g1-4, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:54:10 INFO  Metadata:259 - [Consumer clientId=consumer-g1-11, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:54:10 INFO  Metadata:259 - [Consumer clientId=consumer-g1-7, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:54:10 INFO  Metadata:259 - [Consumer clientId=consumer-g1-1, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:54:10 INFO  Metadata:259 - [Consumer clientId=consumer-g1-8, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:54:10 INFO  Metadata:259 - [Consumer clientId=consumer-g1-9, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:54:10 INFO  Metadata:259 - [Consumer clientId=consumer-g1-5, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:54:10 INFO  Metadata:259 - [Consumer clientId=consumer-g1-6, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:54:10 INFO  Metadata:259 - [Consumer clientId=consumer-g1-3, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:54:10 INFO  Metadata:259 - [Consumer clientId=consumer-g1-12, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:54:10 INFO  Metadata:259 - [Consumer clientId=consumer-g1-2, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 3 initially has no partitions to read from.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 0 initially has no partitions to read from.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 11 initially has no partitions to read from.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 8 initially has no partitions to read from.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 9 initially has no partitions to read from.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 1 initially has no partitions to read from.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 2 initially has no partitions to read from.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:696 - Consumer subtask 7 will start reading the following 1 partitions from the latest offsets: [KafkaTopicPartition{topic='T01', partition=0}]
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 5 initially has no partitions to read from.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 6 initially has no partitions to read from.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 10 initially has no partitions to read from.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:745 - Consumer subtask 4 initially has no partitions to read from.
2023-12-26 10:54:10 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12)#0 (ce08e7548623a7fb8da6f2a2ddda5389) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12)#0 (c3c37af38c416f78179bd5c3f29869ff) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12)#0 (c787e45a7df996428f72d21b80449ff8) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12)#0 (78bc38c35abc2e44dbc4891bc034c18b) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12)#0 (ea7465f29e9a4d822d3bf023c7238315) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12)#0 (9cf4b4e02d3af3d541ed3e903dea7486) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12)#0 (6c6967fce7c48e4f8cd3e474f2e5ec72) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12)#0 (d94290d208ffe806e61673392406bd4d) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12)#0 (8237e3d8fe58271b5762d0c5e572d446) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12)#0 (1c439751c0bab6c437d23e9db5a9c4c7) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12)#0 (2b548e32b542d30b756e2273261776fa) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  Task:1067 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12)#0 (0d25ee151b606a4de3c337fe8313cc52) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (12/12) (c3c37af38c416f78179bd5c3f29869ff) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 7 creating fetcher with offsets {KafkaTopicPartition{topic='T01', partition=0}=-915623761774}.
2023-12-26 10:54:10 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (7/12) (78bc38c35abc2e44dbc4891bc034c18b) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 10 creating fetcher with offsets {}.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 6 creating fetcher with offsets {}.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 9 creating fetcher with offsets {}.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 3 creating fetcher with offsets {}.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 1 creating fetcher with offsets {}.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 11 creating fetcher with offsets {}.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 4 creating fetcher with offsets {}.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 2 creating fetcher with offsets {}.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 0 creating fetcher with offsets {}.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 5 creating fetcher with offsets {}.
2023-12-26 10:54:10 INFO  FlinkKafkaConsumerBase:796 - Consumer subtask 8 creating fetcher with offsets {}.
2023-12-26 10:54:10 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (8/12) (ea7465f29e9a4d822d3bf023c7238315) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (10/12) (9cf4b4e02d3af3d541ed3e903dea7486) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (1/12) (c787e45a7df996428f72d21b80449ff8) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (9/12) (6c6967fce7c48e4f8cd3e474f2e5ec72) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (3/12) (ce08e7548623a7fb8da6f2a2ddda5389) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (2/12) (d94290d208ffe806e61673392406bd4d) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (4/12) (8237e3d8fe58271b5762d0c5e572d446) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (5/12) (0d25ee151b606a4de3c337fe8313cc52) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (11/12) (1c439751c0bab6c437d23e9db5a9c4c7) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  ExecutionGraph:1438 - Source: TableSourceScan(table=[[default_catalog, db2, kafka_T01]], fields=[C1, C2, C3]) -> NotNullEnforcer(fields=[C1]) (6/12) (2b548e32b542d30b756e2273261776fa) switched from INITIALIZING to RUNNING.
2023-12-26 10:54:10 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:10 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:10 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:10 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:10 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:10 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:10 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:10 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:10 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:10 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:10 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:10 INFO  ConsumerConfig:347 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.20.3.227:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-12-26 10:54:10 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:10 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:10 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:10 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:10 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:10 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559250390
2023-12-26 10:54:10 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:10 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:10 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:10 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:10 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:10 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:10 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:10 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559250390
2023-12-26 10:54:10 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:10 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:10 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:10 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559250390
2023-12-26 10:54:10 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:10 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:10 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559250396
2023-12-26 10:54:10 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:10 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:10 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559250395
2023-12-26 10:54:10 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:10 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:10 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:10 WARN  ConsumerConfig:355 - The configuration 'flink.partition-discovery.interval-millis' was supplied but isn't a known config.
2023-12-26 10:54:10 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:10 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559250395
2023-12-26 10:54:10 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:10 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:10 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559250395
2023-12-26 10:54:10 INFO  KafkaConsumer:1123 - [Consumer clientId=consumer-g1-22, groupId=g1] Subscribed to partition(s): T01-0
2023-12-26 10:54:10 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:10 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:10 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559250394
2023-12-26 10:54:10 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:10 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:10 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559250393
2023-12-26 10:54:10 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:10 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:10 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559250416
2023-12-26 10:54:10 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:10 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:10 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559250416
2023-12-26 10:54:10 INFO  AppInfoParser:117 - Kafka version: 2.4.1
2023-12-26 10:54:10 INFO  AppInfoParser:118 - Kafka commitId: c57222ae8cd7866b
2023-12-26 10:54:10 INFO  AppInfoParser:119 - Kafka startTimeMs: 1703559250412
2023-12-26 10:54:10 INFO  SubscriptionState:564 - [Consumer clientId=consumer-g1-22, groupId=g1] Seeking to LATEST offset of partition T01-0
2023-12-26 10:54:10 INFO  Metadata:259 - [Consumer clientId=consumer-g1-22, groupId=g1] Cluster ID: KCQkapHlSyKQP7V-sEk-bA
2023-12-26 10:54:11 INFO  SubscriptionState:381 - [Consumer clientId=consumer-g1-22, groupId=g1] Resetting offset for partition T01-0 to offset 47.
2023-12-26 10:54:18 INFO  CheckpointCoordinator:742 - Triggering checkpoint 1 (type=CHECKPOINT) @ 1703559258939 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:54:19 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 1
2023-12-26 10:54:19 INFO  CheckpointCoordinator:1246 - Completed checkpoint 1 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 516 ms).
2023-12-26 10:54:20 INFO  AbstractCoordinator:756 - [Consumer clientId=consumer-g1-22, groupId=g1] Discovered group coordinator 172.20.3.227:9092 (id: 2147483647 rack: null)
2023-12-26 10:54:28 INFO  CheckpointCoordinator:742 - Triggering checkpoint 2 (type=CHECKPOINT) @ 1703559268938 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:54:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 2
2023-12-26 10:54:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 2 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 117 ms).
2023-12-26 10:54:38 INFO  CheckpointCoordinator:742 - Triggering checkpoint 3 (type=CHECKPOINT) @ 1703559278938 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:54:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 3
2023-12-26 10:54:39 INFO  CheckpointCoordinator:1246 - Completed checkpoint 3 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 131 ms).
2023-12-26 10:54:48 INFO  CheckpointCoordinator:742 - Triggering checkpoint 4 (type=CHECKPOINT) @ 1703559288936 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:54:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 4
2023-12-26 10:54:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 4 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 87 ms).
2023-12-26 10:54:52 INFO  CodecPool:153 - Got brand-new compressor [.gz]
2023-12-26 10:54:52 INFO  CodecPool:153 - Got brand-new compressor [.gz]
2023-12-26 10:54:58 INFO  CheckpointCoordinator:742 - Triggering checkpoint 5 (type=CHECKPOINT) @ 1703559298938 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:55:00 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 5
2023-12-26 10:55:02 INFO  CheckpointCoordinator:1246 - Completed checkpoint 5 for job 0b5e7a236a5e960961b6cd7adf486ca7 (15782 bytes in 3234 ms).
2023-12-26 10:55:03 INFO  IcebergFilesCommitter:306 - Committing rowDelta with 0 data files and 1 delete files to table iceberg_catalog.db2.ice_T01
2023-12-26 10:55:07 INFO  HiveTableOperations:315 - Committed to table iceberg_catalog.db2.ice_T01 with the new metadata location hdfs://172.20.29.46:8020/user/hive/warehouse/db2.db/ice_T01/metadata/00001-f24a2c96-10a6-48de-a048-44a357a562c3.metadata.json
2023-12-26 10:55:07 INFO  BaseMetastoreTableOperations:137 - Successfully committed to table iceberg_catalog.db2.ice_T01 in 1307 ms
2023-12-26 10:55:07 INFO  SnapshotProducer:327 - Committed snapshot 327684663878466956 (BaseRowDelta)
2023-12-26 10:55:07 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db2.db/ice_T01/metadata/00001-f24a2c96-10a6-48de-a048-44a357a562c3.metadata.json
2023-12-26 10:55:08 INFO  IcebergFilesCommitter:314 - Committed in 5003 ms
2023-12-26 10:55:08 INFO  CheckpointCoordinator:742 - Triggering checkpoint 6 (type=CHECKPOINT) @ 1703559308937 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:55:09 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 6
2023-12-26 10:55:09 INFO  CheckpointCoordinator:1246 - Completed checkpoint 6 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 107 ms).
2023-12-26 10:55:18 INFO  CheckpointCoordinator:742 - Triggering checkpoint 7 (type=CHECKPOINT) @ 1703559318938 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:55:19 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 7
2023-12-26 10:55:19 INFO  CheckpointCoordinator:1246 - Completed checkpoint 7 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 87 ms).
2023-12-26 10:55:28 INFO  CheckpointCoordinator:742 - Triggering checkpoint 8 (type=CHECKPOINT) @ 1703559328940 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:55:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 8
2023-12-26 10:55:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 8 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 102 ms).
2023-12-26 10:55:38 INFO  CheckpointCoordinator:742 - Triggering checkpoint 9 (type=CHECKPOINT) @ 1703559338941 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:55:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 9
2023-12-26 10:55:39 INFO  CheckpointCoordinator:1246 - Completed checkpoint 9 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 93 ms).
2023-12-26 10:55:48 INFO  CheckpointCoordinator:742 - Triggering checkpoint 10 (type=CHECKPOINT) @ 1703559348938 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:55:48 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 10
2023-12-26 10:55:48 INFO  CheckpointCoordinator:1246 - Completed checkpoint 10 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 52 ms).
2023-12-26 10:55:58 INFO  CheckpointCoordinator:742 - Triggering checkpoint 11 (type=CHECKPOINT) @ 1703559358937 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:55:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 11
2023-12-26 10:55:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 11 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 167 ms).
2023-12-26 10:56:08 INFO  CheckpointCoordinator:742 - Triggering checkpoint 12 (type=CHECKPOINT) @ 1703559368935 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:56:09 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 12
2023-12-26 10:56:09 INFO  CheckpointCoordinator:1246 - Completed checkpoint 12 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 66 ms).
2023-12-26 10:56:18 INFO  CheckpointCoordinator:742 - Triggering checkpoint 13 (type=CHECKPOINT) @ 1703559378941 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:56:18 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 13
2023-12-26 10:56:19 INFO  CheckpointCoordinator:1246 - Completed checkpoint 13 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 67 ms).
2023-12-26 10:56:28 INFO  CheckpointCoordinator:742 - Triggering checkpoint 14 (type=CHECKPOINT) @ 1703559388939 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:56:29 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 14
2023-12-26 10:56:29 INFO  CheckpointCoordinator:1246 - Completed checkpoint 14 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 115 ms).
2023-12-26 10:56:38 INFO  CheckpointCoordinator:742 - Triggering checkpoint 15 (type=CHECKPOINT) @ 1703559398939 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:56:39 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 15
2023-12-26 10:56:39 INFO  CheckpointCoordinator:1246 - Completed checkpoint 15 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 72 ms).
2023-12-26 10:56:39 INFO  IcebergFilesCommitter:306 - Committing append with 0 data files and 0 delete files to table iceberg_catalog.db2.ice_T01
2023-12-26 10:56:41 INFO  HiveTableOperations:315 - Committed to table iceberg_catalog.db2.ice_T01 with the new metadata location hdfs://172.20.29.46:8020/user/hive/warehouse/db2.db/ice_T01/metadata/00002-a26456dc-1a37-41a9-a84f-c7c7c6b39a40.metadata.json
2023-12-26 10:56:41 INFO  BaseMetastoreTableOperations:137 - Successfully committed to table iceberg_catalog.db2.ice_T01 in 1710 ms
2023-12-26 10:56:41 INFO  SnapshotProducer:327 - Committed snapshot 4527289024793408613 (MergeAppend)
2023-12-26 10:56:41 INFO  BaseMetastoreTableOperations:184 - Refreshing table metadata from new version: hdfs://172.20.29.46:8020/user/hive/warehouse/db2.db/ice_T01/metadata/00002-a26456dc-1a37-41a9-a84f-c7c7c6b39a40.metadata.json
2023-12-26 10:56:42 INFO  IcebergFilesCommitter:314 - Committed in 3769 ms
2023-12-26 10:56:48 INFO  CheckpointCoordinator:742 - Triggering checkpoint 16 (type=CHECKPOINT) @ 1703559408940 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:56:49 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 16
2023-12-26 10:56:49 INFO  CheckpointCoordinator:1246 - Completed checkpoint 16 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 124 ms).
2023-12-26 10:56:58 INFO  CheckpointCoordinator:742 - Triggering checkpoint 17 (type=CHECKPOINT) @ 1703559418935 for job 0b5e7a236a5e960961b6cd7adf486ca7.
2023-12-26 10:56:59 INFO  IcebergFilesCommitter:162 - Start to flush snapshot state to state backend, table: iceberg_catalog.db2.ice_T01, checkpointId: 17
2023-12-26 10:56:59 INFO  CheckpointCoordinator:1246 - Completed checkpoint 17 for job 0b5e7a236a5e960961b6cd7adf486ca7 (13265 bytes in 90 ms).
2023-12-26 10:57:05 INFO  TransientBlobCache:240 - Shutting down BLOB cache
2023-12-26 10:57:05 INFO  TaskExecutorLocalStateStoresManager:231 - Shutting down TaskExecutorLocalStateStoresManager.
2023-12-26 10:57:05 INFO  PermanentBlobCache:240 - Shutting down BLOB cache
2023-12-26 10:57:05 INFO  FileChannelManagerImpl:149 - FileChannelManager removed spill file directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-netty-shuffle-93ac5af5-45c1-47f8-937c-332bad8e4788
2023-12-26 10:57:05 INFO  FileChannelManagerImpl:149 - FileChannelManager removed spill file directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-io-eabca365-e9d4-406a-ad30-48e46784438f
2023-12-26 10:57:05 INFO  BlobServer:345 - Stopped BLOB server at 0.0.0.0:50014
2023-12-26 10:57:05 INFO  FileCache:160 - removed file cache directory /var/folders/9v/nj82_qkd54s928ndpdzrts280000gn/T/flink-dist-cache-b03912f9-b894-4799-af8a-6a7c13dccaf4
2023-12-26 16:30:34 INFO  FlinkTable2IceTable:53 - create iceberg_catalog now!
2023-12-26 16:30:34 INFO  FlinkTable2IceTable:65 - catalog:create catalog iceberg_catalog with (
   'type'='iceberg',
   'catalog-type'='hive',
   'uri'='thrift://172.20.29.46:9083',
   'hive-conf-dir'='/Users/lifenghua/study/sourcecode/iceberg-demo/config/hive-conf-46',
   'hadoop-conf-dir'='/Users/lifenghua/study/sourcecode/iceberg-demo/config/hive-conf-46',
   'client'='1',
   'property-version'='2',
   'warehouse'='hdfs://172.20.29.46:8020/user/hive/warehouse/')

2023-12-26 16:30:34 WARN  HadoopUtils:139 - Could not find Hadoop configuration via any of the supported methods (Flink configuration, environment variables).
2023-12-26 16:30:35 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-26 16:30:35 INFO  HiveConf:176 - Found configuration file null
2023-12-26 16:30:35 WARN  HiveConf:4015 - HiveConf of name hive.vectorized.use.checked.expressions does not exist
2023-12-26 16:30:35 WARN  HiveConf:4015 - HiveConf of name hive.strict.checks.no.partition.filter does not exist
2023-12-26 16:30:35 WARN  HiveConf:4015 - HiveConf of name hive.strict.checks.orderby.no.limit does not exist
2023-12-26 16:30:35 WARN  HiveConf:4015 - HiveConf of name hive.vectorized.input.format.excludes does not exist
2023-12-26 16:30:35 WARN  HiveConf:4015 - HiveConf of name hive.strict.checks.bucketing does not exist
2023-12-26 16:30:35 INFO  metastore:434 - Trying to connect to metastore with URI thrift://172.20.29.46:9083
